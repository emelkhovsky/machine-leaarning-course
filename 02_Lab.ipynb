{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac3360af-2fb4-482f-aed2-a9ffc27b6529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59002603-7fdb-42eb-8283-462715ea3628",
   "metadata": {},
   "source": [
    "## 1. Загрузка и подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d9e710e-418a-4fdf-bfc8-9414811d4919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19158, 14)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/emelkhovsky/machine-leaarning-course/master/aug_train.csv\"\n",
    "data_raw = pd.read_csv(url)\n",
    "data_raw.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3378a0b2-12ba-4840-8b15-22ccc22510f6",
   "metadata": {},
   "source": [
    "Удалим выбросы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cc1816e-339f-4966-ac3d-5b158d9662e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18891, 14)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_to_drop = data_raw[\n",
    "    (data_raw['training_hours'] < data_raw['training_hours'].quantile(0.005)) | (data_raw['training_hours'] > data_raw['training_hours'].quantile(0.995)) | \n",
    "    (data_raw['city_development_index']  < data_raw['city_development_index' ].quantile(0.005)) | (data_raw['city_development_index']  > data_raw['city_development_index' ].quantile(0.995))].index\n",
    "data = data_raw.drop(rows_to_drop)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5727b531-141a-459c-8f76-a0ccbefa88ee",
   "metadata": {},
   "source": [
    "Удалим пропущенные значения в тех признаках, где процент их минимален"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35c35922-e644-4e66-a833-f1d4f933d829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17769, 14)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "enrollee_id                  0\n",
       "city                         0\n",
       "city_development_index       0\n",
       "gender                    3795\n",
       "relevent_experience          0\n",
       "enrolled_university          0\n",
       "education_level              0\n",
       "major_discipline          2196\n",
       "experience                   0\n",
       "company_size              5229\n",
       "company_type              5394\n",
       "last_new_job                 0\n",
       "training_hours               0\n",
       "target                       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dropna(axis=0, how='all', subset=['enrolled_university'], inplace=True)\n",
    "data.dropna(axis=0, how='all', subset=['education_level'], inplace=True)\n",
    "data.dropna(axis=0, how='all', subset=['experience'], inplace=True)\n",
    "data.dropna(axis=0, how='all', subset=['last_new_job'], inplace=True)\n",
    "print(data.shape)\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ebaa0a7-c73c-4e62-b6e5-9d6102ea8cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['gender'].fillna(data['gender'].mode().iloc[0], inplace=True)\n",
    "data['major_discipline'].fillna(data['major_discipline'].mode().iloc[0], inplace=True)\n",
    "data['company_size'].fillna(data['company_size'].mode().iloc[0], inplace=True)\n",
    "data['company_type'].fillna(data['company_type'].mode().iloc[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c697ab9-3b84-4061-b935-83d6e48098c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17769, 14)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "enrollee_id               0\n",
       "city                      0\n",
       "city_development_index    0\n",
       "gender                    0\n",
       "relevent_experience       0\n",
       "enrolled_university       0\n",
       "education_level           0\n",
       "major_discipline          0\n",
       "experience                0\n",
       "company_size              0\n",
       "company_type              0\n",
       "last_new_job              0\n",
       "training_hours            0\n",
       "target                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.shape)\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cebb82bd-3fc0-4ec5-bd8d-823b5a6a180a",
   "metadata": {},
   "source": [
    "Закодируем категориальные признаки с помощью числовых признаков, применив метод бинаризации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1843728b-7d9e-494f-9729-4b26f82b0e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17769, 180)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enrollee_id</th>\n",
       "      <th>city_development_index</th>\n",
       "      <th>training_hours</th>\n",
       "      <th>target</th>\n",
       "      <th>city_city_1</th>\n",
       "      <th>city_city_10</th>\n",
       "      <th>city_city_100</th>\n",
       "      <th>city_city_101</th>\n",
       "      <th>city_city_102</th>\n",
       "      <th>city_city_103</th>\n",
       "      <th>...</th>\n",
       "      <th>company_type_NGO</th>\n",
       "      <th>company_type_Other</th>\n",
       "      <th>company_type_Public Sector</th>\n",
       "      <th>company_type_Pvt Ltd</th>\n",
       "      <th>last_new_job_1</th>\n",
       "      <th>last_new_job_2</th>\n",
       "      <th>last_new_job_3</th>\n",
       "      <th>last_new_job_4</th>\n",
       "      <th>last_new_job_&gt;4</th>\n",
       "      <th>last_new_job_never</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8949</td>\n",
       "      <td>0.920</td>\n",
       "      <td>36</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29725</td>\n",
       "      <td>0.776</td>\n",
       "      <td>47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11561</td>\n",
       "      <td>0.624</td>\n",
       "      <td>83</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>666</td>\n",
       "      <td>0.767</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21651</td>\n",
       "      <td>0.764</td>\n",
       "      <td>24</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 180 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   enrollee_id  city_development_index  training_hours  target  city_city_1  \\\n",
       "0         8949                   0.920              36     1.0            0   \n",
       "1        29725                   0.776              47     0.0            0   \n",
       "2        11561                   0.624              83     0.0            0   \n",
       "4          666                   0.767               8     0.0            0   \n",
       "5        21651                   0.764              24     1.0            0   \n",
       "\n",
       "   city_city_10  city_city_100  city_city_101  city_city_102  city_city_103  \\\n",
       "0             0              0              0              0              1   \n",
       "1             0              0              0              0              0   \n",
       "2             0              0              0              0              0   \n",
       "4             0              0              0              0              0   \n",
       "5             0              0              0              0              0   \n",
       "\n",
       "   ...  company_type_NGO  company_type_Other  company_type_Public Sector  \\\n",
       "0  ...                 0                   0                           0   \n",
       "1  ...                 0                   0                           0   \n",
       "2  ...                 0                   0                           0   \n",
       "4  ...                 0                   0                           0   \n",
       "5  ...                 0                   0                           0   \n",
       "\n",
       "   company_type_Pvt Ltd  last_new_job_1  last_new_job_2  last_new_job_3  \\\n",
       "0                     1               1               0               0   \n",
       "1                     1               0               0               0   \n",
       "2                     1               0               0               0   \n",
       "4                     0               0               0               0   \n",
       "5                     1               1               0               0   \n",
       "\n",
       "   last_new_job_4  last_new_job_>4  last_new_job_never  \n",
       "0               0                0                   0  \n",
       "1               0                1                   0  \n",
       "2               0                0                   1  \n",
       "4               1                0                   0  \n",
       "5               0                0                   0  \n",
       "\n",
       "[5 rows x 180 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.get_dummies(data)\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "57e63a6c-c057-472d-92ec-283b31016183",
   "metadata": {},
   "source": [
    "Проведем нормализацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8aa7578-8c3c-4d3e-b39a-d5cf3aefcdfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enrollee_id</th>\n",
       "      <th>city_development_index</th>\n",
       "      <th>training_hours</th>\n",
       "      <th>target</th>\n",
       "      <th>city_city_1</th>\n",
       "      <th>city_city_10</th>\n",
       "      <th>city_city_100</th>\n",
       "      <th>city_city_101</th>\n",
       "      <th>city_city_102</th>\n",
       "      <th>city_city_103</th>\n",
       "      <th>...</th>\n",
       "      <th>company_type_NGO</th>\n",
       "      <th>company_type_Other</th>\n",
       "      <th>company_type_Public Sector</th>\n",
       "      <th>company_type_Pvt Ltd</th>\n",
       "      <th>last_new_job_1</th>\n",
       "      <th>last_new_job_2</th>\n",
       "      <th>last_new_job_3</th>\n",
       "      <th>last_new_job_4</th>\n",
       "      <th>last_new_job_&gt;4</th>\n",
       "      <th>last_new_job_never</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>17769.000000</td>\n",
       "      <td>17769.000000</td>\n",
       "      <td>17769.000000</td>\n",
       "      <td>17769.000000</td>\n",
       "      <td>17769.000000</td>\n",
       "      <td>17769.000000</td>\n",
       "      <td>17769.000000</td>\n",
       "      <td>17769.000000</td>\n",
       "      <td>17769.000000</td>\n",
       "      <td>17769.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>17769.000000</td>\n",
       "      <td>17769.000000</td>\n",
       "      <td>17769.000000</td>\n",
       "      <td>17769.000000</td>\n",
       "      <td>17769.000000</td>\n",
       "      <td>17769.000000</td>\n",
       "      <td>17769.000000</td>\n",
       "      <td>17769.000000</td>\n",
       "      <td>17769.000000</td>\n",
       "      <td>17769.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.506152</td>\n",
       "      <td>0.741678</td>\n",
       "      <td>0.195271</td>\n",
       "      <td>0.245146</td>\n",
       "      <td>0.001407</td>\n",
       "      <td>0.004615</td>\n",
       "      <td>0.014520</td>\n",
       "      <td>0.003827</td>\n",
       "      <td>0.016039</td>\n",
       "      <td>0.234735</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027632</td>\n",
       "      <td>0.006303</td>\n",
       "      <td>0.050312</td>\n",
       "      <td>0.830435</td>\n",
       "      <td>0.431594</td>\n",
       "      <td>0.157240</td>\n",
       "      <td>0.055265</td>\n",
       "      <td>0.056334</td>\n",
       "      <td>0.178232</td>\n",
       "      <td>0.121335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.289407</td>\n",
       "      <td>0.292393</td>\n",
       "      <td>0.179970</td>\n",
       "      <td>0.430186</td>\n",
       "      <td>0.037484</td>\n",
       "      <td>0.067777</td>\n",
       "      <td>0.119623</td>\n",
       "      <td>0.061745</td>\n",
       "      <td>0.125630</td>\n",
       "      <td>0.423845</td>\n",
       "      <td>...</td>\n",
       "      <td>0.163922</td>\n",
       "      <td>0.079144</td>\n",
       "      <td>0.218595</td>\n",
       "      <td>0.375261</td>\n",
       "      <td>0.495313</td>\n",
       "      <td>0.364037</td>\n",
       "      <td>0.228503</td>\n",
       "      <td>0.230572</td>\n",
       "      <td>0.382718</td>\n",
       "      <td>0.326525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.255370</td>\n",
       "      <td>0.550971</td>\n",
       "      <td>0.066038</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.509392</td>\n",
       "      <td>0.929612</td>\n",
       "      <td>0.141509</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.758261</td>\n",
       "      <td>0.953883</td>\n",
       "      <td>0.270440</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 180 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        enrollee_id  city_development_index  training_hours        target  \\\n",
       "count  17769.000000            17769.000000    17769.000000  17769.000000   \n",
       "mean       0.506152                0.741678        0.195271      0.245146   \n",
       "std        0.289407                0.292393        0.179970      0.430186   \n",
       "min        0.000000                0.000000        0.000000      0.000000   \n",
       "25%        0.255370                0.550971        0.066038      0.000000   \n",
       "50%        0.509392                0.929612        0.141509      0.000000   \n",
       "75%        0.758261                0.953883        0.270440      0.000000   \n",
       "max        1.000000                1.000000        1.000000      1.000000   \n",
       "\n",
       "        city_city_1  city_city_10  city_city_100  city_city_101  \\\n",
       "count  17769.000000  17769.000000   17769.000000   17769.000000   \n",
       "mean       0.001407      0.004615       0.014520       0.003827   \n",
       "std        0.037484      0.067777       0.119623       0.061745   \n",
       "min        0.000000      0.000000       0.000000       0.000000   \n",
       "25%        0.000000      0.000000       0.000000       0.000000   \n",
       "50%        0.000000      0.000000       0.000000       0.000000   \n",
       "75%        0.000000      0.000000       0.000000       0.000000   \n",
       "max        1.000000      1.000000       1.000000       1.000000   \n",
       "\n",
       "       city_city_102  city_city_103  ...  company_type_NGO  \\\n",
       "count   17769.000000   17769.000000  ...      17769.000000   \n",
       "mean        0.016039       0.234735  ...          0.027632   \n",
       "std         0.125630       0.423845  ...          0.163922   \n",
       "min         0.000000       0.000000  ...          0.000000   \n",
       "25%         0.000000       0.000000  ...          0.000000   \n",
       "50%         0.000000       0.000000  ...          0.000000   \n",
       "75%         0.000000       0.000000  ...          0.000000   \n",
       "max         1.000000       1.000000  ...          1.000000   \n",
       "\n",
       "       company_type_Other  company_type_Public Sector  company_type_Pvt Ltd  \\\n",
       "count        17769.000000                17769.000000          17769.000000   \n",
       "mean             0.006303                    0.050312              0.830435   \n",
       "std              0.079144                    0.218595              0.375261   \n",
       "min              0.000000                    0.000000              0.000000   \n",
       "25%              0.000000                    0.000000              1.000000   \n",
       "50%              0.000000                    0.000000              1.000000   \n",
       "75%              0.000000                    0.000000              1.000000   \n",
       "max              1.000000                    1.000000              1.000000   \n",
       "\n",
       "       last_new_job_1  last_new_job_2  last_new_job_3  last_new_job_4  \\\n",
       "count    17769.000000    17769.000000    17769.000000    17769.000000   \n",
       "mean         0.431594        0.157240        0.055265        0.056334   \n",
       "std          0.495313        0.364037        0.228503        0.230572   \n",
       "min          0.000000        0.000000        0.000000        0.000000   \n",
       "25%          0.000000        0.000000        0.000000        0.000000   \n",
       "50%          0.000000        0.000000        0.000000        0.000000   \n",
       "75%          1.000000        0.000000        0.000000        0.000000   \n",
       "max          1.000000        1.000000        1.000000        1.000000   \n",
       "\n",
       "       last_new_job_>4  last_new_job_never  \n",
       "count     17769.000000        17769.000000  \n",
       "mean          0.178232            0.121335  \n",
       "std           0.382718            0.326525  \n",
       "min           0.000000            0.000000  \n",
       "25%           0.000000            0.000000  \n",
       "50%           0.000000            0.000000  \n",
       "75%           0.000000            0.000000  \n",
       "max           1.000000            1.000000  \n",
       "\n",
       "[8 rows x 180 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_norm = (data - data.min(axis=0)) / (data.max(axis=0) - data.min(axis=0))\n",
    "data_norm.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c159c79-c9c1-4517-99ad-fd88a4f3a759",
   "metadata": {},
   "source": [
    "## 2. Разбиение данных на обучающую и тестовую выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dfb3202-1b44-4c09-acef-469bbc464eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17769, 178)\n",
      "(17769,)\n"
     ]
    }
   ],
   "source": [
    "X = data_norm.drop(['enrollee_id', 'target'], axis=1) \n",
    "y = data_norm['target']\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a7c3f96-facb-4d76-9c02-e7685dcd7038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11905, 5864)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)\n",
    "\n",
    "N_train, _ = X_train.shape \n",
    "N_test,  _ = X_test.shape \n",
    "\n",
    "N_train, N_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caafbe8d-77df-4f23-937e-a6107fade70b",
   "metadata": {},
   "source": [
    "## 3. Подбор гиперпараметров"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7d98847e-3685-4abb-8c72-7d311a1c630f",
   "metadata": {},
   "source": [
    "В качестве алгоритма оптимизации выберем алгоритм Adam, в качестве функции активации выберем Relu, в качетве метрики оценки классификатора возьмем accuracy_score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bba3308d-8243-4af8-bfd0-93b1a705941d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [15:26<00:00, 18.53s/it]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "hidden_layer_size = np.linspace(1, 50, 50)\n",
    "hidden_layer_size = hidden_layer_size.astype(int)\n",
    "test_err, train_err = [], []\n",
    "train_acc, test_acc = [], []\n",
    "\n",
    "for size in tqdm(hidden_layer_size):\n",
    "    model = MLPClassifier(hidden_layer_sizes=(size,), \n",
    "                          solver='adam', activation='relu', max_iter=1000, random_state=13)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    train_err.append(np.mean(y_train != y_train_pred))\n",
    "    test_err.append(np.mean(y_test != y_test_pred))\n",
    "    \n",
    "    train_acc.append(accuracy_score(y_train, y_train_pred))\n",
    "    test_acc.append(accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e14fc208-e48d-4632-9674-82a5612376fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: минимальное значение ошибки = 0.0637547249055019, число нейронов = 50\n",
      "Test: минимальное значение ошибки = 0.21862210095497953, число нейронов = 2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hidden_layer_size, train_err, 'b-o', label = 'train')\n",
    "plt.plot(hidden_layer_size, test_err, 'r-o', label = 'test')\n",
    "plt.xlim([np.min(hidden_layer_size), np.max(hidden_layer_size)])\n",
    "plt.title('Error is as a function of hidden_layer_size')\n",
    "plt.xlabel('hidden_layer_size')\n",
    "plt.ylabel('error')\n",
    "plt.legend()\n",
    "\n",
    "print(f\"Train: минимальное значение ошибки = {np.min(train_err)}, число нейронов = {hidden_layer_size[np.argmin(train_err)]}\")\n",
    "print(f\"Test: минимальное значение ошибки = {np.min(test_err)}, число нейронов = {hidden_layer_size[np.argmin(test_err)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8af571c3-9f17-4b32-a3cb-fbc461f89431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: максимальное значение accuracy = 0.9362452750944981, число нейронов = 50\n",
      "Test: максимальное значение accuracy = 0.7813778990450204, число нейронов = 2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEXCAYAAAC6baP3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAA780lEQVR4nO3dd5hU5fXA8e/Zpe3SmxSBBeyiiIL8NEbF2LuxR2yoqBF7sJJYoliwRiyIio1iYo2JGoktGpUYUFQUjIg0pUtROsv5/fHece/O3jszd3Zmp+z5PM88O3Pre+/u3jNvF1XFGGOMSVVJrhNgjDGmsFjgMMYYE4kFDmOMMZFY4DDGGBOJBQ5jjDGRWOAwxhgTiQUOk1EisreIfFUH5/mtiCwSkZ9EpG22z+c777Ui8mhdnc933l+LyDzvencNWK8isnXIvgNFZGKCY78jIueErOvuHbtB+qlPLNH58413/3vmOh25ZoGjjnn/JMtFpHGu05INqvqeqm6XzXOISEPgbuAgVW2mqsuydJ4BIjLfv0xVb1HVXDzk7gQu9K73kyg7quo4VT0oS+mqV7z7PyvX6cg1Cxx1SES6A3sDChxVx+fO2jfGHOgANAG+yHVC6lAF9et6s6rI/h/qnAWOunU6MAl4AjjDv0JEuorICyKyRESWicj9vnWDRWS6iPwoIl+KyG7e8mrFEyLyhIjc7L0fICLzReQqEVkIPC4irUXk7945lnvvu/j2byMij4vI9976l7zl00TkSN92DUVkqYj0ib/A+G/p3vm/89L+lYjsH3RjRORwEflERFZ5RTI3hGy3LRArClshIm8FFaf4iz9E5EwR+beI3Old17cicmii6xaRpsBrQGeveOInEeksIjeIyFjfvkeJyBcissI75w6+dbNFZKiIfCYiK0XkzyLSJOS6SkTk9yIyR0QWi8hTItJSRBqLyE9AKfCpiHwTtL/nABH52ruGB0RE/NfvO9eBIjLDS9P9gPjWlXr3aamIzAIOj0tnSxF5TEQWeL/Xm0WkNJX7nAoR2cr7nS7z0jBORFp5664Qkefjth8pIvemmLb3ReQeEfkBuCFBGrYWkX9592epiPzZt0699f6/i59EZI2IqG+7s8T9zy4XkddFpCLKfch7qmqvOnoBM4ELgL7ARqCDt7wU+BS4B2iK+zb9S2/dCcB3wO64f/CtgQpvnQJb+47/BHCz934AsAm4HWgMlAFtgeOAcqA58Czwkm//V4A/A62BhsC+3vIrgT/7tjsa+DzkGgcA87332wHzgM7e5+7AVgn22xn3ZaY3sAg4JmTb7t61Nwj67C17BzjHe3+md78He/f6t8D3gCS57p+vxXfcG4Cx3vttgdXAgd5+V3q/40be+tnAR0BnoA0wHTg/5JrO8vbtCTQDXgCe9q2v9rsO2F+BvwOtgG7AEuAQ3/X/23vfDlgFHO+l+TLv7yR2r84HZgBdvTS/HXevXwIexv2dbuFd33mp3OcEaff/rrb27mdjoD3wLnCvt66Td79beZ8bAIuBvimmbRNwkbdfWYL0TACG4f4Wf/5fTPR7AMYBE7z3x3i/yx28c/0e+CDXz59MvnKegPryAn7p/VO18z7PAC7z3u/p/aM3CNjvdeCSkGMmCxwbgCYJ0tQHWO697wRsBloHbNcZ+BFo4X1+Drgy5JgDqAocW3v/2AcADSPer3uBe0LWdSd64JjpW1fubd8xyXX/fC2+ZTdQFTj+APzFt64EF+QHeJ9nA6f61o8ARoVc05vABb7P23l/L7FrTCVw+B9wfwGu9l1/LHCcDkzybSfAfN+9egtfcAMOit1bXBHhenwPXeA3wNvJ7nOS3/XPv6uAdccAn/g+vwYM9t4fAXzpvU8lbXNT/Nt7ChgNdEn2P+ctuwqYEju3l8az4/4u1uB94SuGlxVV1Z0zgImqutT7PJ6q4qquwBxV3RSwX1cgUfFEIktUdV3sg4iUi8jDXnHIKty3uVZedr4r8IOqLo8/iKp+D7wPHOcVGxyK+4aVkKrOBC7FPWwXi8gzItI5aFsR+T8ReVtcMdpK3DffdtEuN6GFvnSt8d42I8F1p6AzMMd33M24HNaWQefFPTyapXIs733sYZ2qVM7V2UsjAOqebPPC1selqQKXS1ngFc2twH3D3yIoDXH3OSUisoX3d/Kd9zc6lup/B08Cp3rvTwWejpA2/3UlciUuoH7kFUOelSC9hwKX4HLHa31p+ZMvHT94x9sy+CiFxwJHHRCRMuBEYF8RWSiuzuEyYBcR2QX3B91Ngivs5gFbhRx6De5bXUzHuPXxQx//DvdN9v9UtQWwTyyJ3nnaxMqTA8T+YU8APlTV70K2q54A1fGq+kvcP5Piis6CjAdeBrqqaktgFL6y9yRWez8T3Yswia47/v7F+x53XQB4dQpdcbmOqKodC1fctAlXZJdJC3BpBKqlOXC9l46Yebhv9e1UtZX3aqGqvTKYvltx97239zd6KtX/Dl4CeovITrgcR+wLTCppS/b7dBupLlTVwaraGTgPeFACmjqLyHa4/4sTVdUflObhisha+V5lqvpBKucvBBY46sYxQCWwI654qA+u/PM9XNHBR7h/2NtEpKmINBGRvbx9HwWGikhfcbb2VbRNBU7xKjQPAfZNko7mwFpcpXIb4PrYClVdgMtiPyiuEr2hiOzj2/clYDfct6unUrloEdlORH4lrunxOu/clQnS9oOqrhOR/sApqZzDS/sS3MP6VO9enEV4sI3fN9F1LwLaikjLkN3/AhwuIvuLayL8O9zDK50HxATgMhHpISLNgFtw9UpBudDaeAXoJSLHel9ULqZ6kP0LcLGIdBGR1sDVsRXevZoI3CUiLcRV6G8lIsn+7qJoDvyE+xvdErjCv9LLQT+H+6LxkarOzXTaROQEqWo0shwXcCrjtmkB/BX4var+O+4Qo4BrRKSXt21LETkhajrymQWOunEG8LiqzvW+zSxU1YXA/cBA3DeqI3F1AnNxZc4nAajqs8Bw3D/Kj7gHeBvvuJd4+63wjvNSknTci6skX4pr3fWPuPWn4crVZ+DqJi6NrfCy4c8DPXAVt6loDNzmnW8hrtjg2pBtLwD+KCI/AtfhHmBRDMY9ZJYBvYj28A68blWdgXugz/KKHaoVs6nqV7hvxCNx13gkcKSqboiYdoAxuGKXd4FvcYH2ojSOk5BXVHoC7veyDNgGVwwZ8wiuXu1T4GNq/q5PBxoBX+Ieqs/h6oky5UbcF5SVuCAX9Lf2JK4hxdNxyzOVtt2B/4hrzfYyro7x27htdsPl3u/2t64CUNUXcTnrZ7zitmm44t2iEWtVYkxSInIdsK2qnpp0Y2OyRES64YJ8R1Vdlev01EfWCcakxCvaOhv37dyYnBCREuBy4BkLGrljRVUmKREZjKvwe01V3811ekzhiess53/tHeEYTXF9UA7EVz+XZnpGhaRnVG2OW19YUZUxxphILMdhjDEmkqKq42jXrp12794918kwxpiCMWXKlKWq2j7KPkUVOLp3787kyZNznQxjjCkYIjIn+VbVWVGVMcaYSCxwGGOMicQChzHGmEiKqo4jyMaNG5k/fz7r1q1LvnEBa9KkCV26dKFhw4a5TooxpsgVfeCYP38+zZs3p3v37riBQIuPqrJs2TLmz59Pjx49cp0cY0yRK/qiqnXr1tG2bduiDRoAIkLbtm2LPldljMmgceOge3f6uhlJIyn6HAdQ1EEjpj5cozEmQ8aNg3PPhTVrkm8boOhzHMYYY+IMG5Z20AALHFm3YsUKHnzwwcj7HXbYYaxYsSLzCTLGmLlza7W7BY54XrkfJSXu57ikU2snFBY4KivDJsJzXn31VVq1alWrcxtjTKBu3ZJvk4AFDr9Yud+cOaDqfp57bq2Cx9VXX80333xDnz592H333dlvv/045ZRT2HnnnQE45phj6Nu3L7169WL06NE/79e9e3eWLl3K7Nmz2WGHHRg8eDC9evXioIMOYu3atbW+VGNMPTZ8ONSi6X69qBz/2aWXwtSp4esnTYL166svW7MGzj4bHnkkeJ8+feDee0MPedtttzFt2jSmTp3KO++8w+GHH860adN+bjY7ZswY2rRpw9q1a9l999057rjjaNu2bbVjfP3110yYMIFHHnmEE088keeff55TT7VJ+IwxaTr6aGjUCBo0gDS+iFqOwy8+aCRbnob+/ftX62tx3333scsuu7DHHnswb948vv766xr79OjRgz59+gDQt29fZs+enbH0GGPqoUcegdWr4a23mAJTou5ev3IcCXIGgKvTmBMwUGRFBbzzTkaS0LRp05/fv/POO7zxxht8+OGHlJeXM2DAgMC+GI0bN/75fWlpqRVVGWPSt3493Hkn7Lcf7LFHWoewHIff8OFQXl59WXm5W56m5s2b8+OPPwauW7lyJa1bt6a8vJwZM2YwadKktM9jjCkQGW6AE9lTT8H338O116Z9iPqV40hm4ED3c9gw11ytWzcXNGLL09C2bVv22msvdtppJ8rKyujQocPP6w455BBGjRpF79692W677dgjzehvjCkQ8R3vYg1woFbPmZRt2gS33w677w7775/2YYpqzvF+/fpp/ERO06dPZ4cddshRiupWfbpWY/LCuHHRvmgmKg6vi7rLCRPglFPgxRfhmGMAEJEpqtovymGsqMoYY9KRTvP9sI53teyQl1Lx1+bNcMstsOOOcNRRtTqdBQ5jjElH0LAda9a45WE6dw5e3rVr+ulINYC98gpMmwbXXOMCTC1Y4DDGmHREzT2sXQu+FpLV9ItUUlRdKgFM1RWjde8OJ5+c/rk8FjiMMSYdvoYu1QQN56HqOhLPmuU6IldUgIjbdu+94YUX4P77o6dh9ergOhNwAUzV5Tw6doT//AdWrYI//zn6eeJY4DDGmKhUoXXr4HW77+7W+916q6uYvuUWuOceVxG+ebN76L/1lqtzuPhiGDIkvK7CX4/RrRucdhr07Jk4jZ06waBBsHixW/bDD7UeRsk7thbNq2/fvhrvyy+/rLGsWNWnazUmo8aOVa2oUBVxP8eOTbz9a6+pgurpp1ft162b6l57ueWHH+4+i6i2b++WDRyounlz8PHWrFHdbju3nf9VXu7SMnasex+/focdVK+/vua6sjLVc85Rbdy45j7g0uwBJmvEZ23OH/aZfOVj4Fi+fLk+8MADae17zz336OrVq1PePtfXakxBCnooxx7YQSorVXv3Vu3ZU3X9+prrDjyw5oO6pET18ccTp6Nr1+CHfHl58gAQFvhEgvcT+fm06QQOK6qKk+lOnenOxwFw7733sqYWk60YY3zC/rmvuSZa66jx4+Gzz1xlc6NG1deVlMBXX9XcZ/NmuOGGxOmbPz94+Zo14ePlxSriBw6sKv6aPbuqL0nY8Om1HFY957mETL5qm+OI+sUjFSeddJI2adJEd9llFx06dKiOGDFC+/XrpzvvvLNed911qqr6008/6WGHHaa9e/fWXr166TPPPKN/+tOftGHDhrrTTjvpgAEDUjqX5ThMUYlafJTsWPH/3KWlqh06BH8jj/tW/rN161xadtvN5S6CpPAtP1BFRXiuItG6qNcd91AjjRxHvRpyJAejqlcbVn3ixIk899xzfPTRR6gqRx11FO+++y5Lliyhc+fOvPLKK4Abw6ply5bcfffdvP3227Rr1y7CVRpTBDI9NEdQk9XKStfKqGVLWLmy5j5B38ofesil5dFHw/tCdOsW3NIp2bf84cNrzgPuHysv0bowWRhGCaxVVTXZHlV94sSJTJw4kV133ZXddtuNGTNm8PXXX7PzzjvzxhtvcNVVV/Hee+/RsmXLzJzQmEKVTue6RML6VqxbBw88UHNwU4C99qr+eeVKuPlmOOAA9wqT7mCpAwfC6NFVTXUrKtzngQMTr0smrBirNqJmUfL5VduiqnRzg4l8++232qtXL1VVvfzyy3XUqFGB2y1btkyffvpp3WuvvfTGG2/00lOhS5YsSflcVlRlika6xT1httgi8T+3v1isWzfV/v3d+quvrmoJ9fvfu2WTJyc/XyaL2bIMqxyvnSyMql5tWPWDDz6YMWPG8NNPPwHw3XffsXjxYr7//nvKy8s59dRTGTp0KB9//HGNfY0pWFFbnKxcWbPSOSadSt1ly2DDBvdN3c//z+3/Vj5nDnzwAZx3Htx2m5u3oksXl9soL4cZM5KfMxvf8vNIvarjSCYbxYH+YdUPPfRQTjnlFPbcc08AmjVrxtixY5k5cyZXXHEFJSUlNGzYkIceegiAc889l0MPPZROnTrx9ttv1/byjKl7UesqFi6EQw91D/pGjdxPv9693dDgDVJ8dKm6861eDTfd5CorU/nnLi119RmLFsFLL1UtX7OmbodBz1dRsyj5/MrHfhx1qT5dqykQUcp///c/1R49VJs2Vf3HP6oX93Ttqrrffvpzp7cuXVIrBnrkEbfPiBHZT3+BwlpVGWPySlil9Jw5bviL116ryuKLuKKgt96C/v3ddvHf6s87z1UK+48TlgP46iu45BI3YdHvfpfZ9Nd2GPQCZ3UcxpjsSVQnscUWcMYZVcOBb97siqG+/jp8n9dfr7lszRrXiQ+q16f07u2C0ZNPpj+MeLY60BW4ehE4XG6suNWHazQF6I9/rLmsvLyqormysvq6desSN7kN+6Y/bx7suafrdBULRBs2uED0zjtpJz8rLWaKQNEHjiZNmrBs2bKifrCqKsuWLaNJkya5Toox1TVt6n62b1+9/8GwYeC1LqwhUTFQ2Df9Zs3go49qdrpavz79vh9Qu/4TRazo5xzfuHEj8+fPZ926dTlKVd1o0qQJXbp0oWHDhrlOijFVDjjAFT3NmuVaKvmlM/92fCstcDmA0aPdMONBzzMRVwxmAqUz53jRV443bNiQHj165DoZxtQ///sfvPmmK5aKDxqQfIiNIInazA8blt5QHyayoi+qMsYEyPQw0EFGj3b9Lc4+O3h9usVAYZ3rrD6izmQ1cIjIISLylYjMFJGrA9a3FpEXReQzEflIRHZKdV9jTJpixT2xSuRYk9ZkwSNKsFm7Fh5/HH79azdtaZhM9rC2+oi6E7XjR6ovoBT4BugJNAI+BXaM2+YO4Hrv/fbAm6nuG/QK6gBojImTTqe2qHMOPPWU2+att7JxBSaDyLOxqvoDM1V1lqpuAJ4Bjo7bZkfgTQBVnQF0F5EOKe5rjIlq+fLgegBI3Jop6mRHo0bBttvCgAFpJdPkt2wGji2Beb7P871lfp8CxwKISH+gAuiS4r54+50rIpNFZPKSJUsylHRjioC/aKmiAk4/HbbeOnz75s3dPBP+4qgxY2DECNdPIkhQsPnsMzdI4Pnn1xxY0BSFbAaOoL+Y+LZytwGtRWQqcBHwCbApxX3dQtXRqtpPVfu1b9++Fsk1pojE12PMnQtPPw2dOsEtt9SsRC4tdZMaDR5cve7j7LPhqqsgrI9QUP3FqFFu+zPOyPx1mbyQzcAxH+jq+9wF+N6/gaquUtVBqtoHOB1oD3ybyr7GmASCJkIC+PFHV+wUX4n85JMuqATp2NHlRIImO1q7Fr79tvrxn34aTjoJ2rTJzLWYvJPNwPFfYBsR6SEijYCTgZf9G4hIK28dwDnAu6q6KpV9jTEJJBqaA4JbMy1cGLzPokXBLZZuvdW9P/BAWLDAbTt+vOsRfv75mb4ik0ey1gFQVTeJyIXA67hWUmNU9QsROd9bPwrYAXhKRCqBL4GzE+2brbQaU3S6dg0OHok6wyWbKzs2hanffvu50Wd3390FkfnzoWFD+OYb2GOP9NNv8lpW+3Go6ququq2qbqWqw71lo7yggap+qKrbqOr2qnqsqi5PtK8xJkWnnFJzWbLOcOl0oPu//4OLL4bvvnNBA2DjxtT6hZiCZT3HjSlGU6ZAixYut5BqZ7h0O9CNH19zWaKmuqbgFf0gh8bUO598Arvt5ubLvuqq7J+vpMQGFyxg6QxyaDkOY4rNHXe4PhnnnVc357PJjuodCxzG5LsoY0R9+y385S8uaLRqVTfps8EF6x0LHMbks6gDEt5zjwswl1xSd2m0wQXrHavjMCafRZnsaOlSVzx00kluZFpjUmB1HMYUm7COfEHLH3jA9eS+4orspsnUexY4jMlnXbsGL48fI2rNGhg5Eo48EnbcMfvpMvWaBQ5j8tnxxwcvX74c/vnPqs+PPw7LlsGVV9ZNuky9ZoHDmHy1fj28/DJ07ly9I9+f/gTbbAOHHgrnnOOWXXghNGoUPteGMRmUtbGqjDG1dN99MHMm/OMfcPDB1dedeSb88pfw2GNVyzZscC2uwFo0mayyHIcx6UjUtyJKv4swCxfCTTfBEUfUDBrghhNZubLmchvqw9QBy3EYE1Wsb0VsvotY34qYsHVRcgHDhsG6dXDXXeHbRJmVz5gMshyHKX6ZyAH4BU2StGYNDBrkZr0LWnfttamnY/JkV9l9ySVu3u4wNtSHyRHrAGiKW3zuANxwGLXp2Rw2qF8ypaVQWZk4Haqu7mLmTPjf/6Bly/DjZePaTL1jHQCNiReWO6hNPUDYFKsVFe4VRKR60IilI9Z81p8b+eAD1x8jUdAAG+rD5IzlOExxy/SQ3xs3wvbbw6xZ1ZfHvulDcC4gaP7vmI4d3XAhmzbVPJ4FAZNlluMwJl6m6wGuu84FjYsuCv6mH5YLCMuJtG4NK1ZUDxpgraNMXrPAYYrbiSfWXNakSXpDfr/xBtx+Owwe7PpYzJ7tci2zZ1fPGQwcWHNd2NDjI0e6jn5BrHWUyVMWOEzxmjPHdZDr0sWN+STiiq622SZ6EdCSJXDaaa6Y6t57o6clUX2EtY4yBcYChylO69e7cZ42bYK33nLf3jdvdrPjff65yz2kavNm11N7+XJ45pmaOYdUBeVEwCZCMgXHAocpTpdd5vpDPPGEy2HEDBniWi9deWXyyvFYS6fSUnj1VTj5ZOjdO/NptdZRpsBY4DDFwd+ctV07eOghNy/Fr39dfbvGjeHmm+GTT2DChMTHi828F/Pss7XvPBgmLDdiTB6ywGEKX/z0qsuWuQCy887B2//mN7DrrlXDegTJRv8PY4qEBQ5T+IIe8ps3wx/+ELx9SQmMGOECzYMPBm8TNjy5tXQyxgKHKQJRpleNOeAAN+rszTe7Su+YdevcHBdhrKWTMRY4TBGIn0Y1JtlD/vbbXdCI1Y106eKa2z72GBx9tLV0MiaEBQ5T2N55x/W8jpfKQ37aNNdiatUqVzfy3XeuiOryy+Gll6ylkzEhLHCYwuJvPdW+vSty6tHDTaca9SE/bFjNgQcBnn/e/bSWTsYEsomcTOGIH0Z86VIXQC65xC2/+OJox0unbsQYYzkOU0DCWk/dckt6x7OhPoxJiwUOk3+CZsp7773MN5G1oT6MSYsVVZn8EjSf92mnucprkeC5NdLNIcTqLIYNc8GnWzcXNKwuw5iELMdh8ktQcZQqtG0Ljz6a+RyCVYAbE5kFDpNfwoqdfvgBzjrLmsgakwesqMrkly22gEWLai6PFUfFZtkzxuSM5ThM/lixws3pLVJ9uVVYG5NXLHCY/HHBBbByJdxwgxVHGZPHsho4ROQQEflKRGaKyNUB61uKyN9E5FMR+UJEBvnWzRaRz0VkqohMzmY6TR4YN87Nj3HDDXDddVZhbUweSxo4ROQIEYkcYESkFHgAOBTYEfiNiOwYt9kQ4EtV3QUYANwlIo186/dT1T6q2i/q+U0BmT3b5TZ++Uu45ppcp8YYk0QqAeFk4GsRGSEiO0Q4dn9gpqrOUtUNwDPA0XHbKNBcRARoBvwAbIpwDlPoNm2CU091759+2g06aIzJa0kDh6qeCuwKfAM8LiIfisi5ItI8ya5bAvN8n+d7y/zuB3YAvgc+By5R1dhE0ApMFJEpInJu2Em8tEwWkclLlixJdjkmHUE9uTN1zIYN4f33XXFU9+61P64xJutSKoJS1VXA87hcQyfg18DHInJRgt0kYFl8t9+DgalAZ6APcL+ItPDW7aWqu+GKuoaIyD4haRutqv1UtV/79u1TuRwTRfy0rHPmuM+1CR5B83k/+WT25vM2xmRUKnUcR4rIi8BbQEOgv6oeCuwCDE2w63ygq+9zF1zOwm8Q8II6M4Fvge0BVPV77+di4EVc0Zepa9mYe9vm8zamoKWS4zgBuEdVe6vqHd6DHFVdA5yVYL//AtuISA+vwvtk4OW4beYC+wOISAdgO2CWiDSNFYWJSFPgIGBahOsymZJo6PFNm9IrxrLhzI0paKn0HL8eWBD7ICJlQAdVna2qb4btpKqbRORC4HWgFBijql+IyPne+lHATcATIvI5rmjrKlVdKiI9gRddnTkNgPGq+o/0LtHUSteuwQ/02PhRa9a4AAJVxVgQ3oR23Tpo1AjWr6+5zoYzN6YgpBI4ngV+4ftc6S3bPdmOqvoq8GrcslG+99/jchPx+83CFYWZXNq82c3DHR84ystdgHj44aqgERMrcgoKHJWVbvn69S54bNhQ/ZjWO9yYgpBKUVUDrzktAN77Rgm2N8VA1fWt+OADOOGEmj2577nH5R6ChOVQLrgAXngB7r0Xxoyx3uHGFKhUchxLROQoVX0ZQESOBpZmN1kmp1ThiitcjuLqq+HWW4O369YteHKlkhJ46SVYvbpqrosWLdxwItdc46Z6BQsUxhSoVALH+cA4EbkfVw8xDzg9q6kyuTFunHvQx4LBgQcmnpZ1+PDqky4BNGkC7drBr3/tOvNVVrrlK1e6z716ZS/9xpg6kUoHwG9UdQ/csCE7quovvKazppgE9a14/30YPz58n4EDa86P8eijMGsWtGpVFTRiKiutya0xRUA0aCrO+I1EDgd6AU1iy1T1j1lMV1r69eunkyfbeIhp6d49uNiposKNJRVVSUnwNK8irtLdGJMXRGRK1PEAU+kAOAo4CbgIV1R1AlCRVgpN/sp034qwprXW5NaYgpdKq6pfqOrpwHJVvRHYk+o9wk2+idopb8ECt22QdB/0w4dnfn5wY0xeSCVwxNpcrhGRzsBGoEf2kmRqJerYUuvWuYrskhJXse1Xmwd9UP2HNbk1piikEjj+JiKtgDuAj4HZwIQspsnURpRxoFRdUPnPf+CZZ1zFdiYf9AMH2oRMxhShhM1xvQmc3lTVFcDzIvJ3oImqrqyLxJk0RKmruOsuNwfGjTfCsce6ZfZwN8YkkTDH4c2NcZfv83oLGnkurE6iUSP473+r139ccQX07w9/+EOdJtEYU9hSKaqaKCLHebP0mXx3002uqMmvYUMXOPr3hzPOqKr/APj888R9NYwxJk4qgeNy3KCG60VklYj8KCKrspwuA+kNWd6smQsK7dtX1VU8/jjMn++G/YjvlLd2rXXKM8ZEknTIEVVNNkWsyYZY66hYRXcqQ5YDPPigGwp91ixoEPfr/fHH4H1sHgxjTASpdADcJ+hVF4mr19KZJe+rr+CNN+C882oGDbBOecaYjEhlkMMrfO+b4KZwnQL8KispMk46PbkfesjVZ5xzTvD6oEEJrVOeMSaiVIqqjvR/FpGuwIispcjAxo1Vw5DHC8sdrF4NTzwBxx8PHToEbxMr4ooNdd6tmwsa1gTXGBNBKpXj8eYDO2U6IfVWfAX4PffA3ntXDUPu16BBeO5g/Hi3zwUXJD6fdcozxtRS0hyHiIwEYsOclgB9gE+zmKb6I6gC/PLLoazM9eTetKkqd1Be7rbbdtuax1F1leI77wx77VW312CMqXdSqePwj1O+CZigqu9nKT31S1AFOECbNnDSSe59LEewciXstJPrh/Hxx9XHlZo0CaZOhVGjavbhMMaYDEslcDwHrFPVSgARKRWRclUNeOKZSMIqur//vuayli3hscfg4INdT+877qha98AD0Ly5FTsZY+pEKnUcbwJlvs9lwBvZSU4R89dldO0Khx0Wvm1YBfhBB7mmtnfd5WbnA1i8GJ591uVEmjXLeLKNMSZeKoGjiar+FPvgvS9PsL2JFz/U+fz58NprsM020Ycyv+MO1xv8zDNdS6oxY2DDhuSV4sYYkyGpBI7VIrJb7IOI9AXWZi9JRSisLmP9+uhDmTdv7oYQmTkTOnaEa66Bxo1dvYcxxtSBVOo4LgWeFZFYwXsn3FSyJlWJOvMNHBi9buK771zT3J+8jOD69akNR2KMMRmQSgfA/4rI9sB2uDnHZ6jqxqynrFg8/njVSLTx0h3qY9gw11TXLzYciQUOY0yWpTJW1RCgqapOU9XPgWYiYgXqqfjTn+Css1wz2rKy6utqM9RHOsORGGNMhqRSxzHYmwEQAFVdDgzOWoryXaKhzv3rWrWCSy+F446DyZPhkUcyNy2rDVZojMmhVAJHiX8SJxEpBRplL0l5ICw4xLeOig11Pm5czXWxIUOOPtpVXmdyqI/hw12Oxc8GKzTG1BHRsPL32AYidwDdgVG4oUfOB+aq6tCspy6ifv366eTJk5NvmEj8MCDgHvwXX+zqK5YurblPixbu56qA+a0qKlygyLRx42ywQmNMrYnIFFXtF2mfFAJHCXAucACucvwToJOqDkk3odmSkcDRvbvLNWSKiMtlGGNMHkoncCQtqlLVzcAkYBbQD9gfmJ5WCgtBWAWzCHTqFLyuosK9gli9gzGmyIQGDhHZVkSuE5HpwP3APABV3U9V76+rBNa5RBXPd9wRXrdg9Q7GmHoiUY5jBi53caSq/lJVRwKVdZOsHLryyprLYgFg4EDXGiqodVSidcYYU0QSdQA8DjgZeFtE/gE8g6vjKG6xkWk7d4YFC2pWPCfq6Z1OL3BjjCkwoYFDVV8EXhSRpsAxwGVABxF5CHhRVSfWTRLr0E8/uQmRjj0Wnn8+16kxxpi8lErl+GpVHaeqRwBdgKnA1dlOWE489hgsXw5XXJHrlBhjTN6KNOe4qv6gqg+r6q9S2V5EDhGRr0RkpojUCDYi0lJE/iYin4rIFyIyKNV9M27jRrj7bjff9x57ZP10xhhTqCIFjii8HuYPAIcCOwK/EZEd4zYbAnypqrsAA4C7RKRRivtm1rPPuqa4ltswxpiEshY4gP7ATFWdpaobcJXrR8dto0Bzb0iTZsAPuHnNU9k3c1RhxAjYYQc4/PCsncYYY4pBNgPHlnh9PzzzvWV+9wM7AN8DnwOXeB0OU9kXABE5V0Qmi8jkJUuWpJfSN96ATz+FoUPd+FTGGGNCZfMpGdR0N358k4Nxle2dgT7A/SLSIsV93ULV0araT1X7tW/fPr2UjhjheoVbU1pjjEkqm4FjPtDV97kLLmfhNwh4QZ2ZwLfA9inumxkff+xyHJdc4gYzNMYYk1A2A8d/gW1EpIeINMJ1Jnw5bpu5uN7piEgH3CyDs1LcN7lU5s7o29f19G7TJvLhjTGmPkplzvG0qOomEbkQeB0oBcao6hcicr63fhRwE/CEiHyOK566SlWXAgTtGykB8cOjx+bOAFi3Di66CNaujSXWTbpUXm7FVcYYk0TSYdULSbVh1dMZHj1bc2cYY0yeysqw6gUrnfm3bc5uY4xJqngDR4cOwctt7gxjjKmVrNVx5NQnn7g5v0Vc/UWMf36M+Olhbe4MY4xJSfHlOKZNgwMPhPbt4Z57bO4MY4zJsOKqHBfRySUl0KIFTJ4MW22V6yQZY0xes8pxgM2bXXPbSZNynRJjjClKxRc4wAWOYcNynQpjjClKxRk4wJrWGmNMlhRv4LCmtcYYkxXFGTi8prWJhqoyxhiTnuLrx1FR4YIGA0OHqrJWt8YYk77iynH07evGmho4kGHDqvfvA/f52mvde8uNGGNMeoorcPiE1Y3PnQunnAKDB7tciGpVbiQWPLIRVCxQGWOKRVEFjilTXEnVkUdWH2nEr3FjmDChakT1mDVr4MorYeTIxEElkbDgEBvhPZ1jGmNMvimqnuMi/RTcsOrbb+8e0P4AUV7uRhY57bTwwBImNuL6uHGui8jcua7h1vDhrs4kfvoPgLIyuOACeOQRWLUq/JjGGJMr1nPcZ+1a98AOGo4qrKVuu3bhx5szB4YODc85XHNNzTqVtWvhrruCgwZYVxNjTGEq2hyHiBt9JEhQ7iCWGxk2LHj+p5KS8OOVlkJlZViaoEsXmDev5jrLcRhjcs1yHD6J+v8lGhx3+HAXRPzKy2HMGLdtkMpKaNUqPB233lrzmOAq6Y0xptAUZeBIZWqNgQPdt/3Nm39uwfvz8qCgcsYZ4cGoogLuvz844MTqQPzH7NIFttwS7rsPPvggvWu0VlrGmJxR1aJ5QV+tqFAdO1azYuxY1fJyVVfD4V7l5VXnGztWtaJCVUSTpmPBAtWtt1YtK1Pt2DG1fVJNhzHGpAqYrBGftUVVx9GvXz+dPHlyVs8R1qoqHffdB5deWnOSwmRzSnXvHlwPY3Umxpio0qnjsMCRQ+kGgJKS4ObEiRoEGGNMEKscLzCJereHmTQpvJLeBgQ2xtQFCxw5FPagLy2F99+vuXz8eBgwwPU3adKk+rqSErjppown0RhjarDAkUNBTX8bN3ZNe/feGw4/3AWXkhK3bOBA2GMP+PJLePTRqlZabdu6IqpFi5Kf01pjGWNqywJHDgU1/X3sMfj2W/jVr+DVV13HQVVYudLlRAYNcoHC35x4yRI45hj4/e9h+vTw89mYWcaYTLDK8TwVteJ80SLo1Qu22soVczUImGmlWzfrwW6Mqc4qx4tI1IrzDh1cJ8SPPnLjY8V7443goJHomMYYE8QCR54KqzhP1HLqpJPguONcP5POnV09RteurkL9wAODcyHgiq0uvtgVm1n9hzEmGQsceSpszKxEQ6mIwP77u7GzFixwAWH+fPjXv+CII1xgiD9mWZnbZ+RIOO+8zNZ/WEW8MUUqalfzfH717du3Vl3v802UIUxiKiqqD0USe1VUJD5mp06J90sn7TYsijH5DxtypHgqx9OVbq/yTPdGD6vc79bNLc/k0C3GmPSlUzkeUuptClXswRy0PBv7hUlUub/ffm5U4A0b3LJYsRhY8DCmEFgdR5FJp24kbD8R1zckHR07Bi8vK3N1LrGgEbNmjcuBGGPynwWOIpNokqoo+3Xo4Jb/9a/Ri6pWrQou9iovd9P5hkmlWbBVuBuTB6JWiuTzq9gqx3Nt5EhXqX3DDanvs3mz6nHHqZaWqg4bFlwRn6wCP4xVuBuTeaRROW45DhNqyBA38+ENN8AWW6T2Lf+uu+D55+H22+Hmm4NnWQwqFgP47W8Tp2fYsOrzxEN2i7gsd2NMMAscJpQI7Luv+7lkSfL+HW+/DVddBccfD5dfHn7c+GKxLbeEli3hoYdg8eLgfTZuDK68h+z0fLdxvYwJl9XAISKHiMhXIjJTRK4OWH+FiEz1XtNEpFJE2njrZovI5966+t3GNoduvLFmfcWaNXDtte69/1v5AQe4upExY8LnDInxD9I4f74bEmXxYjdY47p11bedMwf22Sf8WNmYhyRZ7sZyI6Zei1q2leoLKAW+AXoCjYBPgR0TbH8k8Jbv82ygXZRzWh1H5okE10eA6k47qTZsWH1Zkybp1zk8+6w7xp57qnbr5s7dvr2rx2jeXPXCC2vWcTRsmPx86XSkTHTdxx6r2rix1bWY4kAadRzZDBx7Aq/7Pl8DXJNg+/HAYN9nCxx5IKwiu1mzmkGjtr3NVVWPP77m8URU77rLrfcHgVgQeeut8OOlW6HepUvwtTVoEB5QanPdxuRKOoEjm0VVWwL+8Vjne8tqEJFy4BDged9iBSaKyBQROTfsJCJyrohMFpHJS5YsyUCyjV9Yv5BRo2DTpuB9alPn8NFHNZepwn33uff+Iq5Fi2D77eE3v4GFC4OPl06F+saN0LRpzeXl5fDEE+HFcPV5lGEruqtnokaaVF/ACcCjvs+nASNDtj0J+Fvcss7ezy1wxVz7JDun5TiyI6yoJ91mtYmEFRGJBG//+eeqZWWq++2numlT7Y+nqjpkiNvmt7+tu+suZNZMurBRqEVVwIvAKQmOdQMwNNk5LXDUrWw8MNJ5KI8Z47Zp2bLqIf/AA+7BH1astOWWwcd65BG3fujQaNcNqtdfn/ZlFzQLpIUt3wJHA2AW0IOqyvFeAdu1BH4AmvqWNQWa+95/AByS7JwWOOpeOhXPyY4XNRiNHes6HAblKg44wOVI4tc1b676r39VP87777t6m4MOCs69hF13ly6qHTqotm2r+tVX6V93Ju9jXVm4MDw4J8rVmfyRV4HDpYfDgP/hWlcN85adD5zv2+ZM4Jm4/Xp6geZT4IvYvsleFjiKQ9SHaNg33s6dg493xx2q227rKroHDapaV1KiusUWqj/8ED3NM2e6FmA9e6o++GBw+sOuq1CKevzp79ZN9bTTVFu0CA8cluMoDHkXOOr6ZYGjfkqnHmP5ctXevWvuU5vmxJMmuRxLSUnNIPDb3wYHh5EjXcAJSn+3bu64+ZAbCSue22kn1TvvLIzAZ4KlEzhsPg5T8MLm/qiocC2wwlRUBLeESrZfIlts4XrZxyspSW9ek333hUmTYP36qmXl5akNXJlJqc6vEttm1Cg3o6TJf+nMx2FDjpiCl+5Q8vPmBS+vTbPapUuDlycKGmFD0JeXw7vvVg8aUPfjcy1ZEj7cS+wexppJx763NW6cnfSZ/GCBwxS8dIeSDxuqpDZDmITtW1oavLyiAu68MzjwjR4dfp45c9zw9ZnsPxE0PtegQdC1a/g+8de7225u2QsvpJ8OUwCilm3l88vqOEwU2aiUDjtmWB1HsorzsIp/cMOexPdkr036w85VVqY6YkTq9+rSS13aVq1KLx2mbmGV4xY4TDTZqHhO1Hoq6rnCAtEf/6jatGnwgz7d1kzJGhmkmv733nP7PfNMeunIhXxogJArFjgscJgiFPZQSzQQ42efRX8Yho3PFTUQbdrk+rWceGLkS82JfGoOnYsAZoHDAoepRxIVY0Fws+CwB9Hcua7fS/wx0n2AnneeyxGtXVurS4wsnQdvNnq+ZzJ3me3gYYHDAoepR8IeNA89pNq6deoPwy+/dLmNFi3Cp/uN6vXX3flefrkWFxhRsgdv0MN8+vTwwJtuz/d0A0Cuhm6xwGGBw9Qz6RRjzZ1bfb+SEhc0Pvkkc+lav161VSvVM89ML/1R91m/3vX6D7re9u1VR4+u+TCPDVMTdq9SeWAHpSXdAJCojmn9+uwVY1ngsMBhjKomLsYSqTm2V1lZ5otETjtNtU0b1Q0bgtenk0MI2qe0NHg8slReLVq4HFq6E4TF7xc/wVeUHEyyFnTxv7NMFWNZ4LDAYYyqhj+U773XDfBYF0UiL77ojvvGG8Hrwx6ULVqonn12zYdww4ZuSJigfZo2DR+6pUOH5A9zf5AqK3PnSjZgZbI6pqj398ILa+5TVqZ6+eVu4rRM5or8LHBY4DDmZ1GLsTI9mu3q1VV9WIIkKk6L+hJJnIOJUnz03XeumO0Xv0g8SnKi9AeN63X//eHHmj3bTQuw1VZV0yZn4neWSn2LBQ4LHMYkVZeVsMcdp9qxo2plZfXlc+YED4UPVQ/OdL7NZ2oE4qefdtvceWfw+lmzwqcR9td1iKh26uRyMHvt5eoq4m3apLr33i4n+M03wecL+521aZPo7qf2u7bAYYHDmKTqstnnuHHu+O+/X7Vs0SI3rH1ZWc2ip2Q5hLZt0097lMrlzZtVjz7aFZdNn1593YcfumKx8vKaxWlhaZkwwa0fMqTmuptucuueeipx2uOvO9bc+uKL3b7x1/bVV+GB1p9TscBhgcOYlNRVR7OVK1UbNVL93e/c5xUrVHfd1QWNf/87vRxCXaV9wQL3jb5nz6pcULt2LqfRs6fqjBnR0jJ0qLuOMWOqln3wgct5nXKKC1aJxJ/rqadUL7usehCJDyphr9atVZ94Ihag+6pa4DDG5JNddnEPRxH3Db2kRPW115Lvlw/DgMTmn49/KD/0UPRjbdyouv/+7l507FjVuq1dOxdQ09WmTXBwaNnSTaEcllOpKg6MHjhsdFxjTNaMGwfTp0NlpXtErV8PDRrAsmXJ940N1b55s/tZl/OPxPztbzWXbd4Mt90W/VgNGsDxx7v9Fy5096OyEn78Ef7+9/TTuHx58PJVq+CCC2qOHP3UU27eGNX0z2kTORljsibdSbbyRUlJ8ANWJL2JubJxP9I5ZvXr6ofqZIlyTstxGGOyJmxSrNpMllWXMj1nSzbuRzoTmdVmzhmwwGGMyaJsTJZVl9KdXTJMNu5HOhOZBV1XFBY4jDFZk+kHb11Ld3bJMNm6H1Hrg/zXlQ6r4zDGZNW4cW6O9Llz3Tfr4cNzU9GdL/LtfojIFFXtF2kfCxzGGFN/pRM4rKjKGGNMJBY4jDHGRGKBwxhjTCQWOIwxxkRigcMYY0wkRdWqSkSWAAGd74taO2BprhORR+x+1GT3pDq7H9Vtp6rNo+zQIFspyQVVbZ/rNNQ1EZkctSldMbP7UZPdk+rsflQnIpH7MFhRlTHGmEgscBhjjInEAkfhG53rBOQZux812T2pzu5HdZHvR1FVjhtjjMk+y3EYY4yJxAKHMcaYSCxwFBARGSMii0Vkmm9ZGxH5p4h87f1sncs01iUR6Soib4vIdBH5QkQu8ZbXy3siIk1E5CMR+dS7Hzd6y+vl/YgRkVIR+URE/u59ru/3Y7aIfC4iU2NNcaPeEwscheUJ4JC4ZVcDb6rqNsCb3uf6YhPwO1XdAdgDGCIiO1J/78l64FequgvQBzhERPag/t6PmEuA6b7P9f1+AOynqn18/Vki3RMLHAVEVd8FfohbfDTwpPf+SeCYukxTLqnqAlX92Hv/I+7hsCX19J6o85P3saH3Uurp/QAQkS7A4cCjvsX19n4kEOmeWOAofB1UdQG4BymwRY7TkxMi0h3YFfgP9fieeMUyU4HFwD9VtV7fD+Be4Epgs29Zfb4f4L5MTBSRKSJyrrcs0j0pqiFHTP0kIs2A54FLVXWViOQ6STmjqpVAHxFpBbwoIjvlOEk5IyJHAItVdYqIDMhxcvLJXqr6vYhsAfxTRGZEPYDlOArfIhHpBOD9XJzj9NQpEWmICxrjVPUFb3G9vicAqroCeAdXJ1Zf78dewFEiMht4BviViIyl/t4PAFT1e+/nYuBFoD8R74kFjsL3MnCG9/4M4K85TEudEpe1eAyYrqp3+1bVy3siIu29nAYiUgYcAMygnt4PVb1GVbuoanfgZOAtVT2Veno/AESkqYg0j70HDgKmEfGeWM/xAiIiE4ABuGGhFwHXAy8BfwG6AXOBE1Q1vgK9KInIL4H3gM+pKsO+FlfPUe/uiYj0xlVsluK+FP5FVf8oIm2ph/fDzyuqGqqqR9Tn+yEiPXG5DHBVFeNVdXjUe2KBwxhjTCRWVGWMMSYSCxzGGGMiscBhjDEmEgscxhhjIrHAYYwxJhILHMYYYyKxwGEKkoh09w8v71v+RxE5IGD5gNiw2gHrZotIuwym7QYRGZqp49WGiHQWkedynQ5TXGysKlNUVPW6XKch20SkgapuSmVbb3iJ47OcJFPPWI7DFLJSEXnEm7RoooiUicgTInI8gIgcIiIzROTfwLGxnUSkrbf9JyLyMCC+dad6kyFNFZGHRaTUW/6TiAz3JkmaJCIdUkmgiAwWkf96+z0vIuUi0lxEvvXG2UJEWni5noYispWI/MMbufQ9Edne2+YJEblbRN4Gbg85175euqd619bcnzMTkUd965eIyPXe8iu8NH4m3uRPxiRigcMUsm2AB1S1F7ACOC62QkSaAI8ARwJ7Ax19+10P/FtVd8WN0dPN22cH4CTc6KF9gEpgoLdPU2CSN0nSu8DgFNP4gqru7u03HTjbmzvkHdw8EeDGUXpeVTcCo4GLVLUvMBR40HesbYEDVPV3IecaCgzx0r43sNa/UlXP8dYdDSwDnhCRg3D3sT9u8qe+IrJPitdm6ikLHKaQfauqU733U4DuvnXbe+u/Vjeuzljfun1in1X1FWC5t3x/oC/wX29Oi/2Bnt66DUCsjiT+XIns5OUcPscFoV7e8keBQd77QcDj3vDwvwCe9c7/MNDJd6xnvWHTw7wP3C0iFwOtgoqzvID6LHChqs7BDXJ3EPAJ8DHuvm2T4rWZesrqOEwhW+97XwmUxa1PNBBb0DoBnlTVawLWbdSqgd0qSf1/5wngGFX9VETOxA1Siaq+7xUj7QuUquo0EWkBrPByBUFWJzqRqt4mIq8AhwGTvEYC6+I2G4XLBb3hfRbgVlV9OMXrMcZyHKZozQB6iMhW3uff+Na9i1cEJSKHAq295W8Cx3sT3CAibUSkopbpaA4s8OozBsatewqYADwOoKqrgG9F5ATv/CIiu6R6IhHZSlU/V9Xbgcm43IN//RCguare5lv8OnCWl9tBRLaMXb8xYSxwmKKkquuAc4FXvMrxOb7VNwL7iMjHuGKaud4+XwK/x02r+RnwT6oXFaXjD7hh3v+JC2Z+43BBa4Jv2UDgbBH5FPgCVx+RqktFZJq371rgtbj1Q4GdfRXk56vqRGA88KFXnPYcLtgZE8qGVTcmR7zWX0er6mm5TosxUVgdhzE5ICIjgUNx9RHGFBTLcRiTJhEZBpwQt/hZVR2e5fMOAi6JW/y+qg7J5nmNibHAYYwxJhKrHDfGGBOJBQ5jjDGRWOAwxhgTiQUOY4wxkfw/TuzTF7a12v4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hidden_layer_size, train_acc, 'r-o', label = 'train')\n",
    "plt.plot(hidden_layer_size, test_acc, 'b-o', label = 'test')\n",
    "plt.xlim([np.min(hidden_layer_size), np.max(hidden_layer_size)])\n",
    "plt.title('Accuracy is a function of hidden_layer_size')\n",
    "plt.xlabel('hidden_layer_size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "print(f\"Train: максимальное значение accuracy = {np.max(train_acc)}, число нейронов = {hidden_layer_size[np.argmax(train_acc)]}\")\n",
    "print(f\"Test: максимальное значение accuracy = {np.max(test_acc)}, число нейронов = {hidden_layer_size[np.argmax(test_acc)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cb09ab",
   "metadata": {},
   "source": [
    "Наблюдается переобучение. Попробуем изменить количество обучающих и тестовых данных, параметры, алгоритм(sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b88881e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17769, 178)\n",
      "(17769,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15103, 2666)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data_norm.drop(['enrollee_id', 'target'], axis=1) \n",
    "y = data_norm['target']\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15, random_state = 42)\n",
    "\n",
    "N_train, _ = X_train.shape \n",
    "N_test,  _ = X_test.shape \n",
    "\n",
    "N_train, N_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da01aa6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.56336563\n",
      "Iteration 2, loss = 0.55214717\n",
      "Iteration 3, loss = 0.53906811\n",
      "Iteration 4, loss = 0.51993071\n",
      "Iteration 5, loss = 0.50102790\n",
      "Iteration 6, loss = 0.48682198\n",
      "Iteration 7, loss = 0.47747301\n",
      "Iteration 8, loss = 0.47216379\n",
      "Iteration 9, loss = 0.46840168\n",
      "Iteration 10, loss = 0.46576524\n",
      "Iteration 11, loss = 0.46385347\n",
      "Iteration 12, loss = 0.46249992\n",
      "Iteration 13, loss = 0.46147164\n",
      "Iteration 14, loss = 0.46037522\n",
      "Iteration 15, loss = 0.45955266\n",
      "Iteration 16, loss = 0.45901935\n",
      "Iteration 17, loss = 0.45829605\n",
      "Iteration 18, loss = 0.45782196\n",
      "Iteration 19, loss = 0.45741451\n",
      "Iteration 20, loss = 0.45705472\n",
      "Iteration 21, loss = 0.45659675\n",
      "Iteration 22, loss = 0.45646474\n",
      "Iteration 23, loss = 0.45612734\n",
      "Iteration 24, loss = 0.45568003\n",
      "Iteration 25, loss = 0.45580380\n",
      "Iteration 26, loss = 0.45546393\n",
      "Iteration 27, loss = 0.45530084\n",
      "Iteration 28, loss = 0.45528213\n",
      "Iteration 29, loss = 0.45503180\n",
      "Iteration 30, loss = 0.45481661\n",
      "Iteration 31, loss = 0.45468690\n",
      "Iteration 32, loss = 0.45468645\n",
      "Iteration 33, loss = 0.45451518\n",
      "Iteration 34, loss = 0.45462197\n",
      "Iteration 35, loss = 0.45441137\n",
      "Iteration 36, loss = 0.45412932\n",
      "Iteration 37, loss = 0.45434710\n",
      "Iteration 38, loss = 0.45416412\n",
      "Iteration 39, loss = 0.45401517\n",
      "Iteration 40, loss = 0.45394602\n",
      "Iteration 41, loss = 0.45375708\n",
      "Iteration 42, loss = 0.45388884\n",
      "Iteration 43, loss = 0.45397868\n",
      "Iteration 44, loss = 0.45409264\n",
      "Iteration 45, loss = 0.45394746\n",
      "Iteration 46, loss = 0.45363864\n",
      "Iteration 47, loss = 0.45342622\n",
      "Iteration 48, loss = 0.45360345\n",
      "Iteration 49, loss = 0.45380032\n",
      "Iteration 50, loss = 0.45337855\n",
      "Iteration 51, loss = 0.45325997\n",
      "Iteration 52, loss = 0.45322680\n",
      "Iteration 53, loss = 0.45314284\n",
      "Iteration 54, loss = 0.45328557\n",
      "Iteration 55, loss = 0.45295689\n",
      "Iteration 56, loss = 0.45305845\n",
      "Iteration 57, loss = 0.45294954\n",
      "Iteration 58, loss = 0.45321634\n",
      "Iteration 59, loss = 0.45305918\n",
      "Iteration 60, loss = 0.45304878\n",
      "Iteration 61, loss = 0.45308417\n",
      "Iteration 62, loss = 0.45256437\n",
      "Iteration 63, loss = 0.45273468\n",
      "Iteration 64, loss = 0.45300744\n",
      "Iteration 65, loss = 0.45290823\n",
      "Iteration 66, loss = 0.45290297\n",
      "Iteration 67, loss = 0.45258923\n",
      "Iteration 68, loss = 0.45252022\n",
      "Iteration 69, loss = 0.45288616\n",
      "Iteration 70, loss = 0.45261499\n",
      "Iteration 71, loss = 0.45260779\n",
      "Iteration 72, loss = 0.45261778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:01<00:33,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 73, loss = 0.45290311\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57312145\n",
      "Iteration 2, loss = 0.52353448\n",
      "Iteration 3, loss = 0.49173670\n",
      "Iteration 4, loss = 0.47663868\n",
      "Iteration 5, loss = 0.46977461\n",
      "Iteration 6, loss = 0.46514921\n",
      "Iteration 7, loss = 0.46185519\n",
      "Iteration 8, loss = 0.45920942\n",
      "Iteration 9, loss = 0.45728913\n",
      "Iteration 10, loss = 0.45524415\n",
      "Iteration 11, loss = 0.45385570\n",
      "Iteration 12, loss = 0.45297764\n",
      "Iteration 13, loss = 0.45226579\n",
      "Iteration 14, loss = 0.45134989\n",
      "Iteration 15, loss = 0.45046906\n",
      "Iteration 16, loss = 0.44989769\n",
      "Iteration 17, loss = 0.44934685\n",
      "Iteration 18, loss = 0.44877089\n",
      "Iteration 19, loss = 0.44837485\n",
      "Iteration 20, loss = 0.44781495\n",
      "Iteration 21, loss = 0.44771352\n",
      "Iteration 22, loss = 0.44726890\n",
      "Iteration 23, loss = 0.44695962\n",
      "Iteration 24, loss = 0.44668291\n",
      "Iteration 25, loss = 0.44646829\n",
      "Iteration 26, loss = 0.44624123\n",
      "Iteration 27, loss = 0.44573621\n",
      "Iteration 28, loss = 0.44560514\n",
      "Iteration 29, loss = 0.44543882\n",
      "Iteration 30, loss = 0.44508368\n",
      "Iteration 31, loss = 0.44507668\n",
      "Iteration 32, loss = 0.44477867\n",
      "Iteration 33, loss = 0.44487323\n",
      "Iteration 34, loss = 0.44447027\n",
      "Iteration 35, loss = 0.44441479\n",
      "Iteration 36, loss = 0.44392419\n",
      "Iteration 37, loss = 0.44422244\n",
      "Iteration 38, loss = 0.44414406\n",
      "Iteration 39, loss = 0.44378210\n",
      "Iteration 40, loss = 0.44357893\n",
      "Iteration 41, loss = 0.44328616\n",
      "Iteration 42, loss = 0.44344155\n",
      "Iteration 43, loss = 0.44322188\n",
      "Iteration 44, loss = 0.44314566\n",
      "Iteration 45, loss = 0.44286359\n",
      "Iteration 46, loss = 0.44281518\n",
      "Iteration 47, loss = 0.44265170\n",
      "Iteration 48, loss = 0.44257049\n",
      "Iteration 49, loss = 0.44246987\n",
      "Iteration 50, loss = 0.44250038\n",
      "Iteration 51, loss = 0.44245088\n",
      "Iteration 52, loss = 0.44224428\n",
      "Iteration 53, loss = 0.44215462\n",
      "Iteration 54, loss = 0.44186022\n",
      "Iteration 55, loss = 0.44179551\n",
      "Iteration 56, loss = 0.44176318\n",
      "Iteration 57, loss = 0.44160025\n",
      "Iteration 58, loss = 0.44202025\n",
      "Iteration 59, loss = 0.44144854\n",
      "Iteration 60, loss = 0.44145930\n",
      "Iteration 61, loss = 0.44145654\n",
      "Iteration 62, loss = 0.44113296\n",
      "Iteration 63, loss = 0.44125258\n",
      "Iteration 64, loss = 0.44134406\n",
      "Iteration 65, loss = 0.44132839\n",
      "Iteration 66, loss = 0.44113197\n",
      "Iteration 67, loss = 0.44101298\n",
      "Iteration 68, loss = 0.44098704\n",
      "Iteration 69, loss = 0.44078781\n",
      "Iteration 70, loss = 0.44078860\n",
      "Iteration 71, loss = 0.44093524\n",
      "Iteration 72, loss = 0.44039494\n",
      "Iteration 73, loss = 0.44052070\n",
      "Iteration 74, loss = 0.44088807\n",
      "Iteration 75, loss = 0.44041905\n",
      "Iteration 76, loss = 0.44024391\n",
      "Iteration 77, loss = 0.44053197\n",
      "Iteration 78, loss = 0.44010045\n",
      "Iteration 79, loss = 0.44026372\n",
      "Iteration 80, loss = 0.44005481\n",
      "Iteration 81, loss = 0.43983925\n",
      "Iteration 82, loss = 0.43991370\n",
      "Iteration 83, loss = 0.44003324\n",
      "Iteration 84, loss = 0.44000859\n",
      "Iteration 85, loss = 0.44004454\n",
      "Iteration 86, loss = 0.43953572\n",
      "Iteration 87, loss = 0.43957488\n",
      "Iteration 88, loss = 0.43964035\n",
      "Iteration 89, loss = 0.43962200\n",
      "Iteration 90, loss = 0.43939215\n",
      "Iteration 91, loss = 0.43957485\n",
      "Iteration 92, loss = 0.43936454\n",
      "Iteration 93, loss = 0.43934852\n",
      "Iteration 94, loss = 0.43949388\n",
      "Iteration 95, loss = 0.43902305\n",
      "Iteration 96, loss = 0.43920317\n",
      "Iteration 97, loss = 0.43882714\n",
      "Iteration 98, loss = 0.43888863\n",
      "Iteration 99, loss = 0.43917232\n",
      "Iteration 100, loss = 0.43895438\n",
      "Iteration 101, loss = 0.43869606\n",
      "Iteration 102, loss = 0.43870401\n",
      "Iteration 103, loss = 0.43866159\n",
      "Iteration 104, loss = 0.43890301\n",
      "Iteration 105, loss = 0.43868755\n",
      "Iteration 106, loss = 0.43870756\n",
      "Iteration 107, loss = 0.43853310\n",
      "Iteration 108, loss = 0.43874738\n",
      "Iteration 109, loss = 0.43847268\n",
      "Iteration 110, loss = 0.43853076\n",
      "Iteration 111, loss = 0.43829255\n",
      "Iteration 112, loss = 0.43851267\n",
      "Iteration 113, loss = 0.43821043\n",
      "Iteration 114, loss = 0.43835288\n",
      "Iteration 115, loss = 0.43846277\n",
      "Iteration 116, loss = 0.43855094\n",
      "Iteration 117, loss = 0.43798058\n",
      "Iteration 118, loss = 0.43810152\n",
      "Iteration 119, loss = 0.43800567\n",
      "Iteration 120, loss = 0.43798014\n",
      "Iteration 121, loss = 0.43790770\n",
      "Iteration 122, loss = 0.43820435\n",
      "Iteration 123, loss = 0.43779153\n",
      "Iteration 124, loss = 0.43799572\n",
      "Iteration 125, loss = 0.43803932\n",
      "Iteration 126, loss = 0.43799203\n",
      "Iteration 127, loss = 0.43819678\n",
      "Iteration 128, loss = 0.43770647\n",
      "Iteration 129, loss = 0.43756562\n",
      "Iteration 130, loss = 0.43838156\n",
      "Iteration 131, loss = 0.43745010\n",
      "Iteration 132, loss = 0.43783465\n",
      "Iteration 133, loss = 0.43784639\n",
      "Iteration 134, loss = 0.43780973\n",
      "Iteration 135, loss = 0.43742191\n",
      "Iteration 136, loss = 0.43763298\n",
      "Iteration 137, loss = 0.43741161\n",
      "Iteration 138, loss = 0.43747052\n",
      "Iteration 139, loss = 0.43735165\n",
      "Iteration 140, loss = 0.43751237\n",
      "Iteration 141, loss = 0.43745459\n",
      "Iteration 142, loss = 0.43721261\n",
      "Iteration 143, loss = 0.43735799\n",
      "Iteration 144, loss = 0.43739499\n",
      "Iteration 145, loss = 0.43719460\n",
      "Iteration 146, loss = 0.43725140\n",
      "Iteration 147, loss = 0.43701498\n",
      "Iteration 148, loss = 0.43705844\n",
      "Iteration 149, loss = 0.43700162\n",
      "Iteration 150, loss = 0.43718364\n",
      "Iteration 151, loss = 0.43701341\n",
      "Iteration 152, loss = 0.43714414\n",
      "Iteration 153, loss = 0.43704034\n",
      "Iteration 154, loss = 0.43685035\n",
      "Iteration 155, loss = 0.43664209\n",
      "Iteration 156, loss = 0.43729781\n",
      "Iteration 157, loss = 0.43692663\n",
      "Iteration 158, loss = 0.43687467\n",
      "Iteration 159, loss = 0.43723969\n",
      "Iteration 160, loss = 0.43748983\n",
      "Iteration 161, loss = 0.43672810\n",
      "Iteration 162, loss = 0.43668115\n",
      "Iteration 163, loss = 0.43660422\n",
      "Iteration 164, loss = 0.43668981\n",
      "Iteration 165, loss = 0.43649354\n",
      "Iteration 166, loss = 0.43647902\n",
      "Iteration 167, loss = 0.43661401\n",
      "Iteration 168, loss = 0.43696896\n",
      "Iteration 169, loss = 0.43666347\n",
      "Iteration 170, loss = 0.43660843\n",
      "Iteration 171, loss = 0.43668520\n",
      "Iteration 172, loss = 0.43698633\n",
      "Iteration 173, loss = 0.43668576\n",
      "Iteration 174, loss = 0.43657130\n",
      "Iteration 175, loss = 0.43635567\n",
      "Iteration 176, loss = 0.43634144\n",
      "Iteration 177, loss = 0.43649592\n",
      "Iteration 178, loss = 0.43662907\n",
      "Iteration 179, loss = 0.43608373\n",
      "Iteration 180, loss = 0.43618900\n",
      "Iteration 181, loss = 0.43649260\n",
      "Iteration 182, loss = 0.43662046\n",
      "Iteration 183, loss = 0.43637572\n",
      "Iteration 184, loss = 0.43646310\n",
      "Iteration 185, loss = 0.43609221\n",
      "Iteration 186, loss = 0.43609807\n",
      "Iteration 187, loss = 0.43626687\n",
      "Iteration 188, loss = 0.43597279\n",
      "Iteration 189, loss = 0.43632144\n",
      "Iteration 190, loss = 0.43589547\n",
      "Iteration 191, loss = 0.43602985\n",
      "Iteration 192, loss = 0.43584494\n",
      "Iteration 193, loss = 0.43603923\n",
      "Iteration 194, loss = 0.43606507\n",
      "Iteration 195, loss = 0.43576435\n",
      "Iteration 196, loss = 0.43627037\n",
      "Iteration 197, loss = 0.43563265\n",
      "Iteration 198, loss = 0.43587217\n",
      "Iteration 199, loss = 0.43567278\n",
      "Iteration 200, loss = 0.43558057\n",
      "Iteration 201, loss = 0.43576039\n",
      "Iteration 202, loss = 0.43614826\n",
      "Iteration 203, loss = 0.43601083\n",
      "Iteration 204, loss = 0.43567352\n",
      "Iteration 205, loss = 0.43550374\n",
      "Iteration 206, loss = 0.43604552\n",
      "Iteration 207, loss = 0.43530024\n",
      "Iteration 208, loss = 0.43558381\n",
      "Iteration 209, loss = 0.43567524\n",
      "Iteration 210, loss = 0.43582961\n",
      "Iteration 211, loss = 0.43565491\n",
      "Iteration 212, loss = 0.43543108\n",
      "Iteration 213, loss = 0.43575750\n",
      "Iteration 214, loss = 0.43583205\n",
      "Iteration 215, loss = 0.43530111\n",
      "Iteration 216, loss = 0.43532900\n",
      "Iteration 217, loss = 0.43548950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:09<01:33,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 218, loss = 0.43546348\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54495963\n",
      "Iteration 2, loss = 0.49389703\n",
      "Iteration 3, loss = 0.47746988\n",
      "Iteration 4, loss = 0.46790459\n",
      "Iteration 5, loss = 0.46198955\n",
      "Iteration 6, loss = 0.45820068\n",
      "Iteration 7, loss = 0.45607627\n",
      "Iteration 8, loss = 0.45427592\n",
      "Iteration 9, loss = 0.45306841\n",
      "Iteration 10, loss = 0.45230026\n",
      "Iteration 11, loss = 0.45148528\n",
      "Iteration 12, loss = 0.45082432\n",
      "Iteration 13, loss = 0.45030984\n",
      "Iteration 14, loss = 0.45000713\n",
      "Iteration 15, loss = 0.44924474\n",
      "Iteration 16, loss = 0.44900967\n",
      "Iteration 17, loss = 0.44864757\n",
      "Iteration 18, loss = 0.44823480\n",
      "Iteration 19, loss = 0.44793333\n",
      "Iteration 20, loss = 0.44780759\n",
      "Iteration 21, loss = 0.44726804\n",
      "Iteration 22, loss = 0.44729462\n",
      "Iteration 23, loss = 0.44684238\n",
      "Iteration 24, loss = 0.44639899\n",
      "Iteration 25, loss = 0.44637409\n",
      "Iteration 26, loss = 0.44619262\n",
      "Iteration 27, loss = 0.44598932\n",
      "Iteration 28, loss = 0.44597004\n",
      "Iteration 29, loss = 0.44574072\n",
      "Iteration 30, loss = 0.44543326\n",
      "Iteration 31, loss = 0.44534438\n",
      "Iteration 32, loss = 0.44515925\n",
      "Iteration 33, loss = 0.44501361\n",
      "Iteration 34, loss = 0.44507112\n",
      "Iteration 35, loss = 0.44451947\n",
      "Iteration 36, loss = 0.44470934\n",
      "Iteration 37, loss = 0.44437383\n",
      "Iteration 38, loss = 0.44442216\n",
      "Iteration 39, loss = 0.44414649\n",
      "Iteration 40, loss = 0.44403630\n",
      "Iteration 41, loss = 0.44374035\n",
      "Iteration 42, loss = 0.44368123\n",
      "Iteration 43, loss = 0.44330412\n",
      "Iteration 44, loss = 0.44335584\n",
      "Iteration 45, loss = 0.44318791\n",
      "Iteration 46, loss = 0.44314947\n",
      "Iteration 47, loss = 0.44295174\n",
      "Iteration 48, loss = 0.44287854\n",
      "Iteration 49, loss = 0.44277791\n",
      "Iteration 50, loss = 0.44238276\n",
      "Iteration 51, loss = 0.44205186\n",
      "Iteration 52, loss = 0.44225303\n",
      "Iteration 53, loss = 0.44199903\n",
      "Iteration 54, loss = 0.44173202\n",
      "Iteration 55, loss = 0.44143475\n",
      "Iteration 56, loss = 0.44174778\n",
      "Iteration 57, loss = 0.44170070\n",
      "Iteration 58, loss = 0.44119831\n",
      "Iteration 59, loss = 0.44095122\n",
      "Iteration 60, loss = 0.44079337\n",
      "Iteration 61, loss = 0.44072203\n",
      "Iteration 62, loss = 0.44066026\n",
      "Iteration 63, loss = 0.44073588\n",
      "Iteration 64, loss = 0.44058765\n",
      "Iteration 65, loss = 0.44044297\n",
      "Iteration 66, loss = 0.43989371\n",
      "Iteration 67, loss = 0.43980944\n",
      "Iteration 68, loss = 0.43973610\n",
      "Iteration 69, loss = 0.44004093\n",
      "Iteration 70, loss = 0.43939087\n",
      "Iteration 71, loss = 0.43901223\n",
      "Iteration 72, loss = 0.43920733\n",
      "Iteration 73, loss = 0.43870398\n",
      "Iteration 74, loss = 0.43872645\n",
      "Iteration 75, loss = 0.43854600\n",
      "Iteration 76, loss = 0.43808435\n",
      "Iteration 77, loss = 0.43807054\n",
      "Iteration 78, loss = 0.43796776\n",
      "Iteration 79, loss = 0.43773100\n",
      "Iteration 80, loss = 0.43739497\n",
      "Iteration 81, loss = 0.43746644\n",
      "Iteration 82, loss = 0.43709514\n",
      "Iteration 83, loss = 0.43719734\n",
      "Iteration 84, loss = 0.43698793\n",
      "Iteration 85, loss = 0.43646340\n",
      "Iteration 86, loss = 0.43640156\n",
      "Iteration 87, loss = 0.43632274\n",
      "Iteration 88, loss = 0.43583912\n",
      "Iteration 89, loss = 0.43575883\n",
      "Iteration 90, loss = 0.43565973\n",
      "Iteration 91, loss = 0.43572715\n",
      "Iteration 92, loss = 0.43541295\n",
      "Iteration 93, loss = 0.43557732\n",
      "Iteration 94, loss = 0.43479730\n",
      "Iteration 95, loss = 0.43475630\n",
      "Iteration 96, loss = 0.43473793\n",
      "Iteration 97, loss = 0.43437481\n",
      "Iteration 98, loss = 0.43423699\n",
      "Iteration 99, loss = 0.43420755\n",
      "Iteration 100, loss = 0.43429533\n",
      "Iteration 101, loss = 0.43399823\n",
      "Iteration 102, loss = 0.43356356\n",
      "Iteration 103, loss = 0.43406119\n",
      "Iteration 104, loss = 0.43357220\n",
      "Iteration 105, loss = 0.43339515\n",
      "Iteration 106, loss = 0.43299719\n",
      "Iteration 107, loss = 0.43310956\n",
      "Iteration 108, loss = 0.43294841\n",
      "Iteration 109, loss = 0.43277968\n",
      "Iteration 110, loss = 0.43290588\n",
      "Iteration 111, loss = 0.43238127\n",
      "Iteration 112, loss = 0.43211772\n",
      "Iteration 113, loss = 0.43205419\n",
      "Iteration 114, loss = 0.43188784\n",
      "Iteration 115, loss = 0.43170787\n",
      "Iteration 116, loss = 0.43177154\n",
      "Iteration 117, loss = 0.43133934\n",
      "Iteration 118, loss = 0.43130867\n",
      "Iteration 119, loss = 0.43140398\n",
      "Iteration 120, loss = 0.43117177\n",
      "Iteration 121, loss = 0.43103554\n",
      "Iteration 122, loss = 0.43096257\n",
      "Iteration 123, loss = 0.43048357\n",
      "Iteration 124, loss = 0.43091045\n",
      "Iteration 125, loss = 0.43040733\n",
      "Iteration 126, loss = 0.43018297\n",
      "Iteration 127, loss = 0.43017378\n",
      "Iteration 128, loss = 0.43036448\n",
      "Iteration 129, loss = 0.43014047\n",
      "Iteration 130, loss = 0.43011387\n",
      "Iteration 131, loss = 0.42974571\n",
      "Iteration 132, loss = 0.42986439\n",
      "Iteration 133, loss = 0.43001115\n",
      "Iteration 134, loss = 0.42937846\n",
      "Iteration 135, loss = 0.43022681\n",
      "Iteration 136, loss = 0.42925036\n",
      "Iteration 137, loss = 0.42903585\n",
      "Iteration 138, loss = 0.42909983\n",
      "Iteration 139, loss = 0.42909228\n",
      "Iteration 140, loss = 0.42877819\n",
      "Iteration 141, loss = 0.42876265\n",
      "Iteration 142, loss = 0.42870040\n",
      "Iteration 143, loss = 0.42895789\n",
      "Iteration 144, loss = 0.42850064\n",
      "Iteration 145, loss = 0.42815158\n",
      "Iteration 146, loss = 0.42817297\n",
      "Iteration 147, loss = 0.42821765\n",
      "Iteration 148, loss = 0.42796700\n",
      "Iteration 149, loss = 0.42815610\n",
      "Iteration 150, loss = 0.42794941\n",
      "Iteration 151, loss = 0.42763496\n",
      "Iteration 152, loss = 0.42796522\n",
      "Iteration 153, loss = 0.42762936\n",
      "Iteration 154, loss = 0.42762590\n",
      "Iteration 155, loss = 0.42752676\n",
      "Iteration 156, loss = 0.42740689\n",
      "Iteration 157, loss = 0.42727619\n",
      "Iteration 158, loss = 0.42749941\n",
      "Iteration 159, loss = 0.42715257\n",
      "Iteration 160, loss = 0.42711146\n",
      "Iteration 161, loss = 0.42728209\n",
      "Iteration 162, loss = 0.42684334\n",
      "Iteration 163, loss = 0.42699676\n",
      "Iteration 164, loss = 0.42680784\n",
      "Iteration 165, loss = 0.42714153\n",
      "Iteration 166, loss = 0.42661056\n",
      "Iteration 167, loss = 0.42660145\n",
      "Iteration 168, loss = 0.42666667\n",
      "Iteration 169, loss = 0.42645882\n",
      "Iteration 170, loss = 0.42636217\n",
      "Iteration 171, loss = 0.42632048\n",
      "Iteration 172, loss = 0.42635139\n",
      "Iteration 173, loss = 0.42634695\n",
      "Iteration 174, loss = 0.42667701\n",
      "Iteration 175, loss = 0.42634518\n",
      "Iteration 176, loss = 0.42571171\n",
      "Iteration 177, loss = 0.42594483\n",
      "Iteration 178, loss = 0.42572472\n",
      "Iteration 179, loss = 0.42585616\n",
      "Iteration 180, loss = 0.42610473\n",
      "Iteration 181, loss = 0.42568666\n",
      "Iteration 182, loss = 0.42543332\n",
      "Iteration 183, loss = 0.42567582\n",
      "Iteration 184, loss = 0.42534252\n",
      "Iteration 185, loss = 0.42537492\n",
      "Iteration 186, loss = 0.42509614\n",
      "Iteration 187, loss = 0.42548924\n",
      "Iteration 188, loss = 0.42526777\n",
      "Iteration 189, loss = 0.42553169\n",
      "Iteration 190, loss = 0.42500378\n",
      "Iteration 191, loss = 0.42533344\n",
      "Iteration 192, loss = 0.42501480\n",
      "Iteration 193, loss = 0.42522635\n",
      "Iteration 194, loss = 0.42530987\n",
      "Iteration 195, loss = 0.42486268\n",
      "Iteration 196, loss = 0.42485107\n",
      "Iteration 197, loss = 0.42458639\n",
      "Iteration 198, loss = 0.42464349\n",
      "Iteration 199, loss = 0.42447441\n",
      "Iteration 200, loss = 0.42470298\n",
      "Iteration 201, loss = 0.42482054\n",
      "Iteration 202, loss = 0.42453425\n",
      "Iteration 203, loss = 0.42441165\n",
      "Iteration 204, loss = 0.42488574\n",
      "Iteration 205, loss = 0.42444314\n",
      "Iteration 206, loss = 0.42437759\n",
      "Iteration 207, loss = 0.42423630\n",
      "Iteration 208, loss = 0.42399070\n",
      "Iteration 209, loss = 0.42423383\n",
      "Iteration 210, loss = 0.42426144\n",
      "Iteration 211, loss = 0.42416343\n",
      "Iteration 212, loss = 0.42375138\n",
      "Iteration 213, loss = 0.42425095\n",
      "Iteration 214, loss = 0.42429588\n",
      "Iteration 215, loss = 0.42448886\n",
      "Iteration 216, loss = 0.42374596\n",
      "Iteration 217, loss = 0.42414521\n",
      "Iteration 218, loss = 0.42362221\n",
      "Iteration 219, loss = 0.42354056\n",
      "Iteration 220, loss = 0.42385487\n",
      "Iteration 221, loss = 0.42406965\n",
      "Iteration 222, loss = 0.42342192\n",
      "Iteration 223, loss = 0.42337382\n",
      "Iteration 224, loss = 0.42371117\n",
      "Iteration 225, loss = 0.42298940\n",
      "Iteration 226, loss = 0.42352633\n",
      "Iteration 227, loss = 0.42307269\n",
      "Iteration 228, loss = 0.42340351\n",
      "Iteration 229, loss = 0.42374349\n",
      "Iteration 230, loss = 0.42297855\n",
      "Iteration 231, loss = 0.42316057\n",
      "Iteration 232, loss = 0.42279770\n",
      "Iteration 233, loss = 0.42294485\n",
      "Iteration 234, loss = 0.42267454\n",
      "Iteration 235, loss = 0.42280711\n",
      "Iteration 236, loss = 0.42269992\n",
      "Iteration 237, loss = 0.42297019\n",
      "Iteration 238, loss = 0.42273050\n",
      "Iteration 239, loss = 0.42250697\n",
      "Iteration 240, loss = 0.42284397\n",
      "Iteration 241, loss = 0.42283439\n",
      "Iteration 242, loss = 0.42243090\n",
      "Iteration 243, loss = 0.42215193\n",
      "Iteration 244, loss = 0.42250485\n",
      "Iteration 245, loss = 0.42274528\n",
      "Iteration 246, loss = 0.42222531\n",
      "Iteration 247, loss = 0.42213414\n",
      "Iteration 248, loss = 0.42205796\n",
      "Iteration 249, loss = 0.42243631\n",
      "Iteration 250, loss = 0.42213544\n",
      "Iteration 251, loss = 0.42186596\n",
      "Iteration 252, loss = 0.42195173\n",
      "Iteration 253, loss = 0.42210149\n",
      "Iteration 254, loss = 0.42217887\n",
      "Iteration 255, loss = 0.42207759\n",
      "Iteration 256, loss = 0.42157291\n",
      "Iteration 257, loss = 0.42151512\n",
      "Iteration 258, loss = 0.42265068\n",
      "Iteration 259, loss = 0.42203420\n",
      "Iteration 260, loss = 0.42153538\n",
      "Iteration 261, loss = 0.42201160\n",
      "Iteration 262, loss = 0.42137775\n",
      "Iteration 263, loss = 0.42138272\n",
      "Iteration 264, loss = 0.42135661\n",
      "Iteration 265, loss = 0.42139086\n",
      "Iteration 266, loss = 0.42156347\n",
      "Iteration 267, loss = 0.42154947\n",
      "Iteration 268, loss = 0.42134340\n",
      "Iteration 269, loss = 0.42141338\n",
      "Iteration 270, loss = 0.42174069\n",
      "Iteration 271, loss = 0.42107819\n",
      "Iteration 272, loss = 0.42120190\n",
      "Iteration 273, loss = 0.42135599\n",
      "Iteration 274, loss = 0.42103678\n",
      "Iteration 275, loss = 0.42140205\n",
      "Iteration 276, loss = 0.42094245\n",
      "Iteration 277, loss = 0.42111820\n",
      "Iteration 278, loss = 0.42092557\n",
      "Iteration 279, loss = 0.42054491\n",
      "Iteration 280, loss = 0.42023840\n",
      "Iteration 281, loss = 0.42060987\n",
      "Iteration 282, loss = 0.42053296\n",
      "Iteration 283, loss = 0.42052315\n",
      "Iteration 284, loss = 0.42049011\n",
      "Iteration 285, loss = 0.42018242\n",
      "Iteration 286, loss = 0.42036532\n",
      "Iteration 287, loss = 0.42051085\n",
      "Iteration 288, loss = 0.42008423\n",
      "Iteration 289, loss = 0.42019344\n",
      "Iteration 290, loss = 0.42092263\n",
      "Iteration 291, loss = 0.41993940\n",
      "Iteration 292, loss = 0.42087218\n",
      "Iteration 293, loss = 0.42039188\n",
      "Iteration 294, loss = 0.42016051\n",
      "Iteration 295, loss = 0.41998684\n",
      "Iteration 296, loss = 0.41993650\n",
      "Iteration 297, loss = 0.42012610\n",
      "Iteration 298, loss = 0.42025145\n",
      "Iteration 299, loss = 0.41996782\n",
      "Iteration 300, loss = 0.42006662\n",
      "Iteration 301, loss = 0.41984611\n",
      "Iteration 302, loss = 0.41974118\n",
      "Iteration 303, loss = 0.42008995\n",
      "Iteration 304, loss = 0.41960461\n",
      "Iteration 305, loss = 0.41967012\n",
      "Iteration 306, loss = 0.41970390\n",
      "Iteration 307, loss = 0.41948722\n",
      "Iteration 308, loss = 0.41959211\n",
      "Iteration 309, loss = 0.41959194\n",
      "Iteration 310, loss = 0.41963583\n",
      "Iteration 311, loss = 0.41953248\n",
      "Iteration 312, loss = 0.41953247\n",
      "Iteration 313, loss = 0.41954665\n",
      "Iteration 314, loss = 0.41911237\n",
      "Iteration 315, loss = 0.41940010\n",
      "Iteration 316, loss = 0.41968469\n",
      "Iteration 317, loss = 0.41946693\n",
      "Iteration 318, loss = 0.41920786\n",
      "Iteration 319, loss = 0.41951009\n",
      "Iteration 320, loss = 0.41942864\n",
      "Iteration 321, loss = 0.41917587\n",
      "Iteration 322, loss = 0.41890795\n",
      "Iteration 323, loss = 0.41889775\n",
      "Iteration 324, loss = 0.41875401\n",
      "Iteration 325, loss = 0.41930234\n",
      "Iteration 326, loss = 0.41939816\n",
      "Iteration 327, loss = 0.41868784\n",
      "Iteration 328, loss = 0.41902473\n",
      "Iteration 329, loss = 0.41842072\n",
      "Iteration 330, loss = 0.41835227\n",
      "Iteration 331, loss = 0.41894251\n",
      "Iteration 332, loss = 0.41907484\n",
      "Iteration 333, loss = 0.41838470\n",
      "Iteration 334, loss = 0.41880607\n",
      "Iteration 335, loss = 0.41890959\n",
      "Iteration 336, loss = 0.41816287\n",
      "Iteration 337, loss = 0.41853970\n",
      "Iteration 338, loss = 0.41863980\n",
      "Iteration 339, loss = 0.41880624\n",
      "Iteration 340, loss = 0.41852180\n",
      "Iteration 341, loss = 0.41815348\n",
      "Iteration 342, loss = 0.41877682\n",
      "Iteration 343, loss = 0.41915818\n",
      "Iteration 344, loss = 0.41857073\n",
      "Iteration 345, loss = 0.41834256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:22<02:28,  8.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 346, loss = 0.41830646\n",
      "Iteration 347, loss = 0.41817515\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55015141\n",
      "Iteration 2, loss = 0.49529072\n",
      "Iteration 3, loss = 0.47480369\n",
      "Iteration 4, loss = 0.46624616\n",
      "Iteration 5, loss = 0.46100087\n",
      "Iteration 6, loss = 0.45789707\n",
      "Iteration 7, loss = 0.45586921\n",
      "Iteration 8, loss = 0.45428660\n",
      "Iteration 9, loss = 0.45342334\n",
      "Iteration 10, loss = 0.45259440\n",
      "Iteration 11, loss = 0.45182073\n",
      "Iteration 12, loss = 0.45104617\n",
      "Iteration 13, loss = 0.45037828\n",
      "Iteration 14, loss = 0.45000182\n",
      "Iteration 15, loss = 0.44920871\n",
      "Iteration 16, loss = 0.44919487\n",
      "Iteration 17, loss = 0.44853485\n",
      "Iteration 18, loss = 0.44809505\n",
      "Iteration 19, loss = 0.44812551\n",
      "Iteration 20, loss = 0.44756999\n",
      "Iteration 21, loss = 0.44720030\n",
      "Iteration 22, loss = 0.44686435\n",
      "Iteration 23, loss = 0.44667240\n",
      "Iteration 24, loss = 0.44620077\n",
      "Iteration 25, loss = 0.44600761\n",
      "Iteration 26, loss = 0.44553708\n",
      "Iteration 27, loss = 0.44541060\n",
      "Iteration 28, loss = 0.44527742\n",
      "Iteration 29, loss = 0.44519451\n",
      "Iteration 30, loss = 0.44481160\n",
      "Iteration 31, loss = 0.44438736\n",
      "Iteration 32, loss = 0.44465546\n",
      "Iteration 33, loss = 0.44409220\n",
      "Iteration 34, loss = 0.44399956\n",
      "Iteration 35, loss = 0.44356377\n",
      "Iteration 36, loss = 0.44363799\n",
      "Iteration 37, loss = 0.44334014\n",
      "Iteration 38, loss = 0.44345148\n",
      "Iteration 39, loss = 0.44294733\n",
      "Iteration 40, loss = 0.44308750\n",
      "Iteration 41, loss = 0.44260406\n",
      "Iteration 42, loss = 0.44257567\n",
      "Iteration 43, loss = 0.44251917\n",
      "Iteration 44, loss = 0.44207269\n",
      "Iteration 45, loss = 0.44231880\n",
      "Iteration 46, loss = 0.44193836\n",
      "Iteration 47, loss = 0.44159253\n",
      "Iteration 48, loss = 0.44149089\n",
      "Iteration 49, loss = 0.44139103\n",
      "Iteration 50, loss = 0.44102526\n",
      "Iteration 51, loss = 0.44088891\n",
      "Iteration 52, loss = 0.44085586\n",
      "Iteration 53, loss = 0.44037279\n",
      "Iteration 54, loss = 0.44012428\n",
      "Iteration 55, loss = 0.44007162\n",
      "Iteration 56, loss = 0.43977373\n",
      "Iteration 57, loss = 0.43977417\n",
      "Iteration 58, loss = 0.43940475\n",
      "Iteration 59, loss = 0.43937171\n",
      "Iteration 60, loss = 0.43936213\n",
      "Iteration 61, loss = 0.43905944\n",
      "Iteration 62, loss = 0.43869754\n",
      "Iteration 63, loss = 0.43912409\n",
      "Iteration 64, loss = 0.43837837\n",
      "Iteration 65, loss = 0.43843184\n",
      "Iteration 66, loss = 0.43802143\n",
      "Iteration 67, loss = 0.43833617\n",
      "Iteration 68, loss = 0.43792797\n",
      "Iteration 69, loss = 0.43784112\n",
      "Iteration 70, loss = 0.43720753\n",
      "Iteration 71, loss = 0.43718810\n",
      "Iteration 72, loss = 0.43705470\n",
      "Iteration 73, loss = 0.43691013\n",
      "Iteration 74, loss = 0.43678277\n",
      "Iteration 75, loss = 0.43684791\n",
      "Iteration 76, loss = 0.43653021\n",
      "Iteration 77, loss = 0.43637033\n",
      "Iteration 78, loss = 0.43617181\n",
      "Iteration 79, loss = 0.43585615\n",
      "Iteration 80, loss = 0.43600238\n",
      "Iteration 81, loss = 0.43544393\n",
      "Iteration 82, loss = 0.43559346\n",
      "Iteration 83, loss = 0.43578743\n",
      "Iteration 84, loss = 0.43554359\n",
      "Iteration 85, loss = 0.43496598\n",
      "Iteration 86, loss = 0.43474894\n",
      "Iteration 87, loss = 0.43470271\n",
      "Iteration 88, loss = 0.43453965\n",
      "Iteration 89, loss = 0.43466687\n",
      "Iteration 90, loss = 0.43406249\n",
      "Iteration 91, loss = 0.43394582\n",
      "Iteration 92, loss = 0.43406989\n",
      "Iteration 93, loss = 0.43388608\n",
      "Iteration 94, loss = 0.43350421\n",
      "Iteration 95, loss = 0.43324843\n",
      "Iteration 96, loss = 0.43294369\n",
      "Iteration 97, loss = 0.43298304\n",
      "Iteration 98, loss = 0.43287635\n",
      "Iteration 99, loss = 0.43272180\n",
      "Iteration 100, loss = 0.43247864\n",
      "Iteration 101, loss = 0.43273304\n",
      "Iteration 102, loss = 0.43203814\n",
      "Iteration 103, loss = 0.43217717\n",
      "Iteration 104, loss = 0.43211726\n",
      "Iteration 105, loss = 0.43173771\n",
      "Iteration 106, loss = 0.43196618\n",
      "Iteration 107, loss = 0.43143649\n",
      "Iteration 108, loss = 0.43152083\n",
      "Iteration 109, loss = 0.43119078\n",
      "Iteration 110, loss = 0.43109712\n",
      "Iteration 111, loss = 0.43094040\n",
      "Iteration 112, loss = 0.43096309\n",
      "Iteration 113, loss = 0.43156527\n",
      "Iteration 114, loss = 0.43090961\n",
      "Iteration 115, loss = 0.43068078\n",
      "Iteration 116, loss = 0.43027572\n",
      "Iteration 117, loss = 0.43016205\n",
      "Iteration 118, loss = 0.42996231\n",
      "Iteration 119, loss = 0.43021507\n",
      "Iteration 120, loss = 0.42986096\n",
      "Iteration 121, loss = 0.42962209\n",
      "Iteration 122, loss = 0.42953639\n",
      "Iteration 123, loss = 0.42961681\n",
      "Iteration 124, loss = 0.42922818\n",
      "Iteration 125, loss = 0.42926772\n",
      "Iteration 126, loss = 0.42912082\n",
      "Iteration 127, loss = 0.42936414\n",
      "Iteration 128, loss = 0.42862623\n",
      "Iteration 129, loss = 0.42847748\n",
      "Iteration 130, loss = 0.42860572\n",
      "Iteration 131, loss = 0.42842181\n",
      "Iteration 132, loss = 0.42841371\n",
      "Iteration 133, loss = 0.42841551\n",
      "Iteration 134, loss = 0.42807247\n",
      "Iteration 135, loss = 0.42781774\n",
      "Iteration 136, loss = 0.42777273\n",
      "Iteration 137, loss = 0.42775944\n",
      "Iteration 138, loss = 0.42722148\n",
      "Iteration 139, loss = 0.42737964\n",
      "Iteration 140, loss = 0.42726624\n",
      "Iteration 141, loss = 0.42730567\n",
      "Iteration 142, loss = 0.42699292\n",
      "Iteration 143, loss = 0.42726510\n",
      "Iteration 144, loss = 0.42734804\n",
      "Iteration 145, loss = 0.42685672\n",
      "Iteration 146, loss = 0.42673825\n",
      "Iteration 147, loss = 0.42618734\n",
      "Iteration 148, loss = 0.42603134\n",
      "Iteration 149, loss = 0.42593215\n",
      "Iteration 150, loss = 0.42635015\n",
      "Iteration 151, loss = 0.42585098\n",
      "Iteration 152, loss = 0.42585482\n",
      "Iteration 153, loss = 0.42576251\n",
      "Iteration 154, loss = 0.42534932\n",
      "Iteration 155, loss = 0.42586347\n",
      "Iteration 156, loss = 0.42530212\n",
      "Iteration 157, loss = 0.42548058\n",
      "Iteration 158, loss = 0.42500745\n",
      "Iteration 159, loss = 0.42494977\n",
      "Iteration 160, loss = 0.42536321\n",
      "Iteration 161, loss = 0.42485391\n",
      "Iteration 162, loss = 0.42473481\n",
      "Iteration 163, loss = 0.42492596\n",
      "Iteration 164, loss = 0.42453651\n",
      "Iteration 165, loss = 0.42445558\n",
      "Iteration 166, loss = 0.42420365\n",
      "Iteration 167, loss = 0.42424336\n",
      "Iteration 168, loss = 0.42423121\n",
      "Iteration 169, loss = 0.42399429\n",
      "Iteration 170, loss = 0.42406217\n",
      "Iteration 171, loss = 0.42424966\n",
      "Iteration 172, loss = 0.42361006\n",
      "Iteration 173, loss = 0.42327977\n",
      "Iteration 174, loss = 0.42331325\n",
      "Iteration 175, loss = 0.42336839\n",
      "Iteration 176, loss = 0.42357829\n",
      "Iteration 177, loss = 0.42248028\n",
      "Iteration 178, loss = 0.42368915\n",
      "Iteration 179, loss = 0.42229494\n",
      "Iteration 180, loss = 0.42306725\n",
      "Iteration 181, loss = 0.42290183\n",
      "Iteration 182, loss = 0.42284607\n",
      "Iteration 183, loss = 0.42262941\n",
      "Iteration 184, loss = 0.42230524\n",
      "Iteration 185, loss = 0.42227350\n",
      "Iteration 186, loss = 0.42235861\n",
      "Iteration 187, loss = 0.42192137\n",
      "Iteration 188, loss = 0.42197945\n",
      "Iteration 189, loss = 0.42195845\n",
      "Iteration 190, loss = 0.42194699\n",
      "Iteration 191, loss = 0.42137201\n",
      "Iteration 192, loss = 0.42217516\n",
      "Iteration 193, loss = 0.42154194\n",
      "Iteration 194, loss = 0.42141435\n",
      "Iteration 195, loss = 0.42110511\n",
      "Iteration 196, loss = 0.42160032\n",
      "Iteration 197, loss = 0.42091895\n",
      "Iteration 198, loss = 0.42050637\n",
      "Iteration 199, loss = 0.42125258\n",
      "Iteration 200, loss = 0.42124367\n",
      "Iteration 201, loss = 0.42061306\n",
      "Iteration 202, loss = 0.42038014\n",
      "Iteration 203, loss = 0.42034003\n",
      "Iteration 204, loss = 0.42047335\n",
      "Iteration 205, loss = 0.42055613\n",
      "Iteration 206, loss = 0.42048372\n",
      "Iteration 207, loss = 0.42001122\n",
      "Iteration 208, loss = 0.42017123\n",
      "Iteration 209, loss = 0.42002927\n",
      "Iteration 210, loss = 0.42006001\n",
      "Iteration 211, loss = 0.41957334\n",
      "Iteration 212, loss = 0.41972563\n",
      "Iteration 213, loss = 0.42060938\n",
      "Iteration 214, loss = 0.41986938\n",
      "Iteration 215, loss = 0.41966901\n",
      "Iteration 216, loss = 0.41949992\n",
      "Iteration 217, loss = 0.41951384\n",
      "Iteration 218, loss = 0.41927334\n",
      "Iteration 219, loss = 0.41880782\n",
      "Iteration 220, loss = 0.41859628\n",
      "Iteration 221, loss = 0.41871297\n",
      "Iteration 222, loss = 0.41868802\n",
      "Iteration 223, loss = 0.41855144\n",
      "Iteration 224, loss = 0.41879521\n",
      "Iteration 225, loss = 0.41863675\n",
      "Iteration 226, loss = 0.41845883\n",
      "Iteration 227, loss = 0.41885997\n",
      "Iteration 228, loss = 0.41796730\n",
      "Iteration 229, loss = 0.41777462\n",
      "Iteration 230, loss = 0.41781811\n",
      "Iteration 231, loss = 0.41831099\n",
      "Iteration 232, loss = 0.41787086\n",
      "Iteration 233, loss = 0.41736707\n",
      "Iteration 234, loss = 0.41820706\n",
      "Iteration 235, loss = 0.41775017\n",
      "Iteration 236, loss = 0.41733114\n",
      "Iteration 237, loss = 0.41786960\n",
      "Iteration 238, loss = 0.41748191\n",
      "Iteration 239, loss = 0.41698090\n",
      "Iteration 240, loss = 0.41682400\n",
      "Iteration 241, loss = 0.41753676\n",
      "Iteration 242, loss = 0.41708604\n",
      "Iteration 243, loss = 0.41677609\n",
      "Iteration 244, loss = 0.41713238\n",
      "Iteration 245, loss = 0.41680202\n",
      "Iteration 246, loss = 0.41655892\n",
      "Iteration 247, loss = 0.41689019\n",
      "Iteration 248, loss = 0.41659473\n",
      "Iteration 249, loss = 0.41625662\n",
      "Iteration 250, loss = 0.41610628\n",
      "Iteration 251, loss = 0.41657409\n",
      "Iteration 252, loss = 0.41618370\n",
      "Iteration 253, loss = 0.41568631\n",
      "Iteration 254, loss = 0.41572764\n",
      "Iteration 255, loss = 0.41599067\n",
      "Iteration 256, loss = 0.41583354\n",
      "Iteration 257, loss = 0.41604168\n",
      "Iteration 258, loss = 0.41577987\n",
      "Iteration 259, loss = 0.41549000\n",
      "Iteration 260, loss = 0.41555357\n",
      "Iteration 261, loss = 0.41551802\n",
      "Iteration 262, loss = 0.41569788\n",
      "Iteration 263, loss = 0.41530075\n",
      "Iteration 264, loss = 0.41596240\n",
      "Iteration 265, loss = 0.41549097\n",
      "Iteration 266, loss = 0.41512966\n",
      "Iteration 267, loss = 0.41510943\n",
      "Iteration 268, loss = 0.41497448\n",
      "Iteration 269, loss = 0.41521310\n",
      "Iteration 270, loss = 0.41498572\n",
      "Iteration 271, loss = 0.41484208\n",
      "Iteration 272, loss = 0.41462703\n",
      "Iteration 273, loss = 0.41450257\n",
      "Iteration 274, loss = 0.41462052\n",
      "Iteration 275, loss = 0.41424246\n",
      "Iteration 276, loss = 0.41466987\n",
      "Iteration 277, loss = 0.41437469\n",
      "Iteration 278, loss = 0.41422307\n",
      "Iteration 279, loss = 0.41504733\n",
      "Iteration 280, loss = 0.41450597\n",
      "Iteration 281, loss = 0.41415848\n",
      "Iteration 282, loss = 0.41362546\n",
      "Iteration 283, loss = 0.41361602\n",
      "Iteration 284, loss = 0.41441373\n",
      "Iteration 285, loss = 0.41524766\n",
      "Iteration 286, loss = 0.41400947\n",
      "Iteration 287, loss = 0.41354459\n",
      "Iteration 288, loss = 0.41438268\n",
      "Iteration 289, loss = 0.41313061\n",
      "Iteration 290, loss = 0.41373652\n",
      "Iteration 291, loss = 0.41379160\n",
      "Iteration 292, loss = 0.41315908\n",
      "Iteration 293, loss = 0.41331879\n",
      "Iteration 294, loss = 0.41246828\n",
      "Iteration 295, loss = 0.41329870\n",
      "Iteration 296, loss = 0.41330103\n",
      "Iteration 297, loss = 0.41288894\n",
      "Iteration 298, loss = 0.41356024\n",
      "Iteration 299, loss = 0.41393520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:33<02:33,  9.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 300, loss = 0.41321421\n",
      "Iteration 301, loss = 0.41263839\n",
      "Iteration 302, loss = 0.41274574\n",
      "Iteration 303, loss = 0.41264521\n",
      "Iteration 304, loss = 0.41270040\n",
      "Iteration 305, loss = 0.41247058\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54307993\n",
      "Iteration 2, loss = 0.50163974\n",
      "Iteration 3, loss = 0.48076637\n",
      "Iteration 4, loss = 0.46757852\n",
      "Iteration 5, loss = 0.46093147\n",
      "Iteration 6, loss = 0.45717949\n",
      "Iteration 7, loss = 0.45490936\n",
      "Iteration 8, loss = 0.45328382\n",
      "Iteration 9, loss = 0.45213416\n",
      "Iteration 10, loss = 0.45111269\n",
      "Iteration 11, loss = 0.45054340\n",
      "Iteration 12, loss = 0.44972657\n",
      "Iteration 13, loss = 0.44940390\n",
      "Iteration 14, loss = 0.44892339\n",
      "Iteration 15, loss = 0.44830098\n",
      "Iteration 16, loss = 0.44785722\n",
      "Iteration 17, loss = 0.44761626\n",
      "Iteration 18, loss = 0.44741508\n",
      "Iteration 19, loss = 0.44695735\n",
      "Iteration 20, loss = 0.44663118\n",
      "Iteration 21, loss = 0.44646846\n",
      "Iteration 22, loss = 0.44606887\n",
      "Iteration 23, loss = 0.44569768\n",
      "Iteration 24, loss = 0.44620801\n",
      "Iteration 25, loss = 0.44541939\n",
      "Iteration 26, loss = 0.44526313\n",
      "Iteration 27, loss = 0.44492488\n",
      "Iteration 28, loss = 0.44459622\n",
      "Iteration 29, loss = 0.44435850\n",
      "Iteration 30, loss = 0.44444902\n",
      "Iteration 31, loss = 0.44394853\n",
      "Iteration 32, loss = 0.44382012\n",
      "Iteration 33, loss = 0.44354377\n",
      "Iteration 34, loss = 0.44373674\n",
      "Iteration 35, loss = 0.44348668\n",
      "Iteration 36, loss = 0.44343174\n",
      "Iteration 37, loss = 0.44321508\n",
      "Iteration 38, loss = 0.44284385\n",
      "Iteration 39, loss = 0.44255016\n",
      "Iteration 40, loss = 0.44254443\n",
      "Iteration 41, loss = 0.44224972\n",
      "Iteration 42, loss = 0.44206421\n",
      "Iteration 43, loss = 0.44202046\n",
      "Iteration 44, loss = 0.44194112\n",
      "Iteration 45, loss = 0.44181859\n",
      "Iteration 46, loss = 0.44141393\n",
      "Iteration 47, loss = 0.44132829\n",
      "Iteration 48, loss = 0.44116992\n",
      "Iteration 49, loss = 0.44153957\n",
      "Iteration 50, loss = 0.44072331\n",
      "Iteration 51, loss = 0.44064486\n",
      "Iteration 52, loss = 0.44033037\n",
      "Iteration 53, loss = 0.44072851\n",
      "Iteration 54, loss = 0.44006842\n",
      "Iteration 55, loss = 0.44013109\n",
      "Iteration 56, loss = 0.44019638\n",
      "Iteration 57, loss = 0.43955507\n",
      "Iteration 58, loss = 0.43955145\n",
      "Iteration 59, loss = 0.43984601\n",
      "Iteration 60, loss = 0.43949464\n",
      "Iteration 61, loss = 0.43910355\n",
      "Iteration 62, loss = 0.43905012\n",
      "Iteration 63, loss = 0.43892124\n",
      "Iteration 64, loss = 0.43865451\n",
      "Iteration 65, loss = 0.43851927\n",
      "Iteration 66, loss = 0.43831015\n",
      "Iteration 67, loss = 0.43847136\n",
      "Iteration 68, loss = 0.43804991\n",
      "Iteration 69, loss = 0.43786764\n",
      "Iteration 70, loss = 0.43770621\n",
      "Iteration 71, loss = 0.43759309\n",
      "Iteration 72, loss = 0.43744082\n",
      "Iteration 73, loss = 0.43730468\n",
      "Iteration 74, loss = 0.43699562\n",
      "Iteration 75, loss = 0.43697820\n",
      "Iteration 76, loss = 0.43696372\n",
      "Iteration 77, loss = 0.43665940\n",
      "Iteration 78, loss = 0.43658076\n",
      "Iteration 79, loss = 0.43649130\n",
      "Iteration 80, loss = 0.43622914\n",
      "Iteration 81, loss = 0.43601098\n",
      "Iteration 82, loss = 0.43624072\n",
      "Iteration 83, loss = 0.43600374\n",
      "Iteration 84, loss = 0.43579921\n",
      "Iteration 85, loss = 0.43563226\n",
      "Iteration 86, loss = 0.43545084\n",
      "Iteration 87, loss = 0.43533374\n",
      "Iteration 88, loss = 0.43528977\n",
      "Iteration 89, loss = 0.43500650\n",
      "Iteration 90, loss = 0.43467698\n",
      "Iteration 91, loss = 0.43468984\n",
      "Iteration 92, loss = 0.43430435\n",
      "Iteration 93, loss = 0.43421928\n",
      "Iteration 94, loss = 0.43389048\n",
      "Iteration 95, loss = 0.43381143\n",
      "Iteration 96, loss = 0.43408542\n",
      "Iteration 97, loss = 0.43343718\n",
      "Iteration 98, loss = 0.43396144\n",
      "Iteration 99, loss = 0.43383789\n",
      "Iteration 100, loss = 0.43304713\n",
      "Iteration 101, loss = 0.43318225\n",
      "Iteration 102, loss = 0.43249017\n",
      "Iteration 103, loss = 0.43246263\n",
      "Iteration 104, loss = 0.43235383\n",
      "Iteration 105, loss = 0.43248250\n",
      "Iteration 106, loss = 0.43209708\n",
      "Iteration 107, loss = 0.43163145\n",
      "Iteration 108, loss = 0.43196148\n",
      "Iteration 109, loss = 0.43136789\n",
      "Iteration 110, loss = 0.43151837\n",
      "Iteration 111, loss = 0.43133985\n",
      "Iteration 112, loss = 0.43105002\n",
      "Iteration 113, loss = 0.43074989\n",
      "Iteration 114, loss = 0.43073439\n",
      "Iteration 115, loss = 0.43034651\n",
      "Iteration 116, loss = 0.43022046\n",
      "Iteration 117, loss = 0.42991817\n",
      "Iteration 118, loss = 0.42981513\n",
      "Iteration 119, loss = 0.42952929\n",
      "Iteration 120, loss = 0.42971757\n",
      "Iteration 121, loss = 0.42924317\n",
      "Iteration 122, loss = 0.42874744\n",
      "Iteration 123, loss = 0.42915094\n",
      "Iteration 124, loss = 0.42855642\n",
      "Iteration 125, loss = 0.42859404\n",
      "Iteration 126, loss = 0.42827115\n",
      "Iteration 127, loss = 0.42832013\n",
      "Iteration 128, loss = 0.42776848\n",
      "Iteration 129, loss = 0.42830196\n",
      "Iteration 130, loss = 0.42748898\n",
      "Iteration 131, loss = 0.42734356\n",
      "Iteration 132, loss = 0.42727586\n",
      "Iteration 133, loss = 0.42755687\n",
      "Iteration 134, loss = 0.42702683\n",
      "Iteration 135, loss = 0.42688447\n",
      "Iteration 136, loss = 0.42647506\n",
      "Iteration 137, loss = 0.42655179\n",
      "Iteration 138, loss = 0.42654927\n",
      "Iteration 139, loss = 0.42623312\n",
      "Iteration 140, loss = 0.42654329\n",
      "Iteration 141, loss = 0.42575479\n",
      "Iteration 142, loss = 0.42585555\n",
      "Iteration 143, loss = 0.42626096\n",
      "Iteration 144, loss = 0.42563056\n",
      "Iteration 145, loss = 0.42522355\n",
      "Iteration 146, loss = 0.42523318\n",
      "Iteration 147, loss = 0.42500571\n",
      "Iteration 148, loss = 0.42465083\n",
      "Iteration 149, loss = 0.42469195\n",
      "Iteration 150, loss = 0.42472289\n",
      "Iteration 151, loss = 0.42436801\n",
      "Iteration 152, loss = 0.42444645\n",
      "Iteration 153, loss = 0.42398734\n",
      "Iteration 154, loss = 0.42385012\n",
      "Iteration 155, loss = 0.42389175\n",
      "Iteration 156, loss = 0.42347760\n",
      "Iteration 157, loss = 0.42342009\n",
      "Iteration 158, loss = 0.42351561\n",
      "Iteration 159, loss = 0.42329004\n",
      "Iteration 160, loss = 0.42287375\n",
      "Iteration 161, loss = 0.42302216\n",
      "Iteration 162, loss = 0.42261757\n",
      "Iteration 163, loss = 0.42253592\n",
      "Iteration 164, loss = 0.42261535\n",
      "Iteration 165, loss = 0.42253065\n",
      "Iteration 166, loss = 0.42209011\n",
      "Iteration 167, loss = 0.42186709\n",
      "Iteration 168, loss = 0.42177387\n",
      "Iteration 169, loss = 0.42205566\n",
      "Iteration 170, loss = 0.42168018\n",
      "Iteration 171, loss = 0.42128402\n",
      "Iteration 172, loss = 0.42133688\n",
      "Iteration 173, loss = 0.42114282\n",
      "Iteration 174, loss = 0.42145074\n",
      "Iteration 175, loss = 0.42099184\n",
      "Iteration 176, loss = 0.42147491\n",
      "Iteration 177, loss = 0.42115002\n",
      "Iteration 178, loss = 0.42032364\n",
      "Iteration 179, loss = 0.42019765\n",
      "Iteration 180, loss = 0.42033724\n",
      "Iteration 181, loss = 0.42034702\n",
      "Iteration 182, loss = 0.42021889\n",
      "Iteration 183, loss = 0.41987507\n",
      "Iteration 184, loss = 0.42025173\n",
      "Iteration 185, loss = 0.41936637\n",
      "Iteration 186, loss = 0.41964513\n",
      "Iteration 187, loss = 0.41928237\n",
      "Iteration 188, loss = 0.41904082\n",
      "Iteration 189, loss = 0.41894323\n",
      "Iteration 190, loss = 0.41894253\n",
      "Iteration 191, loss = 0.41841271\n",
      "Iteration 192, loss = 0.41846504\n",
      "Iteration 193, loss = 0.41845554\n",
      "Iteration 194, loss = 0.41900865\n",
      "Iteration 195, loss = 0.41828943\n",
      "Iteration 196, loss = 0.41816902\n",
      "Iteration 197, loss = 0.41818299\n",
      "Iteration 198, loss = 0.41751263\n",
      "Iteration 199, loss = 0.41774971\n",
      "Iteration 200, loss = 0.41722208\n",
      "Iteration 201, loss = 0.41750529\n",
      "Iteration 202, loss = 0.41706111\n",
      "Iteration 203, loss = 0.41711186\n",
      "Iteration 204, loss = 0.41713702\n",
      "Iteration 205, loss = 0.41706741\n",
      "Iteration 206, loss = 0.41670063\n",
      "Iteration 207, loss = 0.41624838\n",
      "Iteration 208, loss = 0.41657315\n",
      "Iteration 209, loss = 0.41621506\n",
      "Iteration 210, loss = 0.41614239\n",
      "Iteration 211, loss = 0.41600877\n",
      "Iteration 212, loss = 0.41589741\n",
      "Iteration 213, loss = 0.41568381\n",
      "Iteration 214, loss = 0.41584454\n",
      "Iteration 215, loss = 0.41542042\n",
      "Iteration 216, loss = 0.41503178\n",
      "Iteration 217, loss = 0.41502307\n",
      "Iteration 218, loss = 0.41516736\n",
      "Iteration 219, loss = 0.41472570\n",
      "Iteration 220, loss = 0.41436117\n",
      "Iteration 221, loss = 0.41476581\n",
      "Iteration 222, loss = 0.41465060\n",
      "Iteration 223, loss = 0.41455646\n",
      "Iteration 224, loss = 0.41426216\n",
      "Iteration 225, loss = 0.41432782\n",
      "Iteration 226, loss = 0.41398429\n",
      "Iteration 227, loss = 0.41360834\n",
      "Iteration 228, loss = 0.41390829\n",
      "Iteration 229, loss = 0.41372698\n",
      "Iteration 230, loss = 0.41389089\n",
      "Iteration 231, loss = 0.41385430\n",
      "Iteration 232, loss = 0.41332627\n",
      "Iteration 233, loss = 0.41399955\n",
      "Iteration 234, loss = 0.41381825\n",
      "Iteration 235, loss = 0.41384776\n",
      "Iteration 236, loss = 0.41345127\n",
      "Iteration 237, loss = 0.41275199\n",
      "Iteration 238, loss = 0.41357027\n",
      "Iteration 239, loss = 0.41281074\n",
      "Iteration 240, loss = 0.41289625\n",
      "Iteration 241, loss = 0.41285214\n",
      "Iteration 242, loss = 0.41255655\n",
      "Iteration 243, loss = 0.41256761\n",
      "Iteration 244, loss = 0.41217785\n",
      "Iteration 245, loss = 0.41219282\n",
      "Iteration 246, loss = 0.41232802\n",
      "Iteration 247, loss = 0.41237454\n",
      "Iteration 248, loss = 0.41202560\n",
      "Iteration 249, loss = 0.41222679\n",
      "Iteration 250, loss = 0.41196937\n",
      "Iteration 251, loss = 0.41197128\n",
      "Iteration 252, loss = 0.41149141\n",
      "Iteration 253, loss = 0.41212893\n",
      "Iteration 254, loss = 0.41190035\n",
      "Iteration 255, loss = 0.41145754\n",
      "Iteration 256, loss = 0.41105759\n",
      "Iteration 257, loss = 0.41127172\n",
      "Iteration 258, loss = 0.41099572\n",
      "Iteration 259, loss = 0.41165266\n",
      "Iteration 260, loss = 0.41093252\n",
      "Iteration 261, loss = 0.41084631\n",
      "Iteration 262, loss = 0.41048409\n",
      "Iteration 263, loss = 0.41091495\n",
      "Iteration 264, loss = 0.41062421\n",
      "Iteration 265, loss = 0.41013230\n",
      "Iteration 266, loss = 0.41059008\n",
      "Iteration 267, loss = 0.41059830\n",
      "Iteration 268, loss = 0.41065001\n",
      "Iteration 269, loss = 0.41023210\n",
      "Iteration 270, loss = 0.41040052\n",
      "Iteration 271, loss = 0.41067222\n",
      "Iteration 272, loss = 0.40977850\n",
      "Iteration 273, loss = 0.40970446\n",
      "Iteration 274, loss = 0.40950465\n",
      "Iteration 275, loss = 0.40942896\n",
      "Iteration 276, loss = 0.40934240\n",
      "Iteration 277, loss = 0.40974235\n",
      "Iteration 278, loss = 0.40954072\n",
      "Iteration 279, loss = 0.40934957\n",
      "Iteration 280, loss = 0.40907358\n",
      "Iteration 281, loss = 0.40870020\n",
      "Iteration 282, loss = 0.40944346\n",
      "Iteration 283, loss = 0.40899545\n",
      "Iteration 284, loss = 0.40897893\n",
      "Iteration 285, loss = 0.40874095\n",
      "Iteration 286, loss = 0.40841025\n",
      "Iteration 287, loss = 0.40875229\n",
      "Iteration 288, loss = 0.40836863\n",
      "Iteration 289, loss = 0.40863653\n",
      "Iteration 290, loss = 0.40936116\n",
      "Iteration 291, loss = 0.40878387\n",
      "Iteration 292, loss = 0.40818000\n",
      "Iteration 293, loss = 0.40801419\n",
      "Iteration 294, loss = 0.40843391\n",
      "Iteration 295, loss = 0.40819282\n",
      "Iteration 296, loss = 0.40831037\n",
      "Iteration 297, loss = 0.40879716\n",
      "Iteration 298, loss = 0.40750931\n",
      "Iteration 299, loss = 0.40740718\n",
      "Iteration 300, loss = 0.40793194\n",
      "Iteration 301, loss = 0.40748030\n",
      "Iteration 302, loss = 0.40748122\n",
      "Iteration 303, loss = 0.40731346\n",
      "Iteration 304, loss = 0.40720994\n",
      "Iteration 305, loss = 0.40735730\n",
      "Iteration 306, loss = 0.40753006\n",
      "Iteration 307, loss = 0.40711398\n",
      "Iteration 308, loss = 0.40723351\n",
      "Iteration 309, loss = 0.40711347\n",
      "Iteration 310, loss = 0.40762838\n",
      "Iteration 311, loss = 0.40648115\n",
      "Iteration 312, loss = 0.40730518\n",
      "Iteration 313, loss = 0.40709032\n",
      "Iteration 314, loss = 0.40715009\n",
      "Iteration 315, loss = 0.40678537\n",
      "Iteration 316, loss = 0.40702002\n",
      "Iteration 317, loss = 0.40663989\n",
      "Iteration 318, loss = 0.40674239\n",
      "Iteration 319, loss = 0.40672041\n",
      "Iteration 320, loss = 0.40613144\n",
      "Iteration 321, loss = 0.40630329\n",
      "Iteration 322, loss = 0.40638929\n",
      "Iteration 323, loss = 0.40637426\n",
      "Iteration 324, loss = 0.40593480\n",
      "Iteration 325, loss = 0.40674855\n",
      "Iteration 326, loss = 0.40616581\n",
      "Iteration 327, loss = 0.40584766\n",
      "Iteration 328, loss = 0.40600021\n",
      "Iteration 329, loss = 0.40580443\n",
      "Iteration 330, loss = 0.40577751\n",
      "Iteration 331, loss = 0.40561402\n",
      "Iteration 332, loss = 0.40572147\n",
      "Iteration 333, loss = 0.40578754\n",
      "Iteration 334, loss = 0.40552886\n",
      "Iteration 335, loss = 0.40567799\n",
      "Iteration 336, loss = 0.40509626\n",
      "Iteration 337, loss = 0.40543302\n",
      "Iteration 338, loss = 0.40578755\n",
      "Iteration 339, loss = 0.40548792\n",
      "Iteration 340, loss = 0.40503255\n",
      "Iteration 341, loss = 0.40504116\n",
      "Iteration 342, loss = 0.40461158\n",
      "Iteration 343, loss = 0.40476958\n",
      "Iteration 344, loss = 0.40465505\n",
      "Iteration 345, loss = 0.40501549\n",
      "Iteration 346, loss = 0.40472638\n",
      "Iteration 347, loss = 0.40442119\n",
      "Iteration 348, loss = 0.40459744\n",
      "Iteration 349, loss = 0.40419045\n",
      "Iteration 350, loss = 0.40444436\n",
      "Iteration 351, loss = 0.40511291\n",
      "Iteration 352, loss = 0.40411678\n",
      "Iteration 353, loss = 0.40489062\n",
      "Iteration 354, loss = 0.40380560\n",
      "Iteration 355, loss = 0.40413145\n",
      "Iteration 356, loss = 0.40415490\n",
      "Iteration 357, loss = 0.40442541\n",
      "Iteration 358, loss = 0.40450665\n",
      "Iteration 359, loss = 0.40431046\n",
      "Iteration 360, loss = 0.40449612\n",
      "Iteration 361, loss = 0.40422277\n",
      "Iteration 362, loss = 0.40342692\n",
      "Iteration 363, loss = 0.40371028\n",
      "Iteration 364, loss = 0.40368807\n",
      "Iteration 365, loss = 0.40350718\n",
      "Iteration 366, loss = 0.40297206\n",
      "Iteration 367, loss = 0.40338761\n",
      "Iteration 368, loss = 0.40315461\n",
      "Iteration 369, loss = 0.40342410\n",
      "Iteration 370, loss = 0.40347075\n",
      "Iteration 371, loss = 0.40345947\n",
      "Iteration 372, loss = 0.40340988\n",
      "Iteration 373, loss = 0.40306052\n",
      "Iteration 374, loss = 0.40298511\n",
      "Iteration 375, loss = 0.40282792\n",
      "Iteration 376, loss = 0.40218834\n",
      "Iteration 377, loss = 0.40254121\n",
      "Iteration 378, loss = 0.40267763\n",
      "Iteration 379, loss = 0.40322646\n",
      "Iteration 380, loss = 0.40251336\n",
      "Iteration 381, loss = 0.40278454\n",
      "Iteration 382, loss = 0.40248049\n",
      "Iteration 383, loss = 0.40252458\n",
      "Iteration 384, loss = 0.40263899\n",
      "Iteration 385, loss = 0.40304588\n",
      "Iteration 386, loss = 0.40234303\n",
      "Iteration 387, loss = 0.40187973\n",
      "Iteration 388, loss = 0.40250670\n",
      "Iteration 389, loss = 0.40200470\n",
      "Iteration 390, loss = 0.40232831\n",
      "Iteration 391, loss = 0.40224723\n",
      "Iteration 392, loss = 0.40211338\n",
      "Iteration 393, loss = 0.40160749\n",
      "Iteration 394, loss = 0.40185923\n",
      "Iteration 395, loss = 0.40244513\n",
      "Iteration 396, loss = 0.40192256\n",
      "Iteration 397, loss = 0.40290030\n",
      "Iteration 398, loss = 0.40174537\n",
      "Iteration 399, loss = 0.40164437\n",
      "Iteration 400, loss = 0.40211342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [00:47<02:50, 11.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 401, loss = 0.40217687\n",
      "Iteration 402, loss = 0.40182245\n",
      "Iteration 403, loss = 0.40214436\n",
      "Iteration 404, loss = 0.40201474\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55216699\n",
      "Iteration 2, loss = 0.50939486\n",
      "Iteration 3, loss = 0.48753719\n",
      "Iteration 4, loss = 0.47432735\n",
      "Iteration 5, loss = 0.46584971\n",
      "Iteration 6, loss = 0.46133228\n",
      "Iteration 7, loss = 0.45821101\n",
      "Iteration 8, loss = 0.45642995\n",
      "Iteration 9, loss = 0.45489619\n",
      "Iteration 10, loss = 0.45334643\n",
      "Iteration 11, loss = 0.45281191\n",
      "Iteration 12, loss = 0.45168772\n",
      "Iteration 13, loss = 0.45109888\n",
      "Iteration 14, loss = 0.45067628\n",
      "Iteration 15, loss = 0.44990434\n",
      "Iteration 16, loss = 0.44941704\n",
      "Iteration 17, loss = 0.44931241\n",
      "Iteration 18, loss = 0.44876930\n",
      "Iteration 19, loss = 0.44844528\n",
      "Iteration 20, loss = 0.44786923\n",
      "Iteration 21, loss = 0.44791062\n",
      "Iteration 22, loss = 0.44751085\n",
      "Iteration 23, loss = 0.44730933\n",
      "Iteration 24, loss = 0.44698181\n",
      "Iteration 25, loss = 0.44702072\n",
      "Iteration 26, loss = 0.44664730\n",
      "Iteration 27, loss = 0.44667836\n",
      "Iteration 28, loss = 0.44639621\n",
      "Iteration 29, loss = 0.44599972\n",
      "Iteration 30, loss = 0.44609211\n",
      "Iteration 31, loss = 0.44573311\n",
      "Iteration 32, loss = 0.44535014\n",
      "Iteration 33, loss = 0.44527378\n",
      "Iteration 34, loss = 0.44490682\n",
      "Iteration 35, loss = 0.44533065\n",
      "Iteration 36, loss = 0.44459209\n",
      "Iteration 37, loss = 0.44463054\n",
      "Iteration 38, loss = 0.44439121\n",
      "Iteration 39, loss = 0.44418065\n",
      "Iteration 40, loss = 0.44420454\n",
      "Iteration 41, loss = 0.44416087\n",
      "Iteration 42, loss = 0.44364086\n",
      "Iteration 43, loss = 0.44361215\n",
      "Iteration 44, loss = 0.44351333\n",
      "Iteration 45, loss = 0.44321130\n",
      "Iteration 46, loss = 0.44318039\n",
      "Iteration 47, loss = 0.44280704\n",
      "Iteration 48, loss = 0.44287840\n",
      "Iteration 49, loss = 0.44255129\n",
      "Iteration 50, loss = 0.44261634\n",
      "Iteration 51, loss = 0.44249032\n",
      "Iteration 52, loss = 0.44220018\n",
      "Iteration 53, loss = 0.44192642\n",
      "Iteration 54, loss = 0.44218964\n",
      "Iteration 55, loss = 0.44194433\n",
      "Iteration 56, loss = 0.44191681\n",
      "Iteration 57, loss = 0.44145204\n",
      "Iteration 58, loss = 0.44140697\n",
      "Iteration 59, loss = 0.44124796\n",
      "Iteration 60, loss = 0.44134775\n",
      "Iteration 61, loss = 0.44133481\n",
      "Iteration 62, loss = 0.44089312\n",
      "Iteration 63, loss = 0.44086786\n",
      "Iteration 64, loss = 0.44048891\n",
      "Iteration 65, loss = 0.44049969\n",
      "Iteration 66, loss = 0.44034002\n",
      "Iteration 67, loss = 0.44009925\n",
      "Iteration 68, loss = 0.44026739\n",
      "Iteration 69, loss = 0.44017922\n",
      "Iteration 70, loss = 0.44014029\n",
      "Iteration 71, loss = 0.43960920\n",
      "Iteration 72, loss = 0.43961996\n",
      "Iteration 73, loss = 0.43943505\n",
      "Iteration 74, loss = 0.43946794\n",
      "Iteration 75, loss = 0.43912423\n",
      "Iteration 76, loss = 0.43907632\n",
      "Iteration 77, loss = 0.43881099\n",
      "Iteration 78, loss = 0.43862136\n",
      "Iteration 79, loss = 0.43854409\n",
      "Iteration 80, loss = 0.43829592\n",
      "Iteration 81, loss = 0.43835091\n",
      "Iteration 82, loss = 0.43839631\n",
      "Iteration 83, loss = 0.43784762\n",
      "Iteration 84, loss = 0.43777538\n",
      "Iteration 85, loss = 0.43762819\n",
      "Iteration 86, loss = 0.43755330\n",
      "Iteration 87, loss = 0.43752998\n",
      "Iteration 88, loss = 0.43715962\n",
      "Iteration 89, loss = 0.43699048\n",
      "Iteration 90, loss = 0.43684602\n",
      "Iteration 91, loss = 0.43653547\n",
      "Iteration 92, loss = 0.43662740\n",
      "Iteration 93, loss = 0.43648592\n",
      "Iteration 94, loss = 0.43622199\n",
      "Iteration 95, loss = 0.43593888\n",
      "Iteration 96, loss = 0.43590633\n",
      "Iteration 97, loss = 0.43578065\n",
      "Iteration 98, loss = 0.43541524\n",
      "Iteration 99, loss = 0.43526635\n",
      "Iteration 100, loss = 0.43505130\n",
      "Iteration 101, loss = 0.43493003\n",
      "Iteration 102, loss = 0.43477019\n",
      "Iteration 103, loss = 0.43453260\n",
      "Iteration 104, loss = 0.43426780\n",
      "Iteration 105, loss = 0.43400758\n",
      "Iteration 106, loss = 0.43391980\n",
      "Iteration 107, loss = 0.43364921\n",
      "Iteration 108, loss = 0.43343211\n",
      "Iteration 109, loss = 0.43308357\n",
      "Iteration 110, loss = 0.43340259\n",
      "Iteration 111, loss = 0.43283659\n",
      "Iteration 112, loss = 0.43297492\n",
      "Iteration 113, loss = 0.43262330\n",
      "Iteration 114, loss = 0.43224494\n",
      "Iteration 115, loss = 0.43212635\n",
      "Iteration 116, loss = 0.43193490\n",
      "Iteration 117, loss = 0.43198627\n",
      "Iteration 118, loss = 0.43183487\n",
      "Iteration 119, loss = 0.43116177\n",
      "Iteration 120, loss = 0.43125096\n",
      "Iteration 121, loss = 0.43096069\n",
      "Iteration 122, loss = 0.43081252\n",
      "Iteration 123, loss = 0.43046780\n",
      "Iteration 124, loss = 0.43036682\n",
      "Iteration 125, loss = 0.43014921\n",
      "Iteration 126, loss = 0.42990532\n",
      "Iteration 127, loss = 0.42967413\n",
      "Iteration 128, loss = 0.42950430\n",
      "Iteration 129, loss = 0.42939252\n",
      "Iteration 130, loss = 0.42911369\n",
      "Iteration 131, loss = 0.42896079\n",
      "Iteration 132, loss = 0.42850005\n",
      "Iteration 133, loss = 0.42851744\n",
      "Iteration 134, loss = 0.42828302\n",
      "Iteration 135, loss = 0.42831603\n",
      "Iteration 136, loss = 0.42817607\n",
      "Iteration 137, loss = 0.42750951\n",
      "Iteration 138, loss = 0.42777442\n",
      "Iteration 139, loss = 0.42761832\n",
      "Iteration 140, loss = 0.42765015\n",
      "Iteration 141, loss = 0.42707874\n",
      "Iteration 142, loss = 0.42726071\n",
      "Iteration 143, loss = 0.42668453\n",
      "Iteration 144, loss = 0.42645077\n",
      "Iteration 145, loss = 0.42629567\n",
      "Iteration 146, loss = 0.42646824\n",
      "Iteration 147, loss = 0.42583410\n",
      "Iteration 148, loss = 0.42600029\n",
      "Iteration 149, loss = 0.42583543\n",
      "Iteration 150, loss = 0.42579217\n",
      "Iteration 151, loss = 0.42577163\n",
      "Iteration 152, loss = 0.42504968\n",
      "Iteration 153, loss = 0.42482584\n",
      "Iteration 154, loss = 0.42468261\n",
      "Iteration 155, loss = 0.42465901\n",
      "Iteration 156, loss = 0.42455917\n",
      "Iteration 157, loss = 0.42429967\n",
      "Iteration 158, loss = 0.42456403\n",
      "Iteration 159, loss = 0.42384849\n",
      "Iteration 160, loss = 0.42373430\n",
      "Iteration 161, loss = 0.42342511\n",
      "Iteration 162, loss = 0.42343451\n",
      "Iteration 163, loss = 0.42382668\n",
      "Iteration 164, loss = 0.42280847\n",
      "Iteration 165, loss = 0.42270398\n",
      "Iteration 166, loss = 0.42273483\n",
      "Iteration 167, loss = 0.42294747\n",
      "Iteration 168, loss = 0.42205701\n",
      "Iteration 169, loss = 0.42245352\n",
      "Iteration 170, loss = 0.42180245\n",
      "Iteration 171, loss = 0.42166628\n",
      "Iteration 172, loss = 0.42178504\n",
      "Iteration 173, loss = 0.42147660\n",
      "Iteration 174, loss = 0.42092694\n",
      "Iteration 175, loss = 0.42097248\n",
      "Iteration 176, loss = 0.42101656\n",
      "Iteration 177, loss = 0.42079190\n",
      "Iteration 178, loss = 0.42079521\n",
      "Iteration 179, loss = 0.42036369\n",
      "Iteration 180, loss = 0.42069335\n",
      "Iteration 181, loss = 0.42029782\n",
      "Iteration 182, loss = 0.42042250\n",
      "Iteration 183, loss = 0.41999302\n",
      "Iteration 184, loss = 0.41964833\n",
      "Iteration 185, loss = 0.42031945\n",
      "Iteration 186, loss = 0.41933919\n",
      "Iteration 187, loss = 0.41905792\n",
      "Iteration 188, loss = 0.41920157\n",
      "Iteration 189, loss = 0.41870290\n",
      "Iteration 190, loss = 0.41878024\n",
      "Iteration 191, loss = 0.41874685\n",
      "Iteration 192, loss = 0.41842973\n",
      "Iteration 193, loss = 0.41802255\n",
      "Iteration 194, loss = 0.41822471\n",
      "Iteration 195, loss = 0.41796020\n",
      "Iteration 196, loss = 0.41790113\n",
      "Iteration 197, loss = 0.41773351\n",
      "Iteration 198, loss = 0.41753802\n",
      "Iteration 199, loss = 0.41712702\n",
      "Iteration 200, loss = 0.41699123\n",
      "Iteration 201, loss = 0.41717697\n",
      "Iteration 202, loss = 0.41691886\n",
      "Iteration 203, loss = 0.41648075\n",
      "Iteration 204, loss = 0.41710211\n",
      "Iteration 205, loss = 0.41649501\n",
      "Iteration 206, loss = 0.41641724\n",
      "Iteration 207, loss = 0.41586063\n",
      "Iteration 208, loss = 0.41581539\n",
      "Iteration 209, loss = 0.41608219\n",
      "Iteration 210, loss = 0.41513504\n",
      "Iteration 211, loss = 0.41557477\n",
      "Iteration 212, loss = 0.41527221\n",
      "Iteration 213, loss = 0.41535542\n",
      "Iteration 214, loss = 0.41517550\n",
      "Iteration 215, loss = 0.41514828\n",
      "Iteration 216, loss = 0.41429306\n",
      "Iteration 217, loss = 0.41476054\n",
      "Iteration 218, loss = 0.41468438\n",
      "Iteration 219, loss = 0.41446468\n",
      "Iteration 220, loss = 0.41418633\n",
      "Iteration 221, loss = 0.41434966\n",
      "Iteration 222, loss = 0.41362415\n",
      "Iteration 223, loss = 0.41392674\n",
      "Iteration 224, loss = 0.41375974\n",
      "Iteration 225, loss = 0.41332561\n",
      "Iteration 226, loss = 0.41301841\n",
      "Iteration 227, loss = 0.41386608\n",
      "Iteration 228, loss = 0.41305454\n",
      "Iteration 229, loss = 0.41302836\n",
      "Iteration 230, loss = 0.41268718\n",
      "Iteration 231, loss = 0.41225803\n",
      "Iteration 232, loss = 0.41218346\n",
      "Iteration 233, loss = 0.41246432\n",
      "Iteration 234, loss = 0.41217609\n",
      "Iteration 235, loss = 0.41276043\n",
      "Iteration 236, loss = 0.41218133\n",
      "Iteration 237, loss = 0.41184904\n",
      "Iteration 238, loss = 0.41144047\n",
      "Iteration 239, loss = 0.41179653\n",
      "Iteration 240, loss = 0.41127595\n",
      "Iteration 241, loss = 0.41132078\n",
      "Iteration 242, loss = 0.41080964\n",
      "Iteration 243, loss = 0.41109266\n",
      "Iteration 244, loss = 0.41095031\n",
      "Iteration 245, loss = 0.41105203\n",
      "Iteration 246, loss = 0.41043230\n",
      "Iteration 247, loss = 0.41059324\n",
      "Iteration 248, loss = 0.41033353\n",
      "Iteration 249, loss = 0.41025032\n",
      "Iteration 250, loss = 0.41027859\n",
      "Iteration 251, loss = 0.41008274\n",
      "Iteration 252, loss = 0.40970362\n",
      "Iteration 253, loss = 0.40957475\n",
      "Iteration 254, loss = 0.40924880\n",
      "Iteration 255, loss = 0.40956753\n",
      "Iteration 256, loss = 0.40943426\n",
      "Iteration 257, loss = 0.41027424\n",
      "Iteration 258, loss = 0.40953426\n",
      "Iteration 259, loss = 0.40876723\n",
      "Iteration 260, loss = 0.40886279\n",
      "Iteration 261, loss = 0.40904876\n",
      "Iteration 262, loss = 0.40847194\n",
      "Iteration 263, loss = 0.40875267\n",
      "Iteration 264, loss = 0.40818418\n",
      "Iteration 265, loss = 0.40899106\n",
      "Iteration 266, loss = 0.40825006\n",
      "Iteration 267, loss = 0.40755339\n",
      "Iteration 268, loss = 0.40773668\n",
      "Iteration 269, loss = 0.40823357\n",
      "Iteration 270, loss = 0.40755575\n",
      "Iteration 271, loss = 0.40741771\n",
      "Iteration 272, loss = 0.40740134\n",
      "Iteration 273, loss = 0.40797809\n",
      "Iteration 274, loss = 0.40737775\n",
      "Iteration 275, loss = 0.40672535\n",
      "Iteration 276, loss = 0.40688585\n",
      "Iteration 277, loss = 0.40613206\n",
      "Iteration 278, loss = 0.40683252\n",
      "Iteration 279, loss = 0.40678888\n",
      "Iteration 280, loss = 0.40627427\n",
      "Iteration 281, loss = 0.40665620\n",
      "Iteration 282, loss = 0.40626165\n",
      "Iteration 283, loss = 0.40609393\n",
      "Iteration 284, loss = 0.40656841\n",
      "Iteration 285, loss = 0.40597174\n",
      "Iteration 286, loss = 0.40670955\n",
      "Iteration 287, loss = 0.40586520\n",
      "Iteration 288, loss = 0.40558174\n",
      "Iteration 289, loss = 0.40529399\n",
      "Iteration 290, loss = 0.40492951\n",
      "Iteration 291, loss = 0.40503466\n",
      "Iteration 292, loss = 0.40536220\n",
      "Iteration 293, loss = 0.40466527\n",
      "Iteration 294, loss = 0.40488882\n",
      "Iteration 295, loss = 0.40487389\n",
      "Iteration 296, loss = 0.40475445\n",
      "Iteration 297, loss = 0.40479684\n",
      "Iteration 298, loss = 0.40450661\n",
      "Iteration 299, loss = 0.40481945\n",
      "Iteration 300, loss = 0.40424875\n",
      "Iteration 301, loss = 0.40399085\n",
      "Iteration 302, loss = 0.40443805\n",
      "Iteration 303, loss = 0.40446792\n",
      "Iteration 304, loss = 0.40357046\n",
      "Iteration 305, loss = 0.40343729\n",
      "Iteration 306, loss = 0.40360805\n",
      "Iteration 307, loss = 0.40348842\n",
      "Iteration 308, loss = 0.40354806\n",
      "Iteration 309, loss = 0.40334195\n",
      "Iteration 310, loss = 0.40280897\n",
      "Iteration 311, loss = 0.40325229\n",
      "Iteration 312, loss = 0.40250791\n",
      "Iteration 313, loss = 0.40288867\n",
      "Iteration 314, loss = 0.40272850\n",
      "Iteration 315, loss = 0.40259280\n",
      "Iteration 316, loss = 0.40243365\n",
      "Iteration 317, loss = 0.40229152\n",
      "Iteration 318, loss = 0.40276075\n",
      "Iteration 319, loss = 0.40312104\n",
      "Iteration 320, loss = 0.40264337\n",
      "Iteration 321, loss = 0.40127456\n",
      "Iteration 322, loss = 0.40175017\n",
      "Iteration 323, loss = 0.40171789\n",
      "Iteration 324, loss = 0.40172315\n",
      "Iteration 325, loss = 0.40166830\n",
      "Iteration 326, loss = 0.40120187\n",
      "Iteration 327, loss = 0.40168839\n",
      "Iteration 328, loss = 0.40175209\n",
      "Iteration 329, loss = 0.40084469\n",
      "Iteration 330, loss = 0.40129382\n",
      "Iteration 331, loss = 0.40095703\n",
      "Iteration 332, loss = 0.40106845\n",
      "Iteration 333, loss = 0.40028724\n",
      "Iteration 334, loss = 0.40036323\n",
      "Iteration 335, loss = 0.40061457\n",
      "Iteration 336, loss = 0.40054970\n",
      "Iteration 337, loss = 0.40040442\n",
      "Iteration 338, loss = 0.40036546\n",
      "Iteration 339, loss = 0.40020228\n",
      "Iteration 340, loss = 0.39999627\n",
      "Iteration 341, loss = 0.39939708\n",
      "Iteration 342, loss = 0.39988602\n",
      "Iteration 343, loss = 0.39990425\n",
      "Iteration 344, loss = 0.40018976\n",
      "Iteration 345, loss = 0.39877892\n",
      "Iteration 346, loss = 0.39899389\n",
      "Iteration 347, loss = 0.39885260\n",
      "Iteration 348, loss = 0.39922781\n",
      "Iteration 349, loss = 0.39877300\n",
      "Iteration 350, loss = 0.39997023\n",
      "Iteration 351, loss = 0.39902708\n",
      "Iteration 352, loss = 0.39921562\n",
      "Iteration 353, loss = 0.39860486\n",
      "Iteration 354, loss = 0.39853478\n",
      "Iteration 355, loss = 0.39869525\n",
      "Iteration 356, loss = 0.39772249\n",
      "Iteration 357, loss = 0.39864234\n",
      "Iteration 358, loss = 0.39858157\n",
      "Iteration 359, loss = 0.39803947\n",
      "Iteration 360, loss = 0.39836082\n",
      "Iteration 361, loss = 0.39792795\n",
      "Iteration 362, loss = 0.39773521\n",
      "Iteration 363, loss = 0.39827039\n",
      "Iteration 364, loss = 0.39763537\n",
      "Iteration 365, loss = 0.39788624\n",
      "Iteration 366, loss = 0.39786448\n",
      "Iteration 367, loss = 0.39717592\n",
      "Iteration 368, loss = 0.39782921\n",
      "Iteration 369, loss = 0.39728744\n",
      "Iteration 370, loss = 0.39765622\n",
      "Iteration 371, loss = 0.39728680\n",
      "Iteration 372, loss = 0.39698654\n",
      "Iteration 373, loss = 0.39707272\n",
      "Iteration 374, loss = 0.39697157\n",
      "Iteration 375, loss = 0.39702312\n",
      "Iteration 376, loss = 0.39666392\n",
      "Iteration 377, loss = 0.39718660\n",
      "Iteration 378, loss = 0.39665906\n",
      "Iteration 379, loss = 0.39686921\n",
      "Iteration 380, loss = 0.39674343\n",
      "Iteration 381, loss = 0.39695589\n",
      "Iteration 382, loss = 0.39631227\n",
      "Iteration 383, loss = 0.39599083\n",
      "Iteration 384, loss = 0.39521790\n",
      "Iteration 385, loss = 0.39629068\n",
      "Iteration 386, loss = 0.39591445\n",
      "Iteration 387, loss = 0.39589386\n",
      "Iteration 388, loss = 0.39621644\n",
      "Iteration 389, loss = 0.39519430\n",
      "Iteration 390, loss = 0.39586877\n",
      "Iteration 391, loss = 0.39540876\n",
      "Iteration 392, loss = 0.39584075\n",
      "Iteration 393, loss = 0.39545837\n",
      "Iteration 394, loss = 0.39529049\n",
      "Iteration 395, loss = 0.39507950\n",
      "Iteration 396, loss = 0.39526780\n",
      "Iteration 397, loss = 0.39540532\n",
      "Iteration 398, loss = 0.39502213\n",
      "Iteration 399, loss = 0.39478582\n",
      "Iteration 400, loss = 0.39486379\n",
      "Iteration 401, loss = 0.39470938\n",
      "Iteration 402, loss = 0.39506932\n",
      "Iteration 403, loss = 0.39433421\n",
      "Iteration 404, loss = 0.39430500\n",
      "Iteration 405, loss = 0.39509038\n",
      "Iteration 406, loss = 0.39447298\n",
      "Iteration 407, loss = 0.39476955\n",
      "Iteration 408, loss = 0.39473215\n",
      "Iteration 409, loss = 0.39420297\n",
      "Iteration 410, loss = 0.39374460\n",
      "Iteration 411, loss = 0.39453181\n",
      "Iteration 412, loss = 0.39380227\n",
      "Iteration 413, loss = 0.39411513\n",
      "Iteration 414, loss = 0.39347206\n",
      "Iteration 415, loss = 0.39420979\n",
      "Iteration 416, loss = 0.39305795\n",
      "Iteration 417, loss = 0.39307501\n",
      "Iteration 418, loss = 0.39363520\n",
      "Iteration 419, loss = 0.39404545\n",
      "Iteration 420, loss = 0.39325895\n",
      "Iteration 421, loss = 0.39320093\n",
      "Iteration 422, loss = 0.39282777\n",
      "Iteration 423, loss = 0.39277268\n",
      "Iteration 424, loss = 0.39270374\n",
      "Iteration 425, loss = 0.39275715\n",
      "Iteration 426, loss = 0.39304000\n",
      "Iteration 427, loss = 0.39372694\n",
      "Iteration 428, loss = 0.39304393\n",
      "Iteration 429, loss = 0.39280116\n",
      "Iteration 430, loss = 0.39323691\n",
      "Iteration 431, loss = 0.39250825\n",
      "Iteration 432, loss = 0.39290269\n",
      "Iteration 433, loss = 0.39261356\n",
      "Iteration 434, loss = 0.39215388\n",
      "Iteration 435, loss = 0.39221069\n",
      "Iteration 436, loss = 0.39226679\n",
      "Iteration 437, loss = 0.39219935\n",
      "Iteration 438, loss = 0.39223809\n",
      "Iteration 439, loss = 0.39167404\n",
      "Iteration 440, loss = 0.39165677\n",
      "Iteration 441, loss = 0.39166763\n",
      "Iteration 442, loss = 0.39215246\n",
      "Iteration 443, loss = 0.39182218\n",
      "Iteration 444, loss = 0.39124682\n",
      "Iteration 445, loss = 0.39245130\n",
      "Iteration 446, loss = 0.39147762\n",
      "Iteration 447, loss = 0.39136562\n",
      "Iteration 448, loss = 0.39159478\n",
      "Iteration 449, loss = 0.39163340\n",
      "Iteration 450, loss = 0.39107921\n",
      "Iteration 451, loss = 0.39164546\n",
      "Iteration 452, loss = 0.39151420\n",
      "Iteration 453, loss = 0.39186156\n",
      "Iteration 454, loss = 0.39082938\n",
      "Iteration 455, loss = 0.39112623\n",
      "Iteration 456, loss = 0.39060638\n",
      "Iteration 457, loss = 0.39120687\n",
      "Iteration 458, loss = 0.39039706\n",
      "Iteration 459, loss = 0.39087090\n",
      "Iteration 460, loss = 0.39062591\n",
      "Iteration 461, loss = 0.39009579\n",
      "Iteration 462, loss = 0.39099796\n",
      "Iteration 463, loss = 0.39036567\n",
      "Iteration 464, loss = 0.39039289\n",
      "Iteration 465, loss = 0.38964651\n",
      "Iteration 466, loss = 0.39055668\n",
      "Iteration 467, loss = 0.39036346\n",
      "Iteration 468, loss = 0.39017743\n",
      "Iteration 469, loss = 0.38995444\n",
      "Iteration 470, loss = 0.39009702\n",
      "Iteration 471, loss = 0.39033023\n",
      "Iteration 472, loss = 0.39014070\n",
      "Iteration 473, loss = 0.39026613\n",
      "Iteration 474, loss = 0.38962915\n",
      "Iteration 475, loss = 0.38914913\n",
      "Iteration 476, loss = 0.38961457\n",
      "Iteration 477, loss = 0.38925619\n",
      "Iteration 478, loss = 0.38963401\n",
      "Iteration 479, loss = 0.38999322\n",
      "Iteration 480, loss = 0.38929031\n",
      "Iteration 481, loss = 0.38932998\n",
      "Iteration 482, loss = 0.38976379\n",
      "Iteration 483, loss = 0.38928828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [01:06<03:14, 13.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 484, loss = 0.38925381\n",
      "Iteration 485, loss = 0.38929465\n",
      "Iteration 486, loss = 0.38979720\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57040809\n",
      "Iteration 2, loss = 0.50009502\n",
      "Iteration 3, loss = 0.47877229\n",
      "Iteration 4, loss = 0.46790889\n",
      "Iteration 5, loss = 0.46171805\n",
      "Iteration 6, loss = 0.45826339\n",
      "Iteration 7, loss = 0.45614588\n",
      "Iteration 8, loss = 0.45458218\n",
      "Iteration 9, loss = 0.45337489\n",
      "Iteration 10, loss = 0.45252242\n",
      "Iteration 11, loss = 0.45157266\n",
      "Iteration 12, loss = 0.45094709\n",
      "Iteration 13, loss = 0.45043691\n",
      "Iteration 14, loss = 0.44992927\n",
      "Iteration 15, loss = 0.44939412\n",
      "Iteration 16, loss = 0.44880164\n",
      "Iteration 17, loss = 0.44836322\n",
      "Iteration 18, loss = 0.44796256\n",
      "Iteration 19, loss = 0.44759503\n",
      "Iteration 20, loss = 0.44747092\n",
      "Iteration 21, loss = 0.44725829\n",
      "Iteration 22, loss = 0.44699416\n",
      "Iteration 23, loss = 0.44629258\n",
      "Iteration 24, loss = 0.44610396\n",
      "Iteration 25, loss = 0.44586741\n",
      "Iteration 26, loss = 0.44562559\n",
      "Iteration 27, loss = 0.44558939\n",
      "Iteration 28, loss = 0.44483153\n",
      "Iteration 29, loss = 0.44491892\n",
      "Iteration 30, loss = 0.44465318\n",
      "Iteration 31, loss = 0.44425071\n",
      "Iteration 32, loss = 0.44406108\n",
      "Iteration 33, loss = 0.44383181\n",
      "Iteration 34, loss = 0.44361490\n",
      "Iteration 35, loss = 0.44336328\n",
      "Iteration 36, loss = 0.44308745\n",
      "Iteration 37, loss = 0.44298692\n",
      "Iteration 38, loss = 0.44276430\n",
      "Iteration 39, loss = 0.44242379\n",
      "Iteration 40, loss = 0.44218961\n",
      "Iteration 41, loss = 0.44209762\n",
      "Iteration 42, loss = 0.44173013\n",
      "Iteration 43, loss = 0.44158612\n",
      "Iteration 44, loss = 0.44146752\n",
      "Iteration 45, loss = 0.44096688\n",
      "Iteration 46, loss = 0.44079607\n",
      "Iteration 47, loss = 0.44054992\n",
      "Iteration 48, loss = 0.44042384\n",
      "Iteration 49, loss = 0.44007627\n",
      "Iteration 50, loss = 0.43999539\n",
      "Iteration 51, loss = 0.43980691\n",
      "Iteration 52, loss = 0.43952054\n",
      "Iteration 53, loss = 0.43932033\n",
      "Iteration 54, loss = 0.43911267\n",
      "Iteration 55, loss = 0.43899577\n",
      "Iteration 56, loss = 0.43877016\n",
      "Iteration 57, loss = 0.43828984\n",
      "Iteration 58, loss = 0.43803306\n",
      "Iteration 59, loss = 0.43779164\n",
      "Iteration 60, loss = 0.43769010\n",
      "Iteration 61, loss = 0.43725816\n",
      "Iteration 62, loss = 0.43694842\n",
      "Iteration 63, loss = 0.43665858\n",
      "Iteration 64, loss = 0.43678557\n",
      "Iteration 65, loss = 0.43631504\n",
      "Iteration 66, loss = 0.43613826\n",
      "Iteration 67, loss = 0.43598914\n",
      "Iteration 68, loss = 0.43541482\n",
      "Iteration 69, loss = 0.43548604\n",
      "Iteration 70, loss = 0.43500722\n",
      "Iteration 71, loss = 0.43498260\n",
      "Iteration 72, loss = 0.43454266\n",
      "Iteration 73, loss = 0.43445752\n",
      "Iteration 74, loss = 0.43445924\n",
      "Iteration 75, loss = 0.43396592\n",
      "Iteration 76, loss = 0.43376899\n",
      "Iteration 77, loss = 0.43348086\n",
      "Iteration 78, loss = 0.43355613\n",
      "Iteration 79, loss = 0.43300739\n",
      "Iteration 80, loss = 0.43258036\n",
      "Iteration 81, loss = 0.43250268\n",
      "Iteration 82, loss = 0.43215779\n",
      "Iteration 83, loss = 0.43229839\n",
      "Iteration 84, loss = 0.43187270\n",
      "Iteration 85, loss = 0.43192854\n",
      "Iteration 86, loss = 0.43153016\n",
      "Iteration 87, loss = 0.43119315\n",
      "Iteration 88, loss = 0.43084402\n",
      "Iteration 89, loss = 0.43095242\n",
      "Iteration 90, loss = 0.43043563\n",
      "Iteration 91, loss = 0.43019056\n",
      "Iteration 92, loss = 0.43047848\n",
      "Iteration 93, loss = 0.42995503\n",
      "Iteration 94, loss = 0.43009154\n",
      "Iteration 95, loss = 0.42969164\n",
      "Iteration 96, loss = 0.42930710\n",
      "Iteration 97, loss = 0.42897152\n",
      "Iteration 98, loss = 0.42958870\n",
      "Iteration 99, loss = 0.42884383\n",
      "Iteration 100, loss = 0.42858750\n",
      "Iteration 101, loss = 0.42841592\n",
      "Iteration 102, loss = 0.42808693\n",
      "Iteration 103, loss = 0.42818905\n",
      "Iteration 104, loss = 0.42804797\n",
      "Iteration 105, loss = 0.42743449\n",
      "Iteration 106, loss = 0.42754120\n",
      "Iteration 107, loss = 0.42755036\n",
      "Iteration 108, loss = 0.42737625\n",
      "Iteration 109, loss = 0.42725620\n",
      "Iteration 110, loss = 0.42680780\n",
      "Iteration 111, loss = 0.42636853\n",
      "Iteration 112, loss = 0.42675261\n",
      "Iteration 113, loss = 0.42630449\n",
      "Iteration 114, loss = 0.42620432\n",
      "Iteration 115, loss = 0.42652382\n",
      "Iteration 116, loss = 0.42613111\n",
      "Iteration 117, loss = 0.42558703\n",
      "Iteration 118, loss = 0.42577122\n",
      "Iteration 119, loss = 0.42553162\n",
      "Iteration 120, loss = 0.42537386\n",
      "Iteration 121, loss = 0.42517521\n",
      "Iteration 122, loss = 0.42490473\n",
      "Iteration 123, loss = 0.42471902\n",
      "Iteration 124, loss = 0.42436820\n",
      "Iteration 125, loss = 0.42446890\n",
      "Iteration 126, loss = 0.42419891\n",
      "Iteration 127, loss = 0.42428396\n",
      "Iteration 128, loss = 0.42395338\n",
      "Iteration 129, loss = 0.42401025\n",
      "Iteration 130, loss = 0.42354313\n",
      "Iteration 131, loss = 0.42373863\n",
      "Iteration 132, loss = 0.42311036\n",
      "Iteration 133, loss = 0.42330964\n",
      "Iteration 134, loss = 0.42307968\n",
      "Iteration 135, loss = 0.42291261\n",
      "Iteration 136, loss = 0.42279188\n",
      "Iteration 137, loss = 0.42305657\n",
      "Iteration 138, loss = 0.42274363\n",
      "Iteration 139, loss = 0.42232281\n",
      "Iteration 140, loss = 0.42217700\n",
      "Iteration 141, loss = 0.42194036\n",
      "Iteration 142, loss = 0.42171753\n",
      "Iteration 143, loss = 0.42173435\n",
      "Iteration 144, loss = 0.42144739\n",
      "Iteration 145, loss = 0.42156423\n",
      "Iteration 146, loss = 0.42126058\n",
      "Iteration 147, loss = 0.42139979\n",
      "Iteration 148, loss = 0.42103859\n",
      "Iteration 149, loss = 0.42127101\n",
      "Iteration 150, loss = 0.42057689\n",
      "Iteration 151, loss = 0.42073174\n",
      "Iteration 152, loss = 0.42055527\n",
      "Iteration 153, loss = 0.42041920\n",
      "Iteration 154, loss = 0.42014690\n",
      "Iteration 155, loss = 0.42027064\n",
      "Iteration 156, loss = 0.42006077\n",
      "Iteration 157, loss = 0.41944519\n",
      "Iteration 158, loss = 0.42004360\n",
      "Iteration 159, loss = 0.41929611\n",
      "Iteration 160, loss = 0.41916879\n",
      "Iteration 161, loss = 0.41948937\n",
      "Iteration 162, loss = 0.41922398\n",
      "Iteration 163, loss = 0.41875486\n",
      "Iteration 164, loss = 0.41853602\n",
      "Iteration 165, loss = 0.41899881\n",
      "Iteration 166, loss = 0.41825625\n",
      "Iteration 167, loss = 0.41805065\n",
      "Iteration 168, loss = 0.41786989\n",
      "Iteration 169, loss = 0.41764005\n",
      "Iteration 170, loss = 0.41798362\n",
      "Iteration 171, loss = 0.41748166\n",
      "Iteration 172, loss = 0.41746601\n",
      "Iteration 173, loss = 0.41699898\n",
      "Iteration 174, loss = 0.41698609\n",
      "Iteration 175, loss = 0.41690445\n",
      "Iteration 176, loss = 0.41687912\n",
      "Iteration 177, loss = 0.41674598\n",
      "Iteration 178, loss = 0.41645445\n",
      "Iteration 179, loss = 0.41654010\n",
      "Iteration 180, loss = 0.41605457\n",
      "Iteration 181, loss = 0.41613645\n",
      "Iteration 182, loss = 0.41613313\n",
      "Iteration 183, loss = 0.41575079\n",
      "Iteration 184, loss = 0.41583064\n",
      "Iteration 185, loss = 0.41551288\n",
      "Iteration 186, loss = 0.41528955\n",
      "Iteration 187, loss = 0.41523706\n",
      "Iteration 188, loss = 0.41461841\n",
      "Iteration 189, loss = 0.41488447\n",
      "Iteration 190, loss = 0.41442410\n",
      "Iteration 191, loss = 0.41453568\n",
      "Iteration 192, loss = 0.41421939\n",
      "Iteration 193, loss = 0.41454825\n",
      "Iteration 194, loss = 0.41425861\n",
      "Iteration 195, loss = 0.41410115\n",
      "Iteration 196, loss = 0.41360366\n",
      "Iteration 197, loss = 0.41408440\n",
      "Iteration 198, loss = 0.41396376\n",
      "Iteration 199, loss = 0.41343475\n",
      "Iteration 200, loss = 0.41327418\n",
      "Iteration 201, loss = 0.41346870\n",
      "Iteration 202, loss = 0.41323279\n",
      "Iteration 203, loss = 0.41280655\n",
      "Iteration 204, loss = 0.41278255\n",
      "Iteration 205, loss = 0.41251758\n",
      "Iteration 206, loss = 0.41302396\n",
      "Iteration 207, loss = 0.41258123\n",
      "Iteration 208, loss = 0.41246205\n",
      "Iteration 209, loss = 0.41209044\n",
      "Iteration 210, loss = 0.41192622\n",
      "Iteration 211, loss = 0.41158633\n",
      "Iteration 212, loss = 0.41245087\n",
      "Iteration 213, loss = 0.41189525\n",
      "Iteration 214, loss = 0.41173789\n",
      "Iteration 215, loss = 0.41100041\n",
      "Iteration 216, loss = 0.41140354\n",
      "Iteration 217, loss = 0.41136262\n",
      "Iteration 218, loss = 0.41144006\n",
      "Iteration 219, loss = 0.41107813\n",
      "Iteration 220, loss = 0.41099853\n",
      "Iteration 221, loss = 0.41137286\n",
      "Iteration 222, loss = 0.41068313\n",
      "Iteration 223, loss = 0.41073141\n",
      "Iteration 224, loss = 0.41028501\n",
      "Iteration 225, loss = 0.41037071\n",
      "Iteration 226, loss = 0.40986966\n",
      "Iteration 227, loss = 0.40988797\n",
      "Iteration 228, loss = 0.40975773\n",
      "Iteration 229, loss = 0.41019615\n",
      "Iteration 230, loss = 0.40978883\n",
      "Iteration 231, loss = 0.40985345\n",
      "Iteration 232, loss = 0.40943378\n",
      "Iteration 233, loss = 0.40908151\n",
      "Iteration 234, loss = 0.40972159\n",
      "Iteration 235, loss = 0.40946778\n",
      "Iteration 236, loss = 0.40913489\n",
      "Iteration 237, loss = 0.40890201\n",
      "Iteration 238, loss = 0.40858131\n",
      "Iteration 239, loss = 0.40909361\n",
      "Iteration 240, loss = 0.40854925\n",
      "Iteration 241, loss = 0.40853907\n",
      "Iteration 242, loss = 0.40809869\n",
      "Iteration 243, loss = 0.40818686\n",
      "Iteration 244, loss = 0.40885720\n",
      "Iteration 245, loss = 0.40830253\n",
      "Iteration 246, loss = 0.40805588\n",
      "Iteration 247, loss = 0.40803145\n",
      "Iteration 248, loss = 0.40788689\n",
      "Iteration 249, loss = 0.40794177\n",
      "Iteration 250, loss = 0.40727706\n",
      "Iteration 251, loss = 0.40820079\n",
      "Iteration 252, loss = 0.40714245\n",
      "Iteration 253, loss = 0.40738220\n",
      "Iteration 254, loss = 0.40725588\n",
      "Iteration 255, loss = 0.40736213\n",
      "Iteration 256, loss = 0.40696002\n",
      "Iteration 257, loss = 0.40760595\n",
      "Iteration 258, loss = 0.40675557\n",
      "Iteration 259, loss = 0.40678722\n",
      "Iteration 260, loss = 0.40648474\n",
      "Iteration 261, loss = 0.40650127\n",
      "Iteration 262, loss = 0.40649765\n",
      "Iteration 263, loss = 0.40632733\n",
      "Iteration 264, loss = 0.40644052\n",
      "Iteration 265, loss = 0.40629448\n",
      "Iteration 266, loss = 0.40594099\n",
      "Iteration 267, loss = 0.40592909\n",
      "Iteration 268, loss = 0.40554870\n",
      "Iteration 269, loss = 0.40576426\n",
      "Iteration 270, loss = 0.40568676\n",
      "Iteration 271, loss = 0.40568099\n",
      "Iteration 272, loss = 0.40501021\n",
      "Iteration 273, loss = 0.40593191\n",
      "Iteration 274, loss = 0.40521085\n",
      "Iteration 275, loss = 0.40499224\n",
      "Iteration 276, loss = 0.40526706\n",
      "Iteration 277, loss = 0.40515902\n",
      "Iteration 278, loss = 0.40523442\n",
      "Iteration 279, loss = 0.40485927\n",
      "Iteration 280, loss = 0.40468195\n",
      "Iteration 281, loss = 0.40536087\n",
      "Iteration 282, loss = 0.40443665\n",
      "Iteration 283, loss = 0.40462231\n",
      "Iteration 284, loss = 0.40438581\n",
      "Iteration 285, loss = 0.40399201\n",
      "Iteration 286, loss = 0.40462423\n",
      "Iteration 287, loss = 0.40412430\n",
      "Iteration 288, loss = 0.40437876\n",
      "Iteration 289, loss = 0.40371709\n",
      "Iteration 290, loss = 0.40436807\n",
      "Iteration 291, loss = 0.40355631\n",
      "Iteration 292, loss = 0.40349857\n",
      "Iteration 293, loss = 0.40346604\n",
      "Iteration 294, loss = 0.40355090\n",
      "Iteration 295, loss = 0.40358648\n",
      "Iteration 296, loss = 0.40314140\n",
      "Iteration 297, loss = 0.40278136\n",
      "Iteration 298, loss = 0.40294100\n",
      "Iteration 299, loss = 0.40319927\n",
      "Iteration 300, loss = 0.40321252\n",
      "Iteration 301, loss = 0.40245903\n",
      "Iteration 302, loss = 0.40211359\n",
      "Iteration 303, loss = 0.40293179\n",
      "Iteration 304, loss = 0.40219624\n",
      "Iteration 305, loss = 0.40291308\n",
      "Iteration 306, loss = 0.40230230\n",
      "Iteration 307, loss = 0.40207584\n",
      "Iteration 308, loss = 0.40223415\n",
      "Iteration 309, loss = 0.40230709\n",
      "Iteration 310, loss = 0.40223753\n",
      "Iteration 311, loss = 0.40184503\n",
      "Iteration 312, loss = 0.40145306\n",
      "Iteration 313, loss = 0.40181601\n",
      "Iteration 314, loss = 0.40159364\n",
      "Iteration 315, loss = 0.40147726\n",
      "Iteration 316, loss = 0.40157809\n",
      "Iteration 317, loss = 0.40162946\n",
      "Iteration 318, loss = 0.40171466\n",
      "Iteration 319, loss = 0.40118966\n",
      "Iteration 320, loss = 0.40127537\n",
      "Iteration 321, loss = 0.40126676\n",
      "Iteration 322, loss = 0.40089513\n",
      "Iteration 323, loss = 0.40078414\n",
      "Iteration 324, loss = 0.40056238\n",
      "Iteration 325, loss = 0.40080501\n",
      "Iteration 326, loss = 0.40065202\n",
      "Iteration 327, loss = 0.40046586\n",
      "Iteration 328, loss = 0.40010392\n",
      "Iteration 329, loss = 0.40093128\n",
      "Iteration 330, loss = 0.40063997\n",
      "Iteration 331, loss = 0.40003554\n",
      "Iteration 332, loss = 0.40053035\n",
      "Iteration 333, loss = 0.40009981\n",
      "Iteration 334, loss = 0.40017622\n",
      "Iteration 335, loss = 0.40007820\n",
      "Iteration 336, loss = 0.40003784\n",
      "Iteration 337, loss = 0.39961246\n",
      "Iteration 338, loss = 0.39948497\n",
      "Iteration 339, loss = 0.39965941\n",
      "Iteration 340, loss = 0.39927822\n",
      "Iteration 341, loss = 0.39929614\n",
      "Iteration 342, loss = 0.39897002\n",
      "Iteration 343, loss = 0.39912812\n",
      "Iteration 344, loss = 0.39913236\n",
      "Iteration 345, loss = 0.39878524\n",
      "Iteration 346, loss = 0.39887064\n",
      "Iteration 347, loss = 0.39863553\n",
      "Iteration 348, loss = 0.39866492\n",
      "Iteration 349, loss = 0.39850878\n",
      "Iteration 350, loss = 0.39854797\n",
      "Iteration 351, loss = 0.39834834\n",
      "Iteration 352, loss = 0.39845842\n",
      "Iteration 353, loss = 0.39816590\n",
      "Iteration 354, loss = 0.39800139\n",
      "Iteration 355, loss = 0.39792572\n",
      "Iteration 356, loss = 0.39783292\n",
      "Iteration 357, loss = 0.39903393\n",
      "Iteration 358, loss = 0.39786099\n",
      "Iteration 359, loss = 0.39749767\n",
      "Iteration 360, loss = 0.39820447\n",
      "Iteration 361, loss = 0.39772873\n",
      "Iteration 362, loss = 0.39762373\n",
      "Iteration 363, loss = 0.39770311\n",
      "Iteration 364, loss = 0.39701047\n",
      "Iteration 365, loss = 0.39681842\n",
      "Iteration 366, loss = 0.39730088\n",
      "Iteration 367, loss = 0.39674338\n",
      "Iteration 368, loss = 0.39700480\n",
      "Iteration 369, loss = 0.39719182\n",
      "Iteration 370, loss = 0.39678788\n",
      "Iteration 371, loss = 0.39669352\n",
      "Iteration 372, loss = 0.39654861\n",
      "Iteration 373, loss = 0.39634553\n",
      "Iteration 374, loss = 0.39636988\n",
      "Iteration 375, loss = 0.39630266\n",
      "Iteration 376, loss = 0.39615646\n",
      "Iteration 377, loss = 0.39579249\n",
      "Iteration 378, loss = 0.39635945\n",
      "Iteration 379, loss = 0.39574191\n",
      "Iteration 380, loss = 0.39574847\n",
      "Iteration 381, loss = 0.39582634\n",
      "Iteration 382, loss = 0.39595049\n",
      "Iteration 383, loss = 0.39543783\n",
      "Iteration 384, loss = 0.39571446\n",
      "Iteration 385, loss = 0.39526927\n",
      "Iteration 386, loss = 0.39523545\n",
      "Iteration 387, loss = 0.39522134\n",
      "Iteration 388, loss = 0.39528886\n",
      "Iteration 389, loss = 0.39528245\n",
      "Iteration 390, loss = 0.39576975\n",
      "Iteration 391, loss = 0.39463519\n",
      "Iteration 392, loss = 0.39514488\n",
      "Iteration 393, loss = 0.39476542\n",
      "Iteration 394, loss = 0.39526043\n",
      "Iteration 395, loss = 0.39488844\n",
      "Iteration 396, loss = 0.39483796\n",
      "Iteration 397, loss = 0.39453766\n",
      "Iteration 398, loss = 0.39466801\n",
      "Iteration 399, loss = 0.39408941\n",
      "Iteration 400, loss = 0.39442190\n",
      "Iteration 401, loss = 0.39389710\n",
      "Iteration 402, loss = 0.39453166\n",
      "Iteration 403, loss = 0.39373711\n",
      "Iteration 404, loss = 0.39419897\n",
      "Iteration 405, loss = 0.39370024\n",
      "Iteration 406, loss = 0.39401357\n",
      "Iteration 407, loss = 0.39368430\n",
      "Iteration 408, loss = 0.39338222\n",
      "Iteration 409, loss = 0.39365218\n",
      "Iteration 410, loss = 0.39324100\n",
      "Iteration 411, loss = 0.39331585\n",
      "Iteration 412, loss = 0.39328225\n",
      "Iteration 413, loss = 0.39300725\n",
      "Iteration 414, loss = 0.39322294\n",
      "Iteration 415, loss = 0.39296153\n",
      "Iteration 416, loss = 0.39296635\n",
      "Iteration 417, loss = 0.39308593\n",
      "Iteration 418, loss = 0.39303314\n",
      "Iteration 419, loss = 0.39309667\n",
      "Iteration 420, loss = 0.39273764\n",
      "Iteration 421, loss = 0.39276008\n",
      "Iteration 422, loss = 0.39301685\n",
      "Iteration 423, loss = 0.39207048\n",
      "Iteration 424, loss = 0.39252852\n",
      "Iteration 425, loss = 0.39295371\n",
      "Iteration 426, loss = 0.39228457\n",
      "Iteration 427, loss = 0.39193896\n",
      "Iteration 428, loss = 0.39294708\n",
      "Iteration 429, loss = 0.39227806\n",
      "Iteration 430, loss = 0.39177532\n",
      "Iteration 431, loss = 0.39211902\n",
      "Iteration 432, loss = 0.39192619\n",
      "Iteration 433, loss = 0.39179307\n",
      "Iteration 434, loss = 0.39123378\n",
      "Iteration 435, loss = 0.39196314\n",
      "Iteration 436, loss = 0.39157923\n",
      "Iteration 437, loss = 0.39172693\n",
      "Iteration 438, loss = 0.39135678\n",
      "Iteration 439, loss = 0.39226203\n",
      "Iteration 440, loss = 0.39128829\n",
      "Iteration 441, loss = 0.39122537\n",
      "Iteration 442, loss = 0.39132220\n",
      "Iteration 443, loss = 0.39170161\n",
      "Iteration 444, loss = 0.39100007\n",
      "Iteration 445, loss = 0.39128757\n",
      "Iteration 446, loss = 0.39091049\n",
      "Iteration 447, loss = 0.39115973\n",
      "Iteration 448, loss = 0.39123003\n",
      "Iteration 449, loss = 0.39088963\n",
      "Iteration 450, loss = 0.39055908\n",
      "Iteration 451, loss = 0.39079103\n",
      "Iteration 452, loss = 0.39086250\n",
      "Iteration 453, loss = 0.39040381\n",
      "Iteration 454, loss = 0.39028147\n",
      "Iteration 455, loss = 0.38998855\n",
      "Iteration 456, loss = 0.39019268\n",
      "Iteration 457, loss = 0.38992152\n",
      "Iteration 458, loss = 0.39035756\n",
      "Iteration 459, loss = 0.39033210\n",
      "Iteration 460, loss = 0.39030002\n",
      "Iteration 461, loss = 0.39011208\n",
      "Iteration 462, loss = 0.39014682\n",
      "Iteration 463, loss = 0.38963066\n",
      "Iteration 464, loss = 0.38961107\n",
      "Iteration 465, loss = 0.39026603\n",
      "Iteration 466, loss = 0.38950465\n",
      "Iteration 467, loss = 0.38997001\n",
      "Iteration 468, loss = 0.38964688\n",
      "Iteration 469, loss = 0.38962402\n",
      "Iteration 470, loss = 0.38976960\n",
      "Iteration 471, loss = 0.38947303\n",
      "Iteration 472, loss = 0.38868393\n",
      "Iteration 473, loss = 0.38947681\n",
      "Iteration 474, loss = 0.38910366\n",
      "Iteration 475, loss = 0.38869659\n",
      "Iteration 476, loss = 0.38891601\n",
      "Iteration 477, loss = 0.38901481\n",
      "Iteration 478, loss = 0.38865948\n",
      "Iteration 479, loss = 0.38922438\n",
      "Iteration 480, loss = 0.38870687\n",
      "Iteration 481, loss = 0.38853153\n",
      "Iteration 482, loss = 0.38875935\n",
      "Iteration 483, loss = 0.38800236\n",
      "Iteration 484, loss = 0.38836680\n",
      "Iteration 485, loss = 0.38828441\n",
      "Iteration 486, loss = 0.38901285\n",
      "Iteration 487, loss = 0.38791525\n",
      "Iteration 488, loss = 0.38824854\n",
      "Iteration 489, loss = 0.38836151\n",
      "Iteration 490, loss = 0.38825011\n",
      "Iteration 491, loss = 0.38783838\n",
      "Iteration 492, loss = 0.38748012\n",
      "Iteration 493, loss = 0.38796836\n",
      "Iteration 494, loss = 0.38781551\n",
      "Iteration 495, loss = 0.38802489\n",
      "Iteration 496, loss = 0.38710655\n",
      "Iteration 497, loss = 0.38756125\n",
      "Iteration 498, loss = 0.38768935\n",
      "Iteration 499, loss = 0.38694280\n",
      "Iteration 500, loss = 0.38760658\n",
      "Iteration 501, loss = 0.38741383\n",
      "Iteration 502, loss = 0.38690088\n",
      "Iteration 503, loss = 0.38720697\n",
      "Iteration 504, loss = 0.38689060\n",
      "Iteration 505, loss = 0.38748757\n",
      "Iteration 506, loss = 0.38693002\n",
      "Iteration 507, loss = 0.38730216\n",
      "Iteration 508, loss = 0.38664363\n",
      "Iteration 509, loss = 0.38692394\n",
      "Iteration 510, loss = 0.38673186\n",
      "Iteration 511, loss = 0.38696329\n",
      "Iteration 512, loss = 0.38685629\n",
      "Iteration 513, loss = 0.38637543\n",
      "Iteration 514, loss = 0.38609930\n",
      "Iteration 515, loss = 0.38670671\n",
      "Iteration 516, loss = 0.38633817\n",
      "Iteration 517, loss = 0.38664251\n",
      "Iteration 518, loss = 0.38666025\n",
      "Iteration 519, loss = 0.38592974\n",
      "Iteration 520, loss = 0.38611778\n",
      "Iteration 521, loss = 0.38558238\n",
      "Iteration 522, loss = 0.38585809\n",
      "Iteration 523, loss = 0.38559421\n",
      "Iteration 524, loss = 0.38606205\n",
      "Iteration 525, loss = 0.38528464\n",
      "Iteration 526, loss = 0.38526508\n",
      "Iteration 527, loss = 0.38534101\n",
      "Iteration 528, loss = 0.38505930\n",
      "Iteration 529, loss = 0.38506608\n",
      "Iteration 530, loss = 0.38561534\n",
      "Iteration 531, loss = 0.38532829\n",
      "Iteration 532, loss = 0.38456629\n",
      "Iteration 533, loss = 0.38473736\n",
      "Iteration 534, loss = 0.38470269\n",
      "Iteration 535, loss = 0.38498853\n",
      "Iteration 536, loss = 0.38520862\n",
      "Iteration 537, loss = 0.38440558\n",
      "Iteration 538, loss = 0.38515710\n",
      "Iteration 539, loss = 0.38391147\n",
      "Iteration 540, loss = 0.38416201\n",
      "Iteration 541, loss = 0.38433490\n",
      "Iteration 542, loss = 0.38396161\n",
      "Iteration 543, loss = 0.38428590\n",
      "Iteration 544, loss = 0.38409313\n",
      "Iteration 545, loss = 0.38402635\n",
      "Iteration 546, loss = 0.38376767\n",
      "Iteration 547, loss = 0.38352662\n",
      "Iteration 548, loss = 0.38372925\n",
      "Iteration 549, loss = 0.38361020\n",
      "Iteration 550, loss = 0.38438598\n",
      "Iteration 551, loss = 0.38411142\n",
      "Iteration 552, loss = 0.38353039\n",
      "Iteration 553, loss = 0.38351160\n",
      "Iteration 554, loss = 0.38387251\n",
      "Iteration 555, loss = 0.38336316\n",
      "Iteration 556, loss = 0.38343137\n",
      "Iteration 557, loss = 0.38315176\n",
      "Iteration 558, loss = 0.38291094\n",
      "Iteration 559, loss = 0.38277500\n",
      "Iteration 560, loss = 0.38335260\n",
      "Iteration 561, loss = 0.38350728\n",
      "Iteration 562, loss = 0.38257709\n",
      "Iteration 563, loss = 0.38322633\n",
      "Iteration 564, loss = 0.38319893\n",
      "Iteration 565, loss = 0.38294495\n",
      "Iteration 566, loss = 0.38210186\n",
      "Iteration 567, loss = 0.38332245\n",
      "Iteration 568, loss = 0.38276893\n",
      "Iteration 569, loss = 0.38272405\n",
      "Iteration 570, loss = 0.38268629\n",
      "Iteration 571, loss = 0.38216877\n",
      "Iteration 572, loss = 0.38216357\n",
      "Iteration 573, loss = 0.38220176\n",
      "Iteration 574, loss = 0.38177054\n",
      "Iteration 575, loss = 0.38225134\n",
      "Iteration 576, loss = 0.38234109\n",
      "Iteration 577, loss = 0.38222878\n",
      "Iteration 578, loss = 0.38205254\n",
      "Iteration 579, loss = 0.38184990\n",
      "Iteration 580, loss = 0.38147871\n",
      "Iteration 581, loss = 0.38179470\n",
      "Iteration 582, loss = 0.38220294\n",
      "Iteration 583, loss = 0.38140992\n",
      "Iteration 584, loss = 0.38125548\n",
      "Iteration 585, loss = 0.38131251\n",
      "Iteration 586, loss = 0.38120790\n",
      "Iteration 587, loss = 0.38121936\n",
      "Iteration 588, loss = 0.38133996\n",
      "Iteration 589, loss = 0.38104393\n",
      "Iteration 590, loss = 0.38152863\n",
      "Iteration 591, loss = 0.38144023\n",
      "Iteration 592, loss = 0.38151182\n",
      "Iteration 593, loss = 0.38129468\n",
      "Iteration 594, loss = 0.38067641\n",
      "Iteration 595, loss = 0.38064850\n",
      "Iteration 596, loss = 0.38058840\n",
      "Iteration 597, loss = 0.38033398\n",
      "Iteration 598, loss = 0.38047194\n",
      "Iteration 599, loss = 0.38060742\n",
      "Iteration 600, loss = 0.38043927\n",
      "Iteration 601, loss = 0.38012690\n",
      "Iteration 602, loss = 0.38006093\n",
      "Iteration 603, loss = 0.38050123\n",
      "Iteration 604, loss = 0.37993504\n",
      "Iteration 605, loss = 0.38042516\n",
      "Iteration 606, loss = 0.37973923\n",
      "Iteration 607, loss = 0.38002778\n",
      "Iteration 608, loss = 0.38003207\n",
      "Iteration 609, loss = 0.37984002\n",
      "Iteration 610, loss = 0.37864997\n",
      "Iteration 611, loss = 0.38027113\n",
      "Iteration 612, loss = 0.37926286\n",
      "Iteration 613, loss = 0.38013236\n",
      "Iteration 614, loss = 0.37935623\n",
      "Iteration 615, loss = 0.37977143\n",
      "Iteration 616, loss = 0.37948630\n",
      "Iteration 617, loss = 0.37940809\n",
      "Iteration 618, loss = 0.37867330\n",
      "Iteration 619, loss = 0.37875705\n",
      "Iteration 620, loss = 0.37949446\n",
      "Iteration 621, loss = 0.38009721\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [01:31<03:47, 17.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.59986529\n",
      "Iteration 2, loss = 0.52829497\n",
      "Iteration 3, loss = 0.50103773\n",
      "Iteration 4, loss = 0.47989008\n",
      "Iteration 5, loss = 0.46896470\n",
      "Iteration 6, loss = 0.46275645\n",
      "Iteration 7, loss = 0.45939424\n",
      "Iteration 8, loss = 0.45691764\n",
      "Iteration 9, loss = 0.45501581\n",
      "Iteration 10, loss = 0.45399077\n",
      "Iteration 11, loss = 0.45278647\n",
      "Iteration 12, loss = 0.45217146\n",
      "Iteration 13, loss = 0.45114674\n",
      "Iteration 14, loss = 0.45058847\n",
      "Iteration 15, loss = 0.44995700\n",
      "Iteration 16, loss = 0.44931621\n",
      "Iteration 17, loss = 0.44868148\n",
      "Iteration 18, loss = 0.44831420\n",
      "Iteration 19, loss = 0.44789175\n",
      "Iteration 20, loss = 0.44757887\n",
      "Iteration 21, loss = 0.44696032\n",
      "Iteration 22, loss = 0.44642457\n",
      "Iteration 23, loss = 0.44629850\n",
      "Iteration 24, loss = 0.44583950\n",
      "Iteration 25, loss = 0.44530397\n",
      "Iteration 26, loss = 0.44476017\n",
      "Iteration 27, loss = 0.44449506\n",
      "Iteration 28, loss = 0.44421853\n",
      "Iteration 29, loss = 0.44393842\n",
      "Iteration 30, loss = 0.44320190\n",
      "Iteration 31, loss = 0.44334389\n",
      "Iteration 32, loss = 0.44278333\n",
      "Iteration 33, loss = 0.44274110\n",
      "Iteration 34, loss = 0.44227707\n",
      "Iteration 35, loss = 0.44202186\n",
      "Iteration 36, loss = 0.44164127\n",
      "Iteration 37, loss = 0.44150331\n",
      "Iteration 38, loss = 0.44095321\n",
      "Iteration 39, loss = 0.44109091\n",
      "Iteration 40, loss = 0.44035491\n",
      "Iteration 41, loss = 0.44048226\n",
      "Iteration 42, loss = 0.43995474\n",
      "Iteration 43, loss = 0.43966061\n",
      "Iteration 44, loss = 0.43940192\n",
      "Iteration 45, loss = 0.43917150\n",
      "Iteration 46, loss = 0.43857365\n",
      "Iteration 47, loss = 0.43846033\n",
      "Iteration 48, loss = 0.43821742\n",
      "Iteration 49, loss = 0.43806141\n",
      "Iteration 50, loss = 0.43776842\n",
      "Iteration 51, loss = 0.43742889\n",
      "Iteration 52, loss = 0.43702310\n",
      "Iteration 53, loss = 0.43691346\n",
      "Iteration 54, loss = 0.43652401\n",
      "Iteration 55, loss = 0.43635459\n",
      "Iteration 56, loss = 0.43629107\n",
      "Iteration 57, loss = 0.43594142\n",
      "Iteration 58, loss = 0.43603871\n",
      "Iteration 59, loss = 0.43536472\n",
      "Iteration 60, loss = 0.43525489\n",
      "Iteration 61, loss = 0.43502348\n",
      "Iteration 62, loss = 0.43456735\n",
      "Iteration 63, loss = 0.43444153\n",
      "Iteration 64, loss = 0.43422990\n",
      "Iteration 65, loss = 0.43419923\n",
      "Iteration 66, loss = 0.43389670\n",
      "Iteration 67, loss = 0.43350649\n",
      "Iteration 68, loss = 0.43332960\n",
      "Iteration 69, loss = 0.43346362\n",
      "Iteration 70, loss = 0.43268288\n",
      "Iteration 71, loss = 0.43284623\n",
      "Iteration 72, loss = 0.43205958\n",
      "Iteration 73, loss = 0.43260026\n",
      "Iteration 74, loss = 0.43194958\n",
      "Iteration 75, loss = 0.43177106\n",
      "Iteration 76, loss = 0.43166496\n",
      "Iteration 77, loss = 0.43133654\n",
      "Iteration 78, loss = 0.43104797\n",
      "Iteration 79, loss = 0.43082693\n",
      "Iteration 80, loss = 0.43075552\n",
      "Iteration 81, loss = 0.43044790\n",
      "Iteration 82, loss = 0.43025978\n",
      "Iteration 83, loss = 0.43001019\n",
      "Iteration 84, loss = 0.43009546\n",
      "Iteration 85, loss = 0.42964720\n",
      "Iteration 86, loss = 0.42952002\n",
      "Iteration 87, loss = 0.42924326\n",
      "Iteration 88, loss = 0.42899409\n",
      "Iteration 89, loss = 0.42896896\n",
      "Iteration 90, loss = 0.42893966\n",
      "Iteration 91, loss = 0.42853841\n",
      "Iteration 92, loss = 0.42804634\n",
      "Iteration 93, loss = 0.42828652\n",
      "Iteration 94, loss = 0.42786882\n",
      "Iteration 95, loss = 0.42777556\n",
      "Iteration 96, loss = 0.42771716\n",
      "Iteration 97, loss = 0.42759323\n",
      "Iteration 98, loss = 0.42723833\n",
      "Iteration 99, loss = 0.42710451\n",
      "Iteration 100, loss = 0.42666417\n",
      "Iteration 101, loss = 0.42666051\n",
      "Iteration 102, loss = 0.42677196\n",
      "Iteration 103, loss = 0.42631876\n",
      "Iteration 104, loss = 0.42626404\n",
      "Iteration 105, loss = 0.42615433\n",
      "Iteration 106, loss = 0.42564958\n",
      "Iteration 107, loss = 0.42564678\n",
      "Iteration 108, loss = 0.42546800\n",
      "Iteration 109, loss = 0.42513692\n",
      "Iteration 110, loss = 0.42518440\n",
      "Iteration 111, loss = 0.42473764\n",
      "Iteration 112, loss = 0.42510092\n",
      "Iteration 113, loss = 0.42446886\n",
      "Iteration 114, loss = 0.42414188\n",
      "Iteration 115, loss = 0.42399151\n",
      "Iteration 116, loss = 0.42382438\n",
      "Iteration 117, loss = 0.42387247\n",
      "Iteration 118, loss = 0.42380588\n",
      "Iteration 119, loss = 0.42370972\n",
      "Iteration 120, loss = 0.42351543\n",
      "Iteration 121, loss = 0.42349196\n",
      "Iteration 122, loss = 0.42363174\n",
      "Iteration 123, loss = 0.42301070\n",
      "Iteration 124, loss = 0.42316442\n",
      "Iteration 125, loss = 0.42277770\n",
      "Iteration 126, loss = 0.42224651\n",
      "Iteration 127, loss = 0.42263893\n",
      "Iteration 128, loss = 0.42236367\n",
      "Iteration 129, loss = 0.42206388\n",
      "Iteration 130, loss = 0.42201269\n",
      "Iteration 131, loss = 0.42189366\n",
      "Iteration 132, loss = 0.42140803\n",
      "Iteration 133, loss = 0.42155751\n",
      "Iteration 134, loss = 0.42139233\n",
      "Iteration 135, loss = 0.42086342\n",
      "Iteration 136, loss = 0.42117149\n",
      "Iteration 137, loss = 0.42067260\n",
      "Iteration 138, loss = 0.42108073\n",
      "Iteration 139, loss = 0.42095793\n",
      "Iteration 140, loss = 0.42037951\n",
      "Iteration 141, loss = 0.42032277\n",
      "Iteration 142, loss = 0.42019019\n",
      "Iteration 143, loss = 0.42006515\n",
      "Iteration 144, loss = 0.41985778\n",
      "Iteration 145, loss = 0.41953423\n",
      "Iteration 146, loss = 0.41942724\n",
      "Iteration 147, loss = 0.41977679\n",
      "Iteration 148, loss = 0.41922724\n",
      "Iteration 149, loss = 0.41899198\n",
      "Iteration 150, loss = 0.41900347\n",
      "Iteration 151, loss = 0.41946400\n",
      "Iteration 152, loss = 0.41922959\n",
      "Iteration 153, loss = 0.41840433\n",
      "Iteration 154, loss = 0.41818671\n",
      "Iteration 155, loss = 0.41831396\n",
      "Iteration 156, loss = 0.41780231\n",
      "Iteration 157, loss = 0.41779199\n",
      "Iteration 158, loss = 0.41828947\n",
      "Iteration 159, loss = 0.41770068\n",
      "Iteration 160, loss = 0.41748835\n",
      "Iteration 161, loss = 0.41817019\n",
      "Iteration 162, loss = 0.41689350\n",
      "Iteration 163, loss = 0.41657115\n",
      "Iteration 164, loss = 0.41678654\n",
      "Iteration 165, loss = 0.41622739\n",
      "Iteration 166, loss = 0.41683756\n",
      "Iteration 167, loss = 0.41676039\n",
      "Iteration 168, loss = 0.41633221\n",
      "Iteration 169, loss = 0.41626354\n",
      "Iteration 170, loss = 0.41632837\n",
      "Iteration 171, loss = 0.41582845\n",
      "Iteration 172, loss = 0.41567162\n",
      "Iteration 173, loss = 0.41593059\n",
      "Iteration 174, loss = 0.41547637\n",
      "Iteration 175, loss = 0.41555818\n",
      "Iteration 176, loss = 0.41506952\n",
      "Iteration 177, loss = 0.41509409\n",
      "Iteration 178, loss = 0.41518258\n",
      "Iteration 179, loss = 0.41494601\n",
      "Iteration 180, loss = 0.41462865\n",
      "Iteration 181, loss = 0.41449675\n",
      "Iteration 182, loss = 0.41409128\n",
      "Iteration 183, loss = 0.41463618\n",
      "Iteration 184, loss = 0.41392551\n",
      "Iteration 185, loss = 0.41403756\n",
      "Iteration 186, loss = 0.41345860\n",
      "Iteration 187, loss = 0.41373691\n",
      "Iteration 188, loss = 0.41385335\n",
      "Iteration 189, loss = 0.41360990\n",
      "Iteration 190, loss = 0.41355689\n",
      "Iteration 191, loss = 0.41327045\n",
      "Iteration 192, loss = 0.41295570\n",
      "Iteration 193, loss = 0.41283411\n",
      "Iteration 194, loss = 0.41239334\n",
      "Iteration 195, loss = 0.41259389\n",
      "Iteration 196, loss = 0.41235535\n",
      "Iteration 197, loss = 0.41221632\n",
      "Iteration 198, loss = 0.41171389\n",
      "Iteration 199, loss = 0.41193134\n",
      "Iteration 200, loss = 0.41229100\n",
      "Iteration 201, loss = 0.41175952\n",
      "Iteration 202, loss = 0.41156239\n",
      "Iteration 203, loss = 0.41157766\n",
      "Iteration 204, loss = 0.41133782\n",
      "Iteration 205, loss = 0.41155085\n",
      "Iteration 206, loss = 0.41091073\n",
      "Iteration 207, loss = 0.41070892\n",
      "Iteration 208, loss = 0.41081340\n",
      "Iteration 209, loss = 0.41018577\n",
      "Iteration 210, loss = 0.41034858\n",
      "Iteration 211, loss = 0.41018078\n",
      "Iteration 212, loss = 0.41120401\n",
      "Iteration 213, loss = 0.40993359\n",
      "Iteration 214, loss = 0.41001830\n",
      "Iteration 215, loss = 0.41039671\n",
      "Iteration 216, loss = 0.40983814\n",
      "Iteration 217, loss = 0.40933747\n",
      "Iteration 218, loss = 0.40956073\n",
      "Iteration 219, loss = 0.40913991\n",
      "Iteration 220, loss = 0.40964920\n",
      "Iteration 221, loss = 0.40888105\n",
      "Iteration 222, loss = 0.40898624\n",
      "Iteration 223, loss = 0.40971463\n",
      "Iteration 224, loss = 0.40874464\n",
      "Iteration 225, loss = 0.40912541\n",
      "Iteration 226, loss = 0.40853940\n",
      "Iteration 227, loss = 0.40826681\n",
      "Iteration 228, loss = 0.40821407\n",
      "Iteration 229, loss = 0.40779029\n",
      "Iteration 230, loss = 0.40770040\n",
      "Iteration 231, loss = 0.40783427\n",
      "Iteration 232, loss = 0.40749167\n",
      "Iteration 233, loss = 0.40741440\n",
      "Iteration 234, loss = 0.40817130\n",
      "Iteration 235, loss = 0.40685507\n",
      "Iteration 236, loss = 0.40704898\n",
      "Iteration 237, loss = 0.40696697\n",
      "Iteration 238, loss = 0.40668781\n",
      "Iteration 239, loss = 0.40662251\n",
      "Iteration 240, loss = 0.40607737\n",
      "Iteration 241, loss = 0.40604443\n",
      "Iteration 242, loss = 0.40639200\n",
      "Iteration 243, loss = 0.40618435\n",
      "Iteration 244, loss = 0.40601218\n",
      "Iteration 245, loss = 0.40637389\n",
      "Iteration 246, loss = 0.40596838\n",
      "Iteration 247, loss = 0.40550901\n",
      "Iteration 248, loss = 0.40582791\n",
      "Iteration 249, loss = 0.40521158\n",
      "Iteration 250, loss = 0.40533379\n",
      "Iteration 251, loss = 0.40531326\n",
      "Iteration 252, loss = 0.40486518\n",
      "Iteration 253, loss = 0.40457992\n",
      "Iteration 254, loss = 0.40469889\n",
      "Iteration 255, loss = 0.40451820\n",
      "Iteration 256, loss = 0.40480143\n",
      "Iteration 257, loss = 0.40492174\n",
      "Iteration 258, loss = 0.40398560\n",
      "Iteration 259, loss = 0.40405317\n",
      "Iteration 260, loss = 0.40424711\n",
      "Iteration 261, loss = 0.40404589\n",
      "Iteration 262, loss = 0.40362213\n",
      "Iteration 263, loss = 0.40351416\n",
      "Iteration 264, loss = 0.40349239\n",
      "Iteration 265, loss = 0.40369893\n",
      "Iteration 266, loss = 0.40312163\n",
      "Iteration 267, loss = 0.40279556\n",
      "Iteration 268, loss = 0.40331169\n",
      "Iteration 269, loss = 0.40314865\n",
      "Iteration 270, loss = 0.40283202\n",
      "Iteration 271, loss = 0.40317363\n",
      "Iteration 272, loss = 0.40266762\n",
      "Iteration 273, loss = 0.40229013\n",
      "Iteration 274, loss = 0.40243784\n",
      "Iteration 275, loss = 0.40210722\n",
      "Iteration 276, loss = 0.40239735\n",
      "Iteration 277, loss = 0.40203847\n",
      "Iteration 278, loss = 0.40217277\n",
      "Iteration 279, loss = 0.40209511\n",
      "Iteration 280, loss = 0.40152191\n",
      "Iteration 281, loss = 0.40150102\n",
      "Iteration 282, loss = 0.40187460\n",
      "Iteration 283, loss = 0.40137679\n",
      "Iteration 284, loss = 0.40142857\n",
      "Iteration 285, loss = 0.40158211\n",
      "Iteration 286, loss = 0.40126763\n",
      "Iteration 287, loss = 0.40075816\n",
      "Iteration 288, loss = 0.40099943\n",
      "Iteration 289, loss = 0.40160071\n",
      "Iteration 290, loss = 0.40064646\n",
      "Iteration 291, loss = 0.40059005\n",
      "Iteration 292, loss = 0.40119633\n",
      "Iteration 293, loss = 0.40025021\n",
      "Iteration 294, loss = 0.40022266\n",
      "Iteration 295, loss = 0.40029543\n",
      "Iteration 296, loss = 0.40017970\n",
      "Iteration 297, loss = 0.40020231\n",
      "Iteration 298, loss = 0.40090175\n",
      "Iteration 299, loss = 0.39982679\n",
      "Iteration 300, loss = 0.40041407\n",
      "Iteration 301, loss = 0.39920765\n",
      "Iteration 302, loss = 0.39956335\n",
      "Iteration 303, loss = 0.39943985\n",
      "Iteration 304, loss = 0.39952718\n",
      "Iteration 305, loss = 0.39904666\n",
      "Iteration 306, loss = 0.39881251\n",
      "Iteration 307, loss = 0.39891801\n",
      "Iteration 308, loss = 0.39870227\n",
      "Iteration 309, loss = 0.39818636\n",
      "Iteration 310, loss = 0.39900809\n",
      "Iteration 311, loss = 0.39839406\n",
      "Iteration 312, loss = 0.39849070\n",
      "Iteration 313, loss = 0.39897077\n",
      "Iteration 314, loss = 0.39897192\n",
      "Iteration 315, loss = 0.39788217\n",
      "Iteration 316, loss = 0.39878514\n",
      "Iteration 317, loss = 0.39853911\n",
      "Iteration 318, loss = 0.39803479\n",
      "Iteration 319, loss = 0.39800278\n",
      "Iteration 320, loss = 0.39801700\n",
      "Iteration 321, loss = 0.39770930\n",
      "Iteration 322, loss = 0.39798265\n",
      "Iteration 323, loss = 0.39689102\n",
      "Iteration 324, loss = 0.39727733\n",
      "Iteration 325, loss = 0.39656530\n",
      "Iteration 326, loss = 0.39690284\n",
      "Iteration 327, loss = 0.39738687\n",
      "Iteration 328, loss = 0.39672867\n",
      "Iteration 329, loss = 0.39739903\n",
      "Iteration 330, loss = 0.39640426\n",
      "Iteration 331, loss = 0.39725301\n",
      "Iteration 332, loss = 0.39578774\n",
      "Iteration 333, loss = 0.39592909\n",
      "Iteration 334, loss = 0.39634873\n",
      "Iteration 335, loss = 0.39669955\n",
      "Iteration 336, loss = 0.39668278\n",
      "Iteration 337, loss = 0.39607670\n",
      "Iteration 338, loss = 0.39584661\n",
      "Iteration 339, loss = 0.39613447\n",
      "Iteration 340, loss = 0.39555314\n",
      "Iteration 341, loss = 0.39563892\n",
      "Iteration 342, loss = 0.39628498\n",
      "Iteration 343, loss = 0.39624034\n",
      "Iteration 344, loss = 0.39599531\n",
      "Iteration 345, loss = 0.39555454\n",
      "Iteration 346, loss = 0.39498405\n",
      "Iteration 347, loss = 0.39492899\n",
      "Iteration 348, loss = 0.39504823\n",
      "Iteration 349, loss = 0.39501779\n",
      "Iteration 350, loss = 0.39538216\n",
      "Iteration 351, loss = 0.39538054\n",
      "Iteration 352, loss = 0.39519546\n",
      "Iteration 353, loss = 0.39497625\n",
      "Iteration 354, loss = 0.39494244\n",
      "Iteration 355, loss = 0.39499254\n",
      "Iteration 356, loss = 0.39459817\n",
      "Iteration 357, loss = 0.39441891\n",
      "Iteration 358, loss = 0.39430574\n",
      "Iteration 359, loss = 0.39447577\n",
      "Iteration 360, loss = 0.39385678\n",
      "Iteration 361, loss = 0.39397738\n",
      "Iteration 362, loss = 0.39434137\n",
      "Iteration 363, loss = 0.39399464\n",
      "Iteration 364, loss = 0.39326063\n",
      "Iteration 365, loss = 0.39391451\n",
      "Iteration 366, loss = 0.39354117\n",
      "Iteration 367, loss = 0.39404520\n",
      "Iteration 368, loss = 0.39362264\n",
      "Iteration 369, loss = 0.39375670\n",
      "Iteration 370, loss = 0.39377345\n",
      "Iteration 371, loss = 0.39314512\n",
      "Iteration 372, loss = 0.39317428\n",
      "Iteration 373, loss = 0.39315810\n",
      "Iteration 374, loss = 0.39319130\n",
      "Iteration 375, loss = 0.39262373\n",
      "Iteration 376, loss = 0.39267673\n",
      "Iteration 377, loss = 0.39245262\n",
      "Iteration 378, loss = 0.39233296\n",
      "Iteration 379, loss = 0.39319308\n",
      "Iteration 380, loss = 0.39237123\n",
      "Iteration 381, loss = 0.39264700\n",
      "Iteration 382, loss = 0.39203819\n",
      "Iteration 383, loss = 0.39187232\n",
      "Iteration 384, loss = 0.39175253\n",
      "Iteration 385, loss = 0.39214692\n",
      "Iteration 386, loss = 0.39171762\n",
      "Iteration 387, loss = 0.39087449\n",
      "Iteration 388, loss = 0.39077925\n",
      "Iteration 389, loss = 0.39133429\n",
      "Iteration 390, loss = 0.39197387\n",
      "Iteration 391, loss = 0.39230842\n",
      "Iteration 392, loss = 0.39130699\n",
      "Iteration 393, loss = 0.39155591\n",
      "Iteration 394, loss = 0.39092886\n",
      "Iteration 395, loss = 0.39081160\n",
      "Iteration 396, loss = 0.39080204\n",
      "Iteration 397, loss = 0.39053440\n",
      "Iteration 398, loss = 0.39034549\n",
      "Iteration 399, loss = 0.39111086\n",
      "Iteration 400, loss = 0.39007076\n",
      "Iteration 401, loss = 0.38984173\n",
      "Iteration 402, loss = 0.38987419\n",
      "Iteration 403, loss = 0.39046164\n",
      "Iteration 404, loss = 0.38999093\n",
      "Iteration 405, loss = 0.39014195\n",
      "Iteration 406, loss = 0.38995744\n",
      "Iteration 407, loss = 0.38978850\n",
      "Iteration 408, loss = 0.39003379\n",
      "Iteration 409, loss = 0.38955700\n",
      "Iteration 410, loss = 0.38990651\n",
      "Iteration 411, loss = 0.38940322\n",
      "Iteration 412, loss = 0.38954292\n",
      "Iteration 413, loss = 0.38979559\n",
      "Iteration 414, loss = 0.38899373\n",
      "Iteration 415, loss = 0.38970364\n",
      "Iteration 416, loss = 0.38868672\n",
      "Iteration 417, loss = 0.38895433\n",
      "Iteration 418, loss = 0.38943611\n",
      "Iteration 419, loss = 0.38869327\n",
      "Iteration 420, loss = 0.38882776\n",
      "Iteration 421, loss = 0.38853834\n",
      "Iteration 422, loss = 0.38894706\n",
      "Iteration 423, loss = 0.38894915\n",
      "Iteration 424, loss = 0.38848206\n",
      "Iteration 425, loss = 0.38808129\n",
      "Iteration 426, loss = 0.38823841\n",
      "Iteration 427, loss = 0.38843675\n",
      "Iteration 428, loss = 0.38792037\n",
      "Iteration 429, loss = 0.38832213\n",
      "Iteration 430, loss = 0.38778145\n",
      "Iteration 431, loss = 0.38765723\n",
      "Iteration 432, loss = 0.38753326\n",
      "Iteration 433, loss = 0.38865546\n",
      "Iteration 434, loss = 0.38788334\n",
      "Iteration 435, loss = 0.38743579\n",
      "Iteration 436, loss = 0.38806131\n",
      "Iteration 437, loss = 0.38763982\n",
      "Iteration 438, loss = 0.38708299\n",
      "Iteration 439, loss = 0.38731029\n",
      "Iteration 440, loss = 0.38724157\n",
      "Iteration 441, loss = 0.38777722\n",
      "Iteration 442, loss = 0.38727626\n",
      "Iteration 443, loss = 0.38776835\n",
      "Iteration 444, loss = 0.38708871\n",
      "Iteration 445, loss = 0.38728500\n",
      "Iteration 446, loss = 0.38629526\n",
      "Iteration 447, loss = 0.38719132\n",
      "Iteration 448, loss = 0.38717320\n",
      "Iteration 449, loss = 0.38668043\n",
      "Iteration 450, loss = 0.38710485\n",
      "Iteration 451, loss = 0.38658754\n",
      "Iteration 452, loss = 0.38698454\n",
      "Iteration 453, loss = 0.38612109\n",
      "Iteration 454, loss = 0.38659813\n",
      "Iteration 455, loss = 0.38581757\n",
      "Iteration 456, loss = 0.38618551\n",
      "Iteration 457, loss = 0.38642654\n",
      "Iteration 458, loss = 0.38579020\n",
      "Iteration 459, loss = 0.38633381\n",
      "Iteration 460, loss = 0.38619735\n",
      "Iteration 461, loss = 0.38590497\n",
      "Iteration 462, loss = 0.38611360\n",
      "Iteration 463, loss = 0.38546203\n",
      "Iteration 464, loss = 0.38604757\n",
      "Iteration 465, loss = 0.38568233\n",
      "Iteration 466, loss = 0.38591419\n",
      "Iteration 467, loss = 0.38600558\n",
      "Iteration 468, loss = 0.38531402\n",
      "Iteration 469, loss = 0.38509982\n",
      "Iteration 470, loss = 0.38577082\n",
      "Iteration 471, loss = 0.38568948\n",
      "Iteration 472, loss = 0.38502800\n",
      "Iteration 473, loss = 0.38527932\n",
      "Iteration 474, loss = 0.38509735\n",
      "Iteration 475, loss = 0.38439725\n",
      "Iteration 476, loss = 0.38508554\n",
      "Iteration 477, loss = 0.38438167\n",
      "Iteration 478, loss = 0.38436278\n",
      "Iteration 479, loss = 0.38403280\n",
      "Iteration 480, loss = 0.38433696\n",
      "Iteration 481, loss = 0.38468792\n",
      "Iteration 482, loss = 0.38478796\n",
      "Iteration 483, loss = 0.38464609\n",
      "Iteration 484, loss = 0.38425258\n",
      "Iteration 485, loss = 0.38443672\n",
      "Iteration 486, loss = 0.38418404\n",
      "Iteration 487, loss = 0.38420529\n",
      "Iteration 488, loss = 0.38369128\n",
      "Iteration 489, loss = 0.38424020\n",
      "Iteration 490, loss = 0.38370739\n",
      "Iteration 491, loss = 0.38365163\n",
      "Iteration 492, loss = 0.38440006\n",
      "Iteration 493, loss = 0.38329613\n",
      "Iteration 494, loss = 0.38376554\n",
      "Iteration 495, loss = 0.38314283\n",
      "Iteration 496, loss = 0.38353392\n",
      "Iteration 497, loss = 0.38234091\n",
      "Iteration 498, loss = 0.38248393\n",
      "Iteration 499, loss = 0.38381461\n",
      "Iteration 500, loss = 0.38240448\n",
      "Iteration 501, loss = 0.38271718\n",
      "Iteration 502, loss = 0.38316409\n",
      "Iteration 503, loss = 0.38246343\n",
      "Iteration 504, loss = 0.38254283\n",
      "Iteration 505, loss = 0.38215510\n",
      "Iteration 506, loss = 0.38206687\n",
      "Iteration 507, loss = 0.38275804\n",
      "Iteration 508, loss = 0.38213277\n",
      "Iteration 509, loss = 0.38225964\n",
      "Iteration 510, loss = 0.38244080\n",
      "Iteration 511, loss = 0.38232236\n",
      "Iteration 512, loss = 0.38163976\n",
      "Iteration 513, loss = 0.38236801\n",
      "Iteration 514, loss = 0.38224855\n",
      "Iteration 515, loss = 0.38127902\n",
      "Iteration 516, loss = 0.38254543\n",
      "Iteration 517, loss = 0.38178171\n",
      "Iteration 518, loss = 0.38240690\n",
      "Iteration 519, loss = 0.38177697\n",
      "Iteration 520, loss = 0.38122908\n",
      "Iteration 521, loss = 0.38175035\n",
      "Iteration 522, loss = 0.38153953\n",
      "Iteration 523, loss = 0.38222158\n",
      "Iteration 524, loss = 0.38105984\n",
      "Iteration 525, loss = 0.38212866\n",
      "Iteration 526, loss = 0.38105050\n",
      "Iteration 527, loss = 0.38233172\n",
      "Iteration 528, loss = 0.38064514\n",
      "Iteration 529, loss = 0.38079979\n",
      "Iteration 530, loss = 0.38103676\n",
      "Iteration 531, loss = 0.38093365\n",
      "Iteration 532, loss = 0.38072225\n",
      "Iteration 533, loss = 0.38080968\n",
      "Iteration 534, loss = 0.38015047\n",
      "Iteration 535, loss = 0.38071158\n",
      "Iteration 536, loss = 0.38062903\n",
      "Iteration 537, loss = 0.37955838\n",
      "Iteration 538, loss = 0.38014692\n",
      "Iteration 539, loss = 0.38038671\n",
      "Iteration 540, loss = 0.37984224\n",
      "Iteration 541, loss = 0.38027965\n",
      "Iteration 542, loss = 0.38017849\n",
      "Iteration 543, loss = 0.38002919\n",
      "Iteration 544, loss = 0.37959702\n",
      "Iteration 545, loss = 0.38092590\n",
      "Iteration 546, loss = 0.37968333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [01:54<03:50, 19.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 547, loss = 0.37974598\n",
      "Iteration 548, loss = 0.37948710\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55542378\n",
      "Iteration 2, loss = 0.50766127\n",
      "Iteration 3, loss = 0.48110291\n",
      "Iteration 4, loss = 0.46853897\n",
      "Iteration 5, loss = 0.46246335\n",
      "Iteration 6, loss = 0.45843445\n",
      "Iteration 7, loss = 0.45590687\n",
      "Iteration 8, loss = 0.45379719\n",
      "Iteration 9, loss = 0.45213783\n",
      "Iteration 10, loss = 0.45119832\n",
      "Iteration 11, loss = 0.44993811\n",
      "Iteration 12, loss = 0.44920150\n",
      "Iteration 13, loss = 0.44854550\n",
      "Iteration 14, loss = 0.44771422\n",
      "Iteration 15, loss = 0.44724064\n",
      "Iteration 16, loss = 0.44672895\n",
      "Iteration 17, loss = 0.44601501\n",
      "Iteration 18, loss = 0.44526971\n",
      "Iteration 19, loss = 0.44535107\n",
      "Iteration 20, loss = 0.44438457\n",
      "Iteration 21, loss = 0.44371375\n",
      "Iteration 22, loss = 0.44331497\n",
      "Iteration 23, loss = 0.44301293\n",
      "Iteration 24, loss = 0.44281787\n",
      "Iteration 25, loss = 0.44257312\n",
      "Iteration 26, loss = 0.44200095\n",
      "Iteration 27, loss = 0.44187955\n",
      "Iteration 28, loss = 0.44139083\n",
      "Iteration 29, loss = 0.44109085\n",
      "Iteration 30, loss = 0.44089894\n",
      "Iteration 31, loss = 0.44032192\n",
      "Iteration 32, loss = 0.43979797\n",
      "Iteration 33, loss = 0.43988546\n",
      "Iteration 34, loss = 0.43933572\n",
      "Iteration 35, loss = 0.43896386\n",
      "Iteration 36, loss = 0.43850657\n",
      "Iteration 37, loss = 0.43858295\n",
      "Iteration 38, loss = 0.43786248\n",
      "Iteration 39, loss = 0.43776862\n",
      "Iteration 40, loss = 0.43738335\n",
      "Iteration 41, loss = 0.43709076\n",
      "Iteration 42, loss = 0.43685624\n",
      "Iteration 43, loss = 0.43654927\n",
      "Iteration 44, loss = 0.43616149\n",
      "Iteration 45, loss = 0.43580023\n",
      "Iteration 46, loss = 0.43575690\n",
      "Iteration 47, loss = 0.43517788\n",
      "Iteration 48, loss = 0.43479727\n",
      "Iteration 49, loss = 0.43496465\n",
      "Iteration 50, loss = 0.43443330\n",
      "Iteration 51, loss = 0.43425186\n",
      "Iteration 52, loss = 0.43373584\n",
      "Iteration 53, loss = 0.43371830\n",
      "Iteration 54, loss = 0.43323394\n",
      "Iteration 55, loss = 0.43335161\n",
      "Iteration 56, loss = 0.43285111\n",
      "Iteration 57, loss = 0.43271492\n",
      "Iteration 58, loss = 0.43219205\n",
      "Iteration 59, loss = 0.43232892\n",
      "Iteration 60, loss = 0.43180512\n",
      "Iteration 61, loss = 0.43164717\n",
      "Iteration 62, loss = 0.43116647\n",
      "Iteration 63, loss = 0.43112907\n",
      "Iteration 64, loss = 0.43072974\n",
      "Iteration 65, loss = 0.43061406\n",
      "Iteration 66, loss = 0.43049725\n",
      "Iteration 67, loss = 0.43015484\n",
      "Iteration 68, loss = 0.43006044\n",
      "Iteration 69, loss = 0.42994971\n",
      "Iteration 70, loss = 0.42926156\n",
      "Iteration 71, loss = 0.42936915\n",
      "Iteration 72, loss = 0.42915276\n",
      "Iteration 73, loss = 0.42904733\n",
      "Iteration 74, loss = 0.42854801\n",
      "Iteration 75, loss = 0.42836624\n",
      "Iteration 76, loss = 0.42813232\n",
      "Iteration 77, loss = 0.42792554\n",
      "Iteration 78, loss = 0.42768779\n",
      "Iteration 79, loss = 0.42787277\n",
      "Iteration 80, loss = 0.42726355\n",
      "Iteration 81, loss = 0.42728199\n",
      "Iteration 82, loss = 0.42707537\n",
      "Iteration 83, loss = 0.42675427\n",
      "Iteration 84, loss = 0.42661118\n",
      "Iteration 85, loss = 0.42633759\n",
      "Iteration 86, loss = 0.42630874\n",
      "Iteration 87, loss = 0.42594681\n",
      "Iteration 88, loss = 0.42585576\n",
      "Iteration 89, loss = 0.42563298\n",
      "Iteration 90, loss = 0.42539918\n",
      "Iteration 91, loss = 0.42506807\n",
      "Iteration 92, loss = 0.42499684\n",
      "Iteration 93, loss = 0.42478597\n",
      "Iteration 94, loss = 0.42453827\n",
      "Iteration 95, loss = 0.42434431\n",
      "Iteration 96, loss = 0.42433751\n",
      "Iteration 97, loss = 0.42409693\n",
      "Iteration 98, loss = 0.42418910\n",
      "Iteration 99, loss = 0.42413471\n",
      "Iteration 100, loss = 0.42371930\n",
      "Iteration 101, loss = 0.42329542\n",
      "Iteration 102, loss = 0.42348832\n",
      "Iteration 103, loss = 0.42284754\n",
      "Iteration 104, loss = 0.42290030\n",
      "Iteration 105, loss = 0.42291566\n",
      "Iteration 106, loss = 0.42272975\n",
      "Iteration 107, loss = 0.42258584\n",
      "Iteration 108, loss = 0.42249871\n",
      "Iteration 109, loss = 0.42222146\n",
      "Iteration 110, loss = 0.42182945\n",
      "Iteration 111, loss = 0.42196840\n",
      "Iteration 112, loss = 0.42149683\n",
      "Iteration 113, loss = 0.42148890\n",
      "Iteration 114, loss = 0.42110049\n",
      "Iteration 115, loss = 0.42091925\n",
      "Iteration 116, loss = 0.42079268\n",
      "Iteration 117, loss = 0.42076654\n",
      "Iteration 118, loss = 0.42079183\n",
      "Iteration 119, loss = 0.42036708\n",
      "Iteration 120, loss = 0.42068553\n",
      "Iteration 121, loss = 0.42003510\n",
      "Iteration 122, loss = 0.42028880\n",
      "Iteration 123, loss = 0.41989154\n",
      "Iteration 124, loss = 0.41951390\n",
      "Iteration 125, loss = 0.41933122\n",
      "Iteration 126, loss = 0.41953463\n",
      "Iteration 127, loss = 0.41915662\n",
      "Iteration 128, loss = 0.41900097\n",
      "Iteration 129, loss = 0.41882843\n",
      "Iteration 130, loss = 0.41869631\n",
      "Iteration 131, loss = 0.41877898\n",
      "Iteration 132, loss = 0.41823689\n",
      "Iteration 133, loss = 0.41845909\n",
      "Iteration 134, loss = 0.41839897\n",
      "Iteration 135, loss = 0.41821608\n",
      "Iteration 136, loss = 0.41822148\n",
      "Iteration 137, loss = 0.41777273\n",
      "Iteration 138, loss = 0.41766154\n",
      "Iteration 139, loss = 0.41750829\n",
      "Iteration 140, loss = 0.41735290\n",
      "Iteration 141, loss = 0.41737035\n",
      "Iteration 142, loss = 0.41691192\n",
      "Iteration 143, loss = 0.41682196\n",
      "Iteration 144, loss = 0.41730404\n",
      "Iteration 145, loss = 0.41717520\n",
      "Iteration 146, loss = 0.41705244\n",
      "Iteration 147, loss = 0.41609474\n",
      "Iteration 148, loss = 0.41667249\n",
      "Iteration 149, loss = 0.41683783\n",
      "Iteration 150, loss = 0.41620576\n",
      "Iteration 151, loss = 0.41586194\n",
      "Iteration 152, loss = 0.41588789\n",
      "Iteration 153, loss = 0.41630894\n",
      "Iteration 154, loss = 0.41579185\n",
      "Iteration 155, loss = 0.41554608\n",
      "Iteration 156, loss = 0.41530420\n",
      "Iteration 157, loss = 0.41559335\n",
      "Iteration 158, loss = 0.41497962\n",
      "Iteration 159, loss = 0.41543417\n",
      "Iteration 160, loss = 0.41517572\n",
      "Iteration 161, loss = 0.41450502\n",
      "Iteration 162, loss = 0.41442419\n",
      "Iteration 163, loss = 0.41451638\n",
      "Iteration 164, loss = 0.41451811\n",
      "Iteration 165, loss = 0.41435548\n",
      "Iteration 166, loss = 0.41406094\n",
      "Iteration 167, loss = 0.41402643\n",
      "Iteration 168, loss = 0.41459698\n",
      "Iteration 169, loss = 0.41371588\n",
      "Iteration 170, loss = 0.41353101\n",
      "Iteration 171, loss = 0.41366876\n",
      "Iteration 172, loss = 0.41389117\n",
      "Iteration 173, loss = 0.41330870\n",
      "Iteration 174, loss = 0.41371359\n",
      "Iteration 175, loss = 0.41348506\n",
      "Iteration 176, loss = 0.41305445\n",
      "Iteration 177, loss = 0.41321675\n",
      "Iteration 178, loss = 0.41300036\n",
      "Iteration 179, loss = 0.41298104\n",
      "Iteration 180, loss = 0.41254090\n",
      "Iteration 181, loss = 0.41227038\n",
      "Iteration 182, loss = 0.41191118\n",
      "Iteration 183, loss = 0.41258844\n",
      "Iteration 184, loss = 0.41172325\n",
      "Iteration 185, loss = 0.41222745\n",
      "Iteration 186, loss = 0.41185831\n",
      "Iteration 187, loss = 0.41162884\n",
      "Iteration 188, loss = 0.41181292\n",
      "Iteration 189, loss = 0.41122888\n",
      "Iteration 190, loss = 0.41175127\n",
      "Iteration 191, loss = 0.41118000\n",
      "Iteration 192, loss = 0.41112400\n",
      "Iteration 193, loss = 0.41065641\n",
      "Iteration 194, loss = 0.41076307\n",
      "Iteration 195, loss = 0.41092485\n",
      "Iteration 196, loss = 0.41070404\n",
      "Iteration 197, loss = 0.41066420\n",
      "Iteration 198, loss = 0.41024963\n",
      "Iteration 199, loss = 0.41055084\n",
      "Iteration 200, loss = 0.41000905\n",
      "Iteration 201, loss = 0.40970360\n",
      "Iteration 202, loss = 0.40981427\n",
      "Iteration 203, loss = 0.40974877\n",
      "Iteration 204, loss = 0.40948178\n",
      "Iteration 205, loss = 0.40932795\n",
      "Iteration 206, loss = 0.40917221\n",
      "Iteration 207, loss = 0.40919273\n",
      "Iteration 208, loss = 0.40901246\n",
      "Iteration 209, loss = 0.40919637\n",
      "Iteration 210, loss = 0.40875247\n",
      "Iteration 211, loss = 0.40878617\n",
      "Iteration 212, loss = 0.40842491\n",
      "Iteration 213, loss = 0.40816537\n",
      "Iteration 214, loss = 0.40829659\n",
      "Iteration 215, loss = 0.40854489\n",
      "Iteration 216, loss = 0.40803553\n",
      "Iteration 217, loss = 0.40815932\n",
      "Iteration 218, loss = 0.40918127\n",
      "Iteration 219, loss = 0.40820626\n",
      "Iteration 220, loss = 0.40814150\n",
      "Iteration 221, loss = 0.40769718\n",
      "Iteration 222, loss = 0.40732651\n",
      "Iteration 223, loss = 0.40793113\n",
      "Iteration 224, loss = 0.40759515\n",
      "Iteration 225, loss = 0.40692914\n",
      "Iteration 226, loss = 0.40716854\n",
      "Iteration 227, loss = 0.40706115\n",
      "Iteration 228, loss = 0.40684423\n",
      "Iteration 229, loss = 0.40694276\n",
      "Iteration 230, loss = 0.40679563\n",
      "Iteration 231, loss = 0.40645931\n",
      "Iteration 232, loss = 0.40632190\n",
      "Iteration 233, loss = 0.40676197\n",
      "Iteration 234, loss = 0.40634690\n",
      "Iteration 235, loss = 0.40598669\n",
      "Iteration 236, loss = 0.40623098\n",
      "Iteration 237, loss = 0.40652315\n",
      "Iteration 238, loss = 0.40579934\n",
      "Iteration 239, loss = 0.40587332\n",
      "Iteration 240, loss = 0.40575237\n",
      "Iteration 241, loss = 0.40508710\n",
      "Iteration 242, loss = 0.40565327\n",
      "Iteration 243, loss = 0.40519319\n",
      "Iteration 244, loss = 0.40487631\n",
      "Iteration 245, loss = 0.40504465\n",
      "Iteration 246, loss = 0.40471946\n",
      "Iteration 247, loss = 0.40444036\n",
      "Iteration 248, loss = 0.40459467\n",
      "Iteration 249, loss = 0.40441586\n",
      "Iteration 250, loss = 0.40382599\n",
      "Iteration 251, loss = 0.40464308\n",
      "Iteration 252, loss = 0.40418275\n",
      "Iteration 253, loss = 0.40384348\n",
      "Iteration 254, loss = 0.40399007\n",
      "Iteration 255, loss = 0.40412554\n",
      "Iteration 256, loss = 0.40383433\n",
      "Iteration 257, loss = 0.40354016\n",
      "Iteration 258, loss = 0.40364526\n",
      "Iteration 259, loss = 0.40269496\n",
      "Iteration 260, loss = 0.40333674\n",
      "Iteration 261, loss = 0.40359065\n",
      "Iteration 262, loss = 0.40280484\n",
      "Iteration 263, loss = 0.40314274\n",
      "Iteration 264, loss = 0.40268294\n",
      "Iteration 265, loss = 0.40306987\n",
      "Iteration 266, loss = 0.40248744\n",
      "Iteration 267, loss = 0.40196712\n",
      "Iteration 268, loss = 0.40241500\n",
      "Iteration 269, loss = 0.40162345\n",
      "Iteration 270, loss = 0.40221784\n",
      "Iteration 271, loss = 0.40198458\n",
      "Iteration 272, loss = 0.40131111\n",
      "Iteration 273, loss = 0.40123197\n",
      "Iteration 274, loss = 0.40109022\n",
      "Iteration 275, loss = 0.40176119\n",
      "Iteration 276, loss = 0.40158094\n",
      "Iteration 277, loss = 0.40135888\n",
      "Iteration 278, loss = 0.40066717\n",
      "Iteration 279, loss = 0.40099937\n",
      "Iteration 280, loss = 0.40108078\n",
      "Iteration 281, loss = 0.40082181\n",
      "Iteration 282, loss = 0.40004492\n",
      "Iteration 283, loss = 0.40126879\n",
      "Iteration 284, loss = 0.40051340\n",
      "Iteration 285, loss = 0.39985259\n",
      "Iteration 286, loss = 0.39985367\n",
      "Iteration 287, loss = 0.39973411\n",
      "Iteration 288, loss = 0.40014882\n",
      "Iteration 289, loss = 0.39985265\n",
      "Iteration 290, loss = 0.39971275\n",
      "Iteration 291, loss = 0.39937350\n",
      "Iteration 292, loss = 0.39916995\n",
      "Iteration 293, loss = 0.39885807\n",
      "Iteration 294, loss = 0.39893273\n",
      "Iteration 295, loss = 0.39969295\n",
      "Iteration 296, loss = 0.39888151\n",
      "Iteration 297, loss = 0.39866921\n",
      "Iteration 298, loss = 0.39867517\n",
      "Iteration 299, loss = 0.39808715\n",
      "Iteration 300, loss = 0.39936848\n",
      "Iteration 301, loss = 0.39881666\n",
      "Iteration 302, loss = 0.39819256\n",
      "Iteration 303, loss = 0.39748242\n",
      "Iteration 304, loss = 0.39775425\n",
      "Iteration 305, loss = 0.39790128\n",
      "Iteration 306, loss = 0.39790477\n",
      "Iteration 307, loss = 0.39783724\n",
      "Iteration 308, loss = 0.39751306\n",
      "Iteration 309, loss = 0.39711816\n",
      "Iteration 310, loss = 0.39712017\n",
      "Iteration 311, loss = 0.39702683\n",
      "Iteration 312, loss = 0.39648667\n",
      "Iteration 313, loss = 0.39658392\n",
      "Iteration 314, loss = 0.39723832\n",
      "Iteration 315, loss = 0.39653858\n",
      "Iteration 316, loss = 0.39576532\n",
      "Iteration 317, loss = 0.39581990\n",
      "Iteration 318, loss = 0.39650315\n",
      "Iteration 319, loss = 0.39582466\n",
      "Iteration 320, loss = 0.39533456\n",
      "Iteration 321, loss = 0.39577791\n",
      "Iteration 322, loss = 0.39624442\n",
      "Iteration 323, loss = 0.39533479\n",
      "Iteration 324, loss = 0.39525451\n",
      "Iteration 325, loss = 0.39544289\n",
      "Iteration 326, loss = 0.39536071\n",
      "Iteration 327, loss = 0.39543726\n",
      "Iteration 328, loss = 0.39477979\n",
      "Iteration 329, loss = 0.39490422\n",
      "Iteration 330, loss = 0.39379488\n",
      "Iteration 331, loss = 0.39457890\n",
      "Iteration 332, loss = 0.39465994\n",
      "Iteration 333, loss = 0.39367142\n",
      "Iteration 334, loss = 0.39400635\n",
      "Iteration 335, loss = 0.39343923\n",
      "Iteration 336, loss = 0.39353067\n",
      "Iteration 337, loss = 0.39351792\n",
      "Iteration 338, loss = 0.39366021\n",
      "Iteration 339, loss = 0.39300601\n",
      "Iteration 340, loss = 0.39362914\n",
      "Iteration 341, loss = 0.39355988\n",
      "Iteration 342, loss = 0.39302072\n",
      "Iteration 343, loss = 0.39273620\n",
      "Iteration 344, loss = 0.39193751\n",
      "Iteration 345, loss = 0.39271717\n",
      "Iteration 346, loss = 0.39212183\n",
      "Iteration 347, loss = 0.39200400\n",
      "Iteration 348, loss = 0.39155190\n",
      "Iteration 349, loss = 0.39207751\n",
      "Iteration 350, loss = 0.39216566\n",
      "Iteration 351, loss = 0.39114278\n",
      "Iteration 352, loss = 0.39141645\n",
      "Iteration 353, loss = 0.39152616\n",
      "Iteration 354, loss = 0.39110068\n",
      "Iteration 355, loss = 0.39169518\n",
      "Iteration 356, loss = 0.39077447\n",
      "Iteration 357, loss = 0.39047887\n",
      "Iteration 358, loss = 0.39124777\n",
      "Iteration 359, loss = 0.39104597\n",
      "Iteration 360, loss = 0.39079523\n",
      "Iteration 361, loss = 0.39065609\n",
      "Iteration 362, loss = 0.39008605\n",
      "Iteration 363, loss = 0.38991174\n",
      "Iteration 364, loss = 0.39000553\n",
      "Iteration 365, loss = 0.39022902\n",
      "Iteration 366, loss = 0.38985381\n",
      "Iteration 367, loss = 0.38890205\n",
      "Iteration 368, loss = 0.38894034\n",
      "Iteration 369, loss = 0.38954709\n",
      "Iteration 370, loss = 0.38943840\n",
      "Iteration 371, loss = 0.38861062\n",
      "Iteration 372, loss = 0.38966886\n",
      "Iteration 373, loss = 0.38900786\n",
      "Iteration 374, loss = 0.38891046\n",
      "Iteration 375, loss = 0.38816264\n",
      "Iteration 376, loss = 0.38864801\n",
      "Iteration 377, loss = 0.38846702\n",
      "Iteration 378, loss = 0.38766559\n",
      "Iteration 379, loss = 0.38746415\n",
      "Iteration 380, loss = 0.38789977\n",
      "Iteration 381, loss = 0.38762248\n",
      "Iteration 382, loss = 0.38854278\n",
      "Iteration 383, loss = 0.38767075\n",
      "Iteration 384, loss = 0.38695405\n",
      "Iteration 385, loss = 0.38688581\n",
      "Iteration 386, loss = 0.38683166\n",
      "Iteration 387, loss = 0.38742768\n",
      "Iteration 388, loss = 0.38656708\n",
      "Iteration 389, loss = 0.38667008\n",
      "Iteration 390, loss = 0.38681274\n",
      "Iteration 391, loss = 0.38641820\n",
      "Iteration 392, loss = 0.38665063\n",
      "Iteration 393, loss = 0.38613282\n",
      "Iteration 394, loss = 0.38663058\n",
      "Iteration 395, loss = 0.38628451\n",
      "Iteration 396, loss = 0.38619646\n",
      "Iteration 397, loss = 0.38557894\n",
      "Iteration 398, loss = 0.38571839\n",
      "Iteration 399, loss = 0.38466190\n",
      "Iteration 400, loss = 0.38541329\n",
      "Iteration 401, loss = 0.38489406\n",
      "Iteration 402, loss = 0.38462425\n",
      "Iteration 403, loss = 0.38531589\n",
      "Iteration 404, loss = 0.38537821\n",
      "Iteration 405, loss = 0.38430290\n",
      "Iteration 406, loss = 0.38491293\n",
      "Iteration 407, loss = 0.38498973\n",
      "Iteration 408, loss = 0.38486759\n",
      "Iteration 409, loss = 0.38423121\n",
      "Iteration 410, loss = 0.38531105\n",
      "Iteration 411, loss = 0.38305764\n",
      "Iteration 412, loss = 0.38418408\n",
      "Iteration 413, loss = 0.38346566\n",
      "Iteration 414, loss = 0.38389974\n",
      "Iteration 415, loss = 0.38482996\n",
      "Iteration 416, loss = 0.38364846\n",
      "Iteration 417, loss = 0.38286872\n",
      "Iteration 418, loss = 0.38316545\n",
      "Iteration 419, loss = 0.38295547\n",
      "Iteration 420, loss = 0.38332907\n",
      "Iteration 421, loss = 0.38261294\n",
      "Iteration 422, loss = 0.38296556\n",
      "Iteration 423, loss = 0.38252366\n",
      "Iteration 424, loss = 0.38191535\n",
      "Iteration 425, loss = 0.38320183\n",
      "Iteration 426, loss = 0.38248717\n",
      "Iteration 427, loss = 0.38234751\n",
      "Iteration 428, loss = 0.38200525\n",
      "Iteration 429, loss = 0.38205214\n",
      "Iteration 430, loss = 0.38167780\n",
      "Iteration 431, loss = 0.38161265\n",
      "Iteration 432, loss = 0.38156498\n",
      "Iteration 433, loss = 0.38186422\n",
      "Iteration 434, loss = 0.38110331\n",
      "Iteration 435, loss = 0.38100963\n",
      "Iteration 436, loss = 0.38214732\n",
      "Iteration 437, loss = 0.38158398\n",
      "Iteration 438, loss = 0.38098055\n",
      "Iteration 439, loss = 0.38072300\n",
      "Iteration 440, loss = 0.38060127\n",
      "Iteration 441, loss = 0.38062618\n",
      "Iteration 442, loss = 0.38075197\n",
      "Iteration 443, loss = 0.38105212\n",
      "Iteration 444, loss = 0.38083774\n",
      "Iteration 445, loss = 0.38015622\n",
      "Iteration 446, loss = 0.37948130\n",
      "Iteration 447, loss = 0.37974669\n",
      "Iteration 448, loss = 0.37953521\n",
      "Iteration 449, loss = 0.37902627\n",
      "Iteration 450, loss = 0.37928737\n",
      "Iteration 451, loss = 0.37941347\n",
      "Iteration 452, loss = 0.37962093\n",
      "Iteration 453, loss = 0.37942367\n",
      "Iteration 454, loss = 0.37940256\n",
      "Iteration 455, loss = 0.37816241\n",
      "Iteration 456, loss = 0.37896821\n",
      "Iteration 457, loss = 0.37896096\n",
      "Iteration 458, loss = 0.37862485\n",
      "Iteration 459, loss = 0.37768522\n",
      "Iteration 460, loss = 0.37905333\n",
      "Iteration 461, loss = 0.37784248\n",
      "Iteration 462, loss = 0.37776489\n",
      "Iteration 463, loss = 0.37805220\n",
      "Iteration 464, loss = 0.37824574\n",
      "Iteration 465, loss = 0.37784440\n",
      "Iteration 466, loss = 0.37740461\n",
      "Iteration 467, loss = 0.37727962\n",
      "Iteration 468, loss = 0.37747860\n",
      "Iteration 469, loss = 0.37685659\n",
      "Iteration 470, loss = 0.37647993\n",
      "Iteration 471, loss = 0.37802411\n",
      "Iteration 472, loss = 0.37677723\n",
      "Iteration 473, loss = 0.37716014\n",
      "Iteration 474, loss = 0.37686789\n",
      "Iteration 475, loss = 0.37627784\n",
      "Iteration 476, loss = 0.37563366\n",
      "Iteration 477, loss = 0.37708251\n",
      "Iteration 478, loss = 0.37613945\n",
      "Iteration 479, loss = 0.37581694\n",
      "Iteration 480, loss = 0.37580008\n",
      "Iteration 481, loss = 0.37621596\n",
      "Iteration 482, loss = 0.37576425\n",
      "Iteration 483, loss = 0.37537303\n",
      "Iteration 484, loss = 0.37509699\n",
      "Iteration 485, loss = 0.37478881\n",
      "Iteration 486, loss = 0.37516440\n",
      "Iteration 487, loss = 0.37552454\n",
      "Iteration 488, loss = 0.37451482\n",
      "Iteration 489, loss = 0.37519654\n",
      "Iteration 490, loss = 0.37465985\n",
      "Iteration 491, loss = 0.37470757\n",
      "Iteration 492, loss = 0.37488079\n",
      "Iteration 493, loss = 0.37421423\n",
      "Iteration 494, loss = 0.37429384\n",
      "Iteration 495, loss = 0.37505304\n",
      "Iteration 496, loss = 0.37474464\n",
      "Iteration 497, loss = 0.37426259\n",
      "Iteration 498, loss = 0.37439294\n",
      "Iteration 499, loss = 0.37286614\n",
      "Iteration 500, loss = 0.37377871\n",
      "Iteration 501, loss = 0.37353966\n",
      "Iteration 502, loss = 0.37426723\n",
      "Iteration 503, loss = 0.37382737\n",
      "Iteration 504, loss = 0.37290580\n",
      "Iteration 505, loss = 0.37260284\n",
      "Iteration 506, loss = 0.37311240\n",
      "Iteration 507, loss = 0.37257965\n",
      "Iteration 508, loss = 0.37239129\n",
      "Iteration 509, loss = 0.37307912\n",
      "Iteration 510, loss = 0.37193675\n",
      "Iteration 511, loss = 0.37335912\n",
      "Iteration 512, loss = 0.37187838\n",
      "Iteration 513, loss = 0.37244515\n",
      "Iteration 514, loss = 0.37257025\n",
      "Iteration 515, loss = 0.37170330\n",
      "Iteration 516, loss = 0.37300129\n",
      "Iteration 517, loss = 0.37189447\n",
      "Iteration 518, loss = 0.37100609\n",
      "Iteration 519, loss = 0.37165603\n",
      "Iteration 520, loss = 0.37139875\n",
      "Iteration 521, loss = 0.37060463\n",
      "Iteration 522, loss = 0.37159889\n",
      "Iteration 523, loss = 0.37087360\n",
      "Iteration 524, loss = 0.37123125\n",
      "Iteration 525, loss = 0.37131993\n",
      "Iteration 526, loss = 0.37048869\n",
      "Iteration 527, loss = 0.37076531\n",
      "Iteration 528, loss = 0.37007403\n",
      "Iteration 529, loss = 0.37153884\n",
      "Iteration 530, loss = 0.37137562\n",
      "Iteration 531, loss = 0.37075492\n",
      "Iteration 532, loss = 0.36965666\n",
      "Iteration 533, loss = 0.37068582\n",
      "Iteration 534, loss = 0.36996754\n",
      "Iteration 535, loss = 0.36985101\n",
      "Iteration 536, loss = 0.36985892\n",
      "Iteration 537, loss = 0.37042734\n",
      "Iteration 538, loss = 0.36992839\n",
      "Iteration 539, loss = 0.36843457\n",
      "Iteration 540, loss = 0.37048978\n",
      "Iteration 541, loss = 0.36877909\n",
      "Iteration 542, loss = 0.36899421\n",
      "Iteration 543, loss = 0.36965476\n",
      "Iteration 544, loss = 0.36896156\n",
      "Iteration 545, loss = 0.36844736\n",
      "Iteration 546, loss = 0.36967658\n",
      "Iteration 547, loss = 0.36920240\n",
      "Iteration 548, loss = 0.36833376\n",
      "Iteration 549, loss = 0.36848813\n",
      "Iteration 550, loss = 0.36828904\n",
      "Iteration 551, loss = 0.36772820\n",
      "Iteration 552, loss = 0.36776577\n",
      "Iteration 553, loss = 0.36807182\n",
      "Iteration 554, loss = 0.36741392\n",
      "Iteration 555, loss = 0.36733690\n",
      "Iteration 556, loss = 0.36839832\n",
      "Iteration 557, loss = 0.36786738\n",
      "Iteration 558, loss = 0.36697699\n",
      "Iteration 559, loss = 0.36752097\n",
      "Iteration 560, loss = 0.36665142\n",
      "Iteration 561, loss = 0.36634983\n",
      "Iteration 562, loss = 0.36717724\n",
      "Iteration 563, loss = 0.36635692\n",
      "Iteration 564, loss = 0.36639273\n",
      "Iteration 565, loss = 0.36725940\n",
      "Iteration 566, loss = 0.36659677\n",
      "Iteration 567, loss = 0.36728060\n",
      "Iteration 568, loss = 0.36624320\n",
      "Iteration 569, loss = 0.36644159\n",
      "Iteration 570, loss = 0.36633973\n",
      "Iteration 571, loss = 0.36596477\n",
      "Iteration 572, loss = 0.36589387\n",
      "Iteration 573, loss = 0.36701457\n",
      "Iteration 574, loss = 0.36535684\n",
      "Iteration 575, loss = 0.36665541\n",
      "Iteration 576, loss = 0.36590973\n",
      "Iteration 577, loss = 0.36635864\n",
      "Iteration 578, loss = 0.36495397\n",
      "Iteration 579, loss = 0.36543331\n",
      "Iteration 580, loss = 0.36472089\n",
      "Iteration 581, loss = 0.36558995\n",
      "Iteration 582, loss = 0.36570727\n",
      "Iteration 583, loss = 0.36601982\n",
      "Iteration 584, loss = 0.36418771\n",
      "Iteration 585, loss = 0.36430665\n",
      "Iteration 586, loss = 0.36590694\n",
      "Iteration 587, loss = 0.36488655\n",
      "Iteration 588, loss = 0.36441771\n",
      "Iteration 589, loss = 0.36490648\n",
      "Iteration 590, loss = 0.36430095\n",
      "Iteration 591, loss = 0.36529263\n",
      "Iteration 592, loss = 0.36467399\n",
      "Iteration 593, loss = 0.36486837\n",
      "Iteration 594, loss = 0.36347677\n",
      "Iteration 595, loss = 0.36489495\n",
      "Iteration 596, loss = 0.36458227\n",
      "Iteration 597, loss = 0.36448235\n",
      "Iteration 598, loss = 0.36323891\n",
      "Iteration 599, loss = 0.36420589\n",
      "Iteration 600, loss = 0.36344723\n",
      "Iteration 601, loss = 0.36411653\n",
      "Iteration 602, loss = 0.36412620\n",
      "Iteration 603, loss = 0.36445807\n",
      "Iteration 604, loss = 0.36332618\n",
      "Iteration 605, loss = 0.36300652\n",
      "Iteration 606, loss = 0.36283367\n",
      "Iteration 607, loss = 0.36317184\n",
      "Iteration 608, loss = 0.36290610\n",
      "Iteration 609, loss = 0.36352290\n",
      "Iteration 610, loss = 0.36394930\n",
      "Iteration 611, loss = 0.36344112\n",
      "Iteration 612, loss = 0.36294117\n",
      "Iteration 613, loss = 0.36346659\n",
      "Iteration 614, loss = 0.36217411\n",
      "Iteration 615, loss = 0.36193533\n",
      "Iteration 616, loss = 0.36293321\n",
      "Iteration 617, loss = 0.36252616\n",
      "Iteration 618, loss = 0.36280369\n",
      "Iteration 619, loss = 0.36264435\n",
      "Iteration 620, loss = 0.36362628\n",
      "Iteration 621, loss = 0.36247800\n",
      "Iteration 622, loss = 0.36137143\n",
      "Iteration 623, loss = 0.36323259\n",
      "Iteration 624, loss = 0.36137022\n",
      "Iteration 625, loss = 0.36236267\n",
      "Iteration 626, loss = 0.36201383\n",
      "Iteration 627, loss = 0.36196953\n",
      "Iteration 628, loss = 0.36134281\n",
      "Iteration 629, loss = 0.36158178\n",
      "Iteration 630, loss = 0.36210814\n",
      "Iteration 631, loss = 0.36118633\n",
      "Iteration 632, loss = 0.36145195\n",
      "Iteration 633, loss = 0.36133170\n",
      "Iteration 634, loss = 0.36144124\n",
      "Iteration 635, loss = 0.36066097\n",
      "Iteration 636, loss = 0.36147561\n",
      "Iteration 637, loss = 0.36157859\n",
      "Iteration 638, loss = 0.36149429\n",
      "Iteration 639, loss = 0.36100381\n",
      "Iteration 640, loss = 0.36019508\n",
      "Iteration 641, loss = 0.36093351\n",
      "Iteration 642, loss = 0.36089927\n",
      "Iteration 643, loss = 0.36066370\n",
      "Iteration 644, loss = 0.35988093\n",
      "Iteration 645, loss = 0.36013315\n",
      "Iteration 646, loss = 0.36076534\n",
      "Iteration 647, loss = 0.36075760\n",
      "Iteration 648, loss = 0.36102685\n",
      "Iteration 649, loss = 0.36052583\n",
      "Iteration 650, loss = 0.36070377\n",
      "Iteration 651, loss = 0.36008208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [02:23<04:03, 22.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 652, loss = 0.35997347\n",
      "Iteration 653, loss = 0.36113021\n",
      "Iteration 654, loss = 0.36120705\n",
      "Iteration 655, loss = 0.35988078\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.53086458\n",
      "Iteration 2, loss = 0.48438906\n",
      "Iteration 3, loss = 0.46953504\n",
      "Iteration 4, loss = 0.46286023\n",
      "Iteration 5, loss = 0.45914386\n",
      "Iteration 6, loss = 0.45694698\n",
      "Iteration 7, loss = 0.45482873\n",
      "Iteration 8, loss = 0.45332904\n",
      "Iteration 9, loss = 0.45228723\n",
      "Iteration 10, loss = 0.45138119\n",
      "Iteration 11, loss = 0.45052693\n",
      "Iteration 12, loss = 0.44976063\n",
      "Iteration 13, loss = 0.44925525\n",
      "Iteration 14, loss = 0.44853117\n",
      "Iteration 15, loss = 0.44819174\n",
      "Iteration 16, loss = 0.44757685\n",
      "Iteration 17, loss = 0.44727719\n",
      "Iteration 18, loss = 0.44668854\n",
      "Iteration 19, loss = 0.44645888\n",
      "Iteration 20, loss = 0.44590108\n",
      "Iteration 21, loss = 0.44555010\n",
      "Iteration 22, loss = 0.44515885\n",
      "Iteration 23, loss = 0.44507657\n",
      "Iteration 24, loss = 0.44443443\n",
      "Iteration 25, loss = 0.44439399\n",
      "Iteration 26, loss = 0.44387716\n",
      "Iteration 27, loss = 0.44351276\n",
      "Iteration 28, loss = 0.44344937\n",
      "Iteration 29, loss = 0.44312075\n",
      "Iteration 30, loss = 0.44305385\n",
      "Iteration 31, loss = 0.44251232\n",
      "Iteration 32, loss = 0.44219454\n",
      "Iteration 33, loss = 0.44190161\n",
      "Iteration 34, loss = 0.44183628\n",
      "Iteration 35, loss = 0.44130724\n",
      "Iteration 36, loss = 0.44117928\n",
      "Iteration 37, loss = 0.44100787\n",
      "Iteration 38, loss = 0.44070347\n",
      "Iteration 39, loss = 0.44043603\n",
      "Iteration 40, loss = 0.44012205\n",
      "Iteration 41, loss = 0.43990994\n",
      "Iteration 42, loss = 0.43983248\n",
      "Iteration 43, loss = 0.43952411\n",
      "Iteration 44, loss = 0.43929276\n",
      "Iteration 45, loss = 0.43896313\n",
      "Iteration 46, loss = 0.43858511\n",
      "Iteration 47, loss = 0.43838024\n",
      "Iteration 48, loss = 0.43809536\n",
      "Iteration 49, loss = 0.43791587\n",
      "Iteration 50, loss = 0.43813725\n",
      "Iteration 51, loss = 0.43743973\n",
      "Iteration 52, loss = 0.43752798\n",
      "Iteration 53, loss = 0.43683421\n",
      "Iteration 54, loss = 0.43683468\n",
      "Iteration 55, loss = 0.43649633\n",
      "Iteration 56, loss = 0.43620255\n",
      "Iteration 57, loss = 0.43623650\n",
      "Iteration 58, loss = 0.43575062\n",
      "Iteration 59, loss = 0.43564887\n",
      "Iteration 60, loss = 0.43551754\n",
      "Iteration 61, loss = 0.43486307\n",
      "Iteration 62, loss = 0.43484755\n",
      "Iteration 63, loss = 0.43488802\n",
      "Iteration 64, loss = 0.43438315\n",
      "Iteration 65, loss = 0.43424354\n",
      "Iteration 66, loss = 0.43401943\n",
      "Iteration 67, loss = 0.43350559\n",
      "Iteration 68, loss = 0.43361523\n",
      "Iteration 69, loss = 0.43324832\n",
      "Iteration 70, loss = 0.43276758\n",
      "Iteration 71, loss = 0.43274545\n",
      "Iteration 72, loss = 0.43258638\n",
      "Iteration 73, loss = 0.43230199\n",
      "Iteration 74, loss = 0.43196999\n",
      "Iteration 75, loss = 0.43177698\n",
      "Iteration 76, loss = 0.43152287\n",
      "Iteration 77, loss = 0.43149633\n",
      "Iteration 78, loss = 0.43101030\n",
      "Iteration 79, loss = 0.43089322\n",
      "Iteration 80, loss = 0.43050993\n",
      "Iteration 81, loss = 0.43044686\n",
      "Iteration 82, loss = 0.42991755\n",
      "Iteration 83, loss = 0.42955164\n",
      "Iteration 84, loss = 0.42955663\n",
      "Iteration 85, loss = 0.42911676\n",
      "Iteration 86, loss = 0.42869895\n",
      "Iteration 87, loss = 0.42869528\n",
      "Iteration 88, loss = 0.42842719\n",
      "Iteration 89, loss = 0.42866226\n",
      "Iteration 90, loss = 0.42791622\n",
      "Iteration 91, loss = 0.42754984\n",
      "Iteration 92, loss = 0.42736585\n",
      "Iteration 93, loss = 0.42724941\n",
      "Iteration 94, loss = 0.42739080\n",
      "Iteration 95, loss = 0.42662884\n",
      "Iteration 96, loss = 0.42644302\n",
      "Iteration 97, loss = 0.42615631\n",
      "Iteration 98, loss = 0.42605625\n",
      "Iteration 99, loss = 0.42570203\n",
      "Iteration 100, loss = 0.42524641\n",
      "Iteration 101, loss = 0.42532039\n",
      "Iteration 102, loss = 0.42496282\n",
      "Iteration 103, loss = 0.42484535\n",
      "Iteration 104, loss = 0.42478849\n",
      "Iteration 105, loss = 0.42404913\n",
      "Iteration 106, loss = 0.42426049\n",
      "Iteration 107, loss = 0.42379042\n",
      "Iteration 108, loss = 0.42381539\n",
      "Iteration 109, loss = 0.42366182\n",
      "Iteration 110, loss = 0.42334364\n",
      "Iteration 111, loss = 0.42290378\n",
      "Iteration 112, loss = 0.42275675\n",
      "Iteration 113, loss = 0.42279439\n",
      "Iteration 114, loss = 0.42288974\n",
      "Iteration 115, loss = 0.42230894\n",
      "Iteration 116, loss = 0.42175819\n",
      "Iteration 117, loss = 0.42207392\n",
      "Iteration 118, loss = 0.42197217\n",
      "Iteration 119, loss = 0.42138631\n",
      "Iteration 120, loss = 0.42087436\n",
      "Iteration 121, loss = 0.42128651\n",
      "Iteration 122, loss = 0.42120185\n",
      "Iteration 123, loss = 0.42106324\n",
      "Iteration 124, loss = 0.42046467\n",
      "Iteration 125, loss = 0.42063521\n",
      "Iteration 126, loss = 0.42021843\n",
      "Iteration 127, loss = 0.41953660\n",
      "Iteration 128, loss = 0.41969336\n",
      "Iteration 129, loss = 0.41928601\n",
      "Iteration 130, loss = 0.41962380\n",
      "Iteration 131, loss = 0.41973595\n",
      "Iteration 132, loss = 0.41873875\n",
      "Iteration 133, loss = 0.41864041\n",
      "Iteration 134, loss = 0.41873329\n",
      "Iteration 135, loss = 0.41819781\n",
      "Iteration 136, loss = 0.41793839\n",
      "Iteration 137, loss = 0.41823746\n",
      "Iteration 138, loss = 0.41827822\n",
      "Iteration 139, loss = 0.41760687\n",
      "Iteration 140, loss = 0.41794409\n",
      "Iteration 141, loss = 0.41747207\n",
      "Iteration 142, loss = 0.41759692\n",
      "Iteration 143, loss = 0.41722773\n",
      "Iteration 144, loss = 0.41664945\n",
      "Iteration 145, loss = 0.41620730\n",
      "Iteration 146, loss = 0.41634276\n",
      "Iteration 147, loss = 0.41593862\n",
      "Iteration 148, loss = 0.41636713\n",
      "Iteration 149, loss = 0.41575565\n",
      "Iteration 150, loss = 0.41564172\n",
      "Iteration 151, loss = 0.41540846\n",
      "Iteration 152, loss = 0.41559837\n",
      "Iteration 153, loss = 0.41493464\n",
      "Iteration 154, loss = 0.41479940\n",
      "Iteration 155, loss = 0.41524562\n",
      "Iteration 156, loss = 0.41457261\n",
      "Iteration 157, loss = 0.41416837\n",
      "Iteration 158, loss = 0.41413884\n",
      "Iteration 159, loss = 0.41442355\n",
      "Iteration 160, loss = 0.41396760\n",
      "Iteration 161, loss = 0.41350061\n",
      "Iteration 162, loss = 0.41333723\n",
      "Iteration 163, loss = 0.41368773\n",
      "Iteration 164, loss = 0.41294750\n",
      "Iteration 165, loss = 0.41353217\n",
      "Iteration 166, loss = 0.41278819\n",
      "Iteration 167, loss = 0.41261574\n",
      "Iteration 168, loss = 0.41251011\n",
      "Iteration 169, loss = 0.41262652\n",
      "Iteration 170, loss = 0.41217417\n",
      "Iteration 171, loss = 0.41234006\n",
      "Iteration 172, loss = 0.41205925\n",
      "Iteration 173, loss = 0.41205147\n",
      "Iteration 174, loss = 0.41146744\n",
      "Iteration 175, loss = 0.41171085\n",
      "Iteration 176, loss = 0.41084831\n",
      "Iteration 177, loss = 0.41124093\n",
      "Iteration 178, loss = 0.41081703\n",
      "Iteration 179, loss = 0.41091831\n",
      "Iteration 180, loss = 0.41075548\n",
      "Iteration 181, loss = 0.41045661\n",
      "Iteration 182, loss = 0.41020846\n",
      "Iteration 183, loss = 0.40988447\n",
      "Iteration 184, loss = 0.40964826\n",
      "Iteration 185, loss = 0.40943255\n",
      "Iteration 186, loss = 0.40939577\n",
      "Iteration 187, loss = 0.40888902\n",
      "Iteration 188, loss = 0.40876411\n",
      "Iteration 189, loss = 0.40892134\n",
      "Iteration 190, loss = 0.40847864\n",
      "Iteration 191, loss = 0.40888491\n",
      "Iteration 192, loss = 0.40842890\n",
      "Iteration 193, loss = 0.40831440\n",
      "Iteration 194, loss = 0.40753516\n",
      "Iteration 195, loss = 0.40794637\n",
      "Iteration 196, loss = 0.40854339\n",
      "Iteration 197, loss = 0.40747812\n",
      "Iteration 198, loss = 0.40731101\n",
      "Iteration 199, loss = 0.40742461\n",
      "Iteration 200, loss = 0.40699885\n",
      "Iteration 201, loss = 0.40718119\n",
      "Iteration 202, loss = 0.40667819\n",
      "Iteration 203, loss = 0.40625394\n",
      "Iteration 204, loss = 0.40618533\n",
      "Iteration 205, loss = 0.40576436\n",
      "Iteration 206, loss = 0.40668869\n",
      "Iteration 207, loss = 0.40647174\n",
      "Iteration 208, loss = 0.40597865\n",
      "Iteration 209, loss = 0.40543485\n",
      "Iteration 210, loss = 0.40524092\n",
      "Iteration 211, loss = 0.40589432\n",
      "Iteration 212, loss = 0.40483855\n",
      "Iteration 213, loss = 0.40487062\n",
      "Iteration 214, loss = 0.40448873\n",
      "Iteration 215, loss = 0.40415563\n",
      "Iteration 216, loss = 0.40452780\n",
      "Iteration 217, loss = 0.40428732\n",
      "Iteration 218, loss = 0.40386072\n",
      "Iteration 219, loss = 0.40457612\n",
      "Iteration 220, loss = 0.40396831\n",
      "Iteration 221, loss = 0.40345166\n",
      "Iteration 222, loss = 0.40342497\n",
      "Iteration 223, loss = 0.40355792\n",
      "Iteration 224, loss = 0.40314213\n",
      "Iteration 225, loss = 0.40267106\n",
      "Iteration 226, loss = 0.40308556\n",
      "Iteration 227, loss = 0.40291317\n",
      "Iteration 228, loss = 0.40232302\n",
      "Iteration 229, loss = 0.40237511\n",
      "Iteration 230, loss = 0.40196278\n",
      "Iteration 231, loss = 0.40245416\n",
      "Iteration 232, loss = 0.40206610\n",
      "Iteration 233, loss = 0.40159390\n",
      "Iteration 234, loss = 0.40102607\n",
      "Iteration 235, loss = 0.40136384\n",
      "Iteration 236, loss = 0.40113951\n",
      "Iteration 237, loss = 0.40099015\n",
      "Iteration 238, loss = 0.40042026\n",
      "Iteration 239, loss = 0.40034371\n",
      "Iteration 240, loss = 0.40021691\n",
      "Iteration 241, loss = 0.39988598\n",
      "Iteration 242, loss = 0.40001165\n",
      "Iteration 243, loss = 0.39978924\n",
      "Iteration 244, loss = 0.39945569\n",
      "Iteration 245, loss = 0.39903593\n",
      "Iteration 246, loss = 0.39944198\n",
      "Iteration 247, loss = 0.39937030\n",
      "Iteration 248, loss = 0.39992370\n",
      "Iteration 249, loss = 0.39952589\n",
      "Iteration 250, loss = 0.39871457\n",
      "Iteration 251, loss = 0.39894349\n",
      "Iteration 252, loss = 0.39810024\n",
      "Iteration 253, loss = 0.39846416\n",
      "Iteration 254, loss = 0.39797450\n",
      "Iteration 255, loss = 0.39817054\n",
      "Iteration 256, loss = 0.39760232\n",
      "Iteration 257, loss = 0.39835610\n",
      "Iteration 258, loss = 0.39795670\n",
      "Iteration 259, loss = 0.39724712\n",
      "Iteration 260, loss = 0.39700405\n",
      "Iteration 261, loss = 0.39710224\n",
      "Iteration 262, loss = 0.39652367\n",
      "Iteration 263, loss = 0.39644388\n",
      "Iteration 264, loss = 0.39670795\n",
      "Iteration 265, loss = 0.39651185\n",
      "Iteration 266, loss = 0.39642833\n",
      "Iteration 267, loss = 0.39720168\n",
      "Iteration 268, loss = 0.39563156\n",
      "Iteration 269, loss = 0.39607443\n",
      "Iteration 270, loss = 0.39487296\n",
      "Iteration 271, loss = 0.39685317\n",
      "Iteration 272, loss = 0.39569262\n",
      "Iteration 273, loss = 0.39544471\n",
      "Iteration 274, loss = 0.39503210\n",
      "Iteration 275, loss = 0.39495044\n",
      "Iteration 276, loss = 0.39469773\n",
      "Iteration 277, loss = 0.39435701\n",
      "Iteration 278, loss = 0.39461934\n",
      "Iteration 279, loss = 0.39411691\n",
      "Iteration 280, loss = 0.39404555\n",
      "Iteration 281, loss = 0.39466550\n",
      "Iteration 282, loss = 0.39431788\n",
      "Iteration 283, loss = 0.39375717\n",
      "Iteration 284, loss = 0.39334441\n",
      "Iteration 285, loss = 0.39345348\n",
      "Iteration 286, loss = 0.39364529\n",
      "Iteration 287, loss = 0.39388687\n",
      "Iteration 288, loss = 0.39345024\n",
      "Iteration 289, loss = 0.39251718\n",
      "Iteration 290, loss = 0.39328224\n",
      "Iteration 291, loss = 0.39245664\n",
      "Iteration 292, loss = 0.39232287\n",
      "Iteration 293, loss = 0.39262204\n",
      "Iteration 294, loss = 0.39238490\n",
      "Iteration 295, loss = 0.39229660\n",
      "Iteration 296, loss = 0.39193290\n",
      "Iteration 297, loss = 0.39139528\n",
      "Iteration 298, loss = 0.39181149\n",
      "Iteration 299, loss = 0.39193888\n",
      "Iteration 300, loss = 0.39142154\n",
      "Iteration 301, loss = 0.39099031\n",
      "Iteration 302, loss = 0.39087034\n",
      "Iteration 303, loss = 0.39112256\n",
      "Iteration 304, loss = 0.39113303\n",
      "Iteration 305, loss = 0.39057204\n",
      "Iteration 306, loss = 0.39072665\n",
      "Iteration 307, loss = 0.39087664\n",
      "Iteration 308, loss = 0.39103327\n",
      "Iteration 309, loss = 0.38949943\n",
      "Iteration 310, loss = 0.38980526\n",
      "Iteration 311, loss = 0.38972820\n",
      "Iteration 312, loss = 0.38936270\n",
      "Iteration 313, loss = 0.38974737\n",
      "Iteration 314, loss = 0.39014578\n",
      "Iteration 315, loss = 0.38932144\n",
      "Iteration 316, loss = 0.38926177\n",
      "Iteration 317, loss = 0.38892659\n",
      "Iteration 318, loss = 0.38875637\n",
      "Iteration 319, loss = 0.38778704\n",
      "Iteration 320, loss = 0.38847711\n",
      "Iteration 321, loss = 0.38837048\n",
      "Iteration 322, loss = 0.38842371\n",
      "Iteration 323, loss = 0.38829297\n",
      "Iteration 324, loss = 0.38822316\n",
      "Iteration 325, loss = 0.38763032\n",
      "Iteration 326, loss = 0.38802522\n",
      "Iteration 327, loss = 0.38773656\n",
      "Iteration 328, loss = 0.38740754\n",
      "Iteration 329, loss = 0.38685895\n",
      "Iteration 330, loss = 0.38758060\n",
      "Iteration 331, loss = 0.38774760\n",
      "Iteration 332, loss = 0.38810902\n",
      "Iteration 333, loss = 0.38668876\n",
      "Iteration 334, loss = 0.38610492\n",
      "Iteration 335, loss = 0.38628565\n",
      "Iteration 336, loss = 0.38583795\n",
      "Iteration 337, loss = 0.38641928\n",
      "Iteration 338, loss = 0.38602296\n",
      "Iteration 339, loss = 0.38574634\n",
      "Iteration 340, loss = 0.38581562\n",
      "Iteration 341, loss = 0.38569790\n",
      "Iteration 342, loss = 0.38528388\n",
      "Iteration 343, loss = 0.38552808\n",
      "Iteration 344, loss = 0.38461795\n",
      "Iteration 345, loss = 0.38457225\n",
      "Iteration 346, loss = 0.38496106\n",
      "Iteration 347, loss = 0.38454390\n",
      "Iteration 348, loss = 0.38471335\n",
      "Iteration 349, loss = 0.38424694\n",
      "Iteration 350, loss = 0.38472255\n",
      "Iteration 351, loss = 0.38411920\n",
      "Iteration 352, loss = 0.38341809\n",
      "Iteration 353, loss = 0.38341286\n",
      "Iteration 354, loss = 0.38381355\n",
      "Iteration 355, loss = 0.38365328\n",
      "Iteration 356, loss = 0.38302706\n",
      "Iteration 357, loss = 0.38289344\n",
      "Iteration 358, loss = 0.38261433\n",
      "Iteration 359, loss = 0.38237081\n",
      "Iteration 360, loss = 0.38303622\n",
      "Iteration 361, loss = 0.38241226\n",
      "Iteration 362, loss = 0.38210887\n",
      "Iteration 363, loss = 0.38281144\n",
      "Iteration 364, loss = 0.38195737\n",
      "Iteration 365, loss = 0.38191691\n",
      "Iteration 366, loss = 0.38160755\n",
      "Iteration 367, loss = 0.38206734\n",
      "Iteration 368, loss = 0.38088083\n",
      "Iteration 369, loss = 0.38112194\n",
      "Iteration 370, loss = 0.38156682\n",
      "Iteration 371, loss = 0.38107472\n",
      "Iteration 372, loss = 0.38030119\n",
      "Iteration 373, loss = 0.38090780\n",
      "Iteration 374, loss = 0.38098309\n",
      "Iteration 375, loss = 0.38069303\n",
      "Iteration 376, loss = 0.38094350\n",
      "Iteration 377, loss = 0.38028913\n",
      "Iteration 378, loss = 0.38169858\n",
      "Iteration 379, loss = 0.38011544\n",
      "Iteration 380, loss = 0.37929050\n",
      "Iteration 381, loss = 0.38111213\n",
      "Iteration 382, loss = 0.37942221\n",
      "Iteration 383, loss = 0.37927841\n",
      "Iteration 384, loss = 0.37837863\n",
      "Iteration 385, loss = 0.37866018\n",
      "Iteration 386, loss = 0.37928544\n",
      "Iteration 387, loss = 0.37949651\n",
      "Iteration 388, loss = 0.37878898\n",
      "Iteration 389, loss = 0.37885305\n",
      "Iteration 390, loss = 0.37863168\n",
      "Iteration 391, loss = 0.37865022\n",
      "Iteration 392, loss = 0.37774281\n",
      "Iteration 393, loss = 0.37836733\n",
      "Iteration 394, loss = 0.37789285\n",
      "Iteration 395, loss = 0.37812726\n",
      "Iteration 396, loss = 0.37755965\n",
      "Iteration 397, loss = 0.37808964\n",
      "Iteration 398, loss = 0.37697998\n",
      "Iteration 399, loss = 0.37710860\n",
      "Iteration 400, loss = 0.37737493\n",
      "Iteration 401, loss = 0.37684787\n",
      "Iteration 402, loss = 0.37733867\n",
      "Iteration 403, loss = 0.37665171\n",
      "Iteration 404, loss = 0.37739226\n",
      "Iteration 405, loss = 0.37749948\n",
      "Iteration 406, loss = 0.37663668\n",
      "Iteration 407, loss = 0.37730482\n",
      "Iteration 408, loss = 0.37649409\n",
      "Iteration 409, loss = 0.37748360\n",
      "Iteration 410, loss = 0.37566522\n",
      "Iteration 411, loss = 0.37679379\n",
      "Iteration 412, loss = 0.37502226\n",
      "Iteration 413, loss = 0.37603958\n",
      "Iteration 414, loss = 0.37524912\n",
      "Iteration 415, loss = 0.37547166\n",
      "Iteration 416, loss = 0.37531876\n",
      "Iteration 417, loss = 0.37550015\n",
      "Iteration 418, loss = 0.37525400\n",
      "Iteration 419, loss = 0.37579942\n",
      "Iteration 420, loss = 0.37492731\n",
      "Iteration 421, loss = 0.37475321\n",
      "Iteration 422, loss = 0.37550159\n",
      "Iteration 423, loss = 0.37489248\n",
      "Iteration 424, loss = 0.37375633\n",
      "Iteration 425, loss = 0.37452018\n",
      "Iteration 426, loss = 0.37437448\n",
      "Iteration 427, loss = 0.37557707\n",
      "Iteration 428, loss = 0.37408070\n",
      "Iteration 429, loss = 0.37422225\n",
      "Iteration 430, loss = 0.37394479\n",
      "Iteration 431, loss = 0.37353509\n",
      "Iteration 432, loss = 0.37324844\n",
      "Iteration 433, loss = 0.37355555\n",
      "Iteration 434, loss = 0.37378457\n",
      "Iteration 435, loss = 0.37321619\n",
      "Iteration 436, loss = 0.37373920\n",
      "Iteration 437, loss = 0.37350708\n",
      "Iteration 438, loss = 0.37276871\n",
      "Iteration 439, loss = 0.37275716\n",
      "Iteration 440, loss = 0.37243162\n",
      "Iteration 441, loss = 0.37300815\n",
      "Iteration 442, loss = 0.37316305\n",
      "Iteration 443, loss = 0.37253156\n",
      "Iteration 444, loss = 0.37280766\n",
      "Iteration 445, loss = 0.37143734\n",
      "Iteration 446, loss = 0.37271316\n",
      "Iteration 447, loss = 0.37217971\n",
      "Iteration 448, loss = 0.37147218\n",
      "Iteration 449, loss = 0.37217857\n",
      "Iteration 450, loss = 0.37181815\n",
      "Iteration 451, loss = 0.37138678\n",
      "Iteration 452, loss = 0.37136826\n",
      "Iteration 453, loss = 0.37145246\n",
      "Iteration 454, loss = 0.37200264\n",
      "Iteration 455, loss = 0.37140968\n",
      "Iteration 456, loss = 0.37084670\n",
      "Iteration 457, loss = 0.37047566\n",
      "Iteration 458, loss = 0.37057267\n",
      "Iteration 459, loss = 0.37133885\n",
      "Iteration 460, loss = 0.37169774\n",
      "Iteration 461, loss = 0.37064588\n",
      "Iteration 462, loss = 0.37058050\n",
      "Iteration 463, loss = 0.36957562\n",
      "Iteration 464, loss = 0.37051123\n",
      "Iteration 465, loss = 0.37036017\n",
      "Iteration 466, loss = 0.36962887\n",
      "Iteration 467, loss = 0.36972186\n",
      "Iteration 468, loss = 0.37018247\n",
      "Iteration 469, loss = 0.36978352\n",
      "Iteration 470, loss = 0.36963133\n",
      "Iteration 471, loss = 0.36955203\n",
      "Iteration 472, loss = 0.36913836\n",
      "Iteration 473, loss = 0.37045299\n",
      "Iteration 474, loss = 0.36947404\n",
      "Iteration 475, loss = 0.36966502\n",
      "Iteration 476, loss = 0.36869677\n",
      "Iteration 477, loss = 0.36853248\n",
      "Iteration 478, loss = 0.36853957\n",
      "Iteration 479, loss = 0.36909236\n",
      "Iteration 480, loss = 0.36895979\n",
      "Iteration 481, loss = 0.36811759\n",
      "Iteration 482, loss = 0.36859173\n",
      "Iteration 483, loss = 0.36862432\n",
      "Iteration 484, loss = 0.36846372\n",
      "Iteration 485, loss = 0.36761385\n",
      "Iteration 486, loss = 0.36841360\n",
      "Iteration 487, loss = 0.36755030\n",
      "Iteration 488, loss = 0.36804812\n",
      "Iteration 489, loss = 0.36779408\n",
      "Iteration 490, loss = 0.36684769\n",
      "Iteration 491, loss = 0.36820533\n",
      "Iteration 492, loss = 0.36775483\n",
      "Iteration 493, loss = 0.36758247\n",
      "Iteration 494, loss = 0.36726097\n",
      "Iteration 495, loss = 0.36726805\n",
      "Iteration 496, loss = 0.36769780\n",
      "Iteration 497, loss = 0.36731181\n",
      "Iteration 498, loss = 0.36752863\n",
      "Iteration 499, loss = 0.36781422\n",
      "Iteration 500, loss = 0.36640242\n",
      "Iteration 501, loss = 0.36671984\n",
      "Iteration 502, loss = 0.36710359\n",
      "Iteration 503, loss = 0.36633989\n",
      "Iteration 504, loss = 0.36726425\n",
      "Iteration 505, loss = 0.36597060\n",
      "Iteration 506, loss = 0.36697430\n",
      "Iteration 507, loss = 0.36594083\n",
      "Iteration 508, loss = 0.36572974\n",
      "Iteration 509, loss = 0.36530473\n",
      "Iteration 510, loss = 0.36581883\n",
      "Iteration 511, loss = 0.36590688\n",
      "Iteration 512, loss = 0.36631668\n",
      "Iteration 513, loss = 0.36643922\n",
      "Iteration 514, loss = 0.36549371\n",
      "Iteration 515, loss = 0.36592438\n",
      "Iteration 516, loss = 0.36511434\n",
      "Iteration 517, loss = 0.36572203\n",
      "Iteration 518, loss = 0.36476864\n",
      "Iteration 519, loss = 0.36482732\n",
      "Iteration 520, loss = 0.36483745\n",
      "Iteration 521, loss = 0.36530389\n",
      "Iteration 522, loss = 0.36487114\n",
      "Iteration 523, loss = 0.36453574\n",
      "Iteration 524, loss = 0.36488991\n",
      "Iteration 525, loss = 0.36434327\n",
      "Iteration 526, loss = 0.36365571\n",
      "Iteration 527, loss = 0.36358851\n",
      "Iteration 528, loss = 0.36510212\n",
      "Iteration 529, loss = 0.36395991\n",
      "Iteration 530, loss = 0.36421000\n",
      "Iteration 531, loss = 0.36443761\n",
      "Iteration 532, loss = 0.36301124\n",
      "Iteration 533, loss = 0.36480159\n",
      "Iteration 534, loss = 0.36337538\n",
      "Iteration 535, loss = 0.36379010\n",
      "Iteration 536, loss = 0.36369793\n",
      "Iteration 537, loss = 0.36306358\n",
      "Iteration 538, loss = 0.36356013\n",
      "Iteration 539, loss = 0.36379638\n",
      "Iteration 540, loss = 0.36325539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [02:48<03:50, 23.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 541, loss = 0.36295305\n",
      "Iteration 542, loss = 0.36300278\n",
      "Iteration 543, loss = 0.36419341\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.53158639\n",
      "Iteration 2, loss = 0.49000321\n",
      "Iteration 3, loss = 0.47327344\n",
      "Iteration 4, loss = 0.46524840\n",
      "Iteration 5, loss = 0.46127511\n",
      "Iteration 6, loss = 0.45854630\n",
      "Iteration 7, loss = 0.45669168\n",
      "Iteration 8, loss = 0.45532258\n",
      "Iteration 9, loss = 0.45421262\n",
      "Iteration 10, loss = 0.45319946\n",
      "Iteration 11, loss = 0.45234733\n",
      "Iteration 12, loss = 0.45136312\n",
      "Iteration 13, loss = 0.45062695\n",
      "Iteration 14, loss = 0.44997489\n",
      "Iteration 15, loss = 0.44975669\n",
      "Iteration 16, loss = 0.44877225\n",
      "Iteration 17, loss = 0.44831149\n",
      "Iteration 18, loss = 0.44781141\n",
      "Iteration 19, loss = 0.44767071\n",
      "Iteration 20, loss = 0.44686659\n",
      "Iteration 21, loss = 0.44672685\n",
      "Iteration 22, loss = 0.44606084\n",
      "Iteration 23, loss = 0.44586395\n",
      "Iteration 24, loss = 0.44543072\n",
      "Iteration 25, loss = 0.44513243\n",
      "Iteration 26, loss = 0.44467846\n",
      "Iteration 27, loss = 0.44432150\n",
      "Iteration 28, loss = 0.44423359\n",
      "Iteration 29, loss = 0.44410911\n",
      "Iteration 30, loss = 0.44361732\n",
      "Iteration 31, loss = 0.44314815\n",
      "Iteration 32, loss = 0.44277194\n",
      "Iteration 33, loss = 0.44251868\n",
      "Iteration 34, loss = 0.44235966\n",
      "Iteration 35, loss = 0.44183680\n",
      "Iteration 36, loss = 0.44180584\n",
      "Iteration 37, loss = 0.44126113\n",
      "Iteration 38, loss = 0.44141120\n",
      "Iteration 39, loss = 0.44070357\n",
      "Iteration 40, loss = 0.44055028\n",
      "Iteration 41, loss = 0.44009330\n",
      "Iteration 42, loss = 0.43992952\n",
      "Iteration 43, loss = 0.43945618\n",
      "Iteration 44, loss = 0.43952016\n",
      "Iteration 45, loss = 0.43914323\n",
      "Iteration 46, loss = 0.43878550\n",
      "Iteration 47, loss = 0.43838581\n",
      "Iteration 48, loss = 0.43819067\n",
      "Iteration 49, loss = 0.43784277\n",
      "Iteration 50, loss = 0.43773340\n",
      "Iteration 51, loss = 0.43719980\n",
      "Iteration 52, loss = 0.43715151\n",
      "Iteration 53, loss = 0.43693170\n",
      "Iteration 54, loss = 0.43659661\n",
      "Iteration 55, loss = 0.43619656\n",
      "Iteration 56, loss = 0.43595145\n",
      "Iteration 57, loss = 0.43591226\n",
      "Iteration 58, loss = 0.43544991\n",
      "Iteration 59, loss = 0.43534530\n",
      "Iteration 60, loss = 0.43494386\n",
      "Iteration 61, loss = 0.43452546\n",
      "Iteration 62, loss = 0.43466365\n",
      "Iteration 63, loss = 0.43422417\n",
      "Iteration 64, loss = 0.43383060\n",
      "Iteration 65, loss = 0.43408089\n",
      "Iteration 66, loss = 0.43331362\n",
      "Iteration 67, loss = 0.43337691\n",
      "Iteration 68, loss = 0.43307997\n",
      "Iteration 69, loss = 0.43258020\n",
      "Iteration 70, loss = 0.43242076\n",
      "Iteration 71, loss = 0.43213965\n",
      "Iteration 72, loss = 0.43202776\n",
      "Iteration 73, loss = 0.43166429\n",
      "Iteration 74, loss = 0.43119235\n",
      "Iteration 75, loss = 0.43113894\n",
      "Iteration 76, loss = 0.43078623\n",
      "Iteration 77, loss = 0.43039997\n",
      "Iteration 78, loss = 0.43083601\n",
      "Iteration 79, loss = 0.43034810\n",
      "Iteration 80, loss = 0.42993911\n",
      "Iteration 81, loss = 0.42976079\n",
      "Iteration 82, loss = 0.42972224\n",
      "Iteration 83, loss = 0.42938839\n",
      "Iteration 84, loss = 0.42902818\n",
      "Iteration 85, loss = 0.42865584\n",
      "Iteration 86, loss = 0.42876868\n",
      "Iteration 87, loss = 0.42822343\n",
      "Iteration 88, loss = 0.42801012\n",
      "Iteration 89, loss = 0.42813151\n",
      "Iteration 90, loss = 0.42760495\n",
      "Iteration 91, loss = 0.42738070\n",
      "Iteration 92, loss = 0.42721289\n",
      "Iteration 93, loss = 0.42680115\n",
      "Iteration 94, loss = 0.42677727\n",
      "Iteration 95, loss = 0.42655882\n",
      "Iteration 96, loss = 0.42630436\n",
      "Iteration 97, loss = 0.42628926\n",
      "Iteration 98, loss = 0.42579358\n",
      "Iteration 99, loss = 0.42562787\n",
      "Iteration 100, loss = 0.42583605\n",
      "Iteration 101, loss = 0.42522916\n",
      "Iteration 102, loss = 0.42544859\n",
      "Iteration 103, loss = 0.42488643\n",
      "Iteration 104, loss = 0.42465463\n",
      "Iteration 105, loss = 0.42428508\n",
      "Iteration 106, loss = 0.42435337\n",
      "Iteration 107, loss = 0.42422630\n",
      "Iteration 108, loss = 0.42383007\n",
      "Iteration 109, loss = 0.42333668\n",
      "Iteration 110, loss = 0.42343492\n",
      "Iteration 111, loss = 0.42329757\n",
      "Iteration 112, loss = 0.42325740\n",
      "Iteration 113, loss = 0.42274585\n",
      "Iteration 114, loss = 0.42257560\n",
      "Iteration 115, loss = 0.42238645\n",
      "Iteration 116, loss = 0.42224730\n",
      "Iteration 117, loss = 0.42207428\n",
      "Iteration 118, loss = 0.42149958\n",
      "Iteration 119, loss = 0.42137384\n",
      "Iteration 120, loss = 0.42116286\n",
      "Iteration 121, loss = 0.42139834\n",
      "Iteration 122, loss = 0.42102377\n",
      "Iteration 123, loss = 0.42068049\n",
      "Iteration 124, loss = 0.42061123\n",
      "Iteration 125, loss = 0.42026203\n",
      "Iteration 126, loss = 0.42028362\n",
      "Iteration 127, loss = 0.41993701\n",
      "Iteration 128, loss = 0.41964969\n",
      "Iteration 129, loss = 0.41930672\n",
      "Iteration 130, loss = 0.41903029\n",
      "Iteration 131, loss = 0.41889925\n",
      "Iteration 132, loss = 0.41913052\n",
      "Iteration 133, loss = 0.41866892\n",
      "Iteration 134, loss = 0.41824812\n",
      "Iteration 135, loss = 0.41810737\n",
      "Iteration 136, loss = 0.41805558\n",
      "Iteration 137, loss = 0.41760772\n",
      "Iteration 138, loss = 0.41779803\n",
      "Iteration 139, loss = 0.41744321\n",
      "Iteration 140, loss = 0.41690620\n",
      "Iteration 141, loss = 0.41711223\n",
      "Iteration 142, loss = 0.41695060\n",
      "Iteration 143, loss = 0.41688536\n",
      "Iteration 144, loss = 0.41652226\n",
      "Iteration 145, loss = 0.41602751\n",
      "Iteration 146, loss = 0.41610233\n",
      "Iteration 147, loss = 0.41572470\n",
      "Iteration 148, loss = 0.41581796\n",
      "Iteration 149, loss = 0.41537041\n",
      "Iteration 150, loss = 0.41520806\n",
      "Iteration 151, loss = 0.41479301\n",
      "Iteration 152, loss = 0.41485369\n",
      "Iteration 153, loss = 0.41469109\n",
      "Iteration 154, loss = 0.41443698\n",
      "Iteration 155, loss = 0.41477435\n",
      "Iteration 156, loss = 0.41404511\n",
      "Iteration 157, loss = 0.41422920\n",
      "Iteration 158, loss = 0.41366256\n",
      "Iteration 159, loss = 0.41350964\n",
      "Iteration 160, loss = 0.41374485\n",
      "Iteration 161, loss = 0.41312207\n",
      "Iteration 162, loss = 0.41285934\n",
      "Iteration 163, loss = 0.41252135\n",
      "Iteration 164, loss = 0.41241623\n",
      "Iteration 165, loss = 0.41219413\n",
      "Iteration 166, loss = 0.41174454\n",
      "Iteration 167, loss = 0.41170725\n",
      "Iteration 168, loss = 0.41169403\n",
      "Iteration 169, loss = 0.41182011\n",
      "Iteration 170, loss = 0.41082764\n",
      "Iteration 171, loss = 0.41140848\n",
      "Iteration 172, loss = 0.41055347\n",
      "Iteration 173, loss = 0.41064799\n",
      "Iteration 174, loss = 0.41039096\n",
      "Iteration 175, loss = 0.41030408\n",
      "Iteration 176, loss = 0.40997458\n",
      "Iteration 177, loss = 0.41049640\n",
      "Iteration 178, loss = 0.40932865\n",
      "Iteration 179, loss = 0.40914902\n",
      "Iteration 180, loss = 0.40964014\n",
      "Iteration 181, loss = 0.40932632\n",
      "Iteration 182, loss = 0.40893051\n",
      "Iteration 183, loss = 0.40850366\n",
      "Iteration 184, loss = 0.40910461\n",
      "Iteration 185, loss = 0.40788793\n",
      "Iteration 186, loss = 0.40798699\n",
      "Iteration 187, loss = 0.40851169\n",
      "Iteration 188, loss = 0.40772159\n",
      "Iteration 189, loss = 0.40755865\n",
      "Iteration 190, loss = 0.40741121\n",
      "Iteration 191, loss = 0.40729925\n",
      "Iteration 192, loss = 0.40692312\n",
      "Iteration 193, loss = 0.40701076\n",
      "Iteration 194, loss = 0.40665347\n",
      "Iteration 195, loss = 0.40657423\n",
      "Iteration 196, loss = 0.40574344\n",
      "Iteration 197, loss = 0.40608373\n",
      "Iteration 198, loss = 0.40609513\n",
      "Iteration 199, loss = 0.40641289\n",
      "Iteration 200, loss = 0.40522638\n",
      "Iteration 201, loss = 0.40466996\n",
      "Iteration 202, loss = 0.40494768\n",
      "Iteration 203, loss = 0.40480361\n",
      "Iteration 204, loss = 0.40468445\n",
      "Iteration 205, loss = 0.40408028\n",
      "Iteration 206, loss = 0.40391541\n",
      "Iteration 207, loss = 0.40369764\n",
      "Iteration 208, loss = 0.40395896\n",
      "Iteration 209, loss = 0.40355920\n",
      "Iteration 210, loss = 0.40347911\n",
      "Iteration 211, loss = 0.40272564\n",
      "Iteration 212, loss = 0.40304357\n",
      "Iteration 213, loss = 0.40278823\n",
      "Iteration 214, loss = 0.40299286\n",
      "Iteration 215, loss = 0.40253601\n",
      "Iteration 216, loss = 0.40212170\n",
      "Iteration 217, loss = 0.40221955\n",
      "Iteration 218, loss = 0.40199456\n",
      "Iteration 219, loss = 0.40152407\n",
      "Iteration 220, loss = 0.40074330\n",
      "Iteration 221, loss = 0.40094953\n",
      "Iteration 222, loss = 0.40072825\n",
      "Iteration 223, loss = 0.40056135\n",
      "Iteration 224, loss = 0.40074696\n",
      "Iteration 225, loss = 0.40080504\n",
      "Iteration 226, loss = 0.39986692\n",
      "Iteration 227, loss = 0.39975322\n",
      "Iteration 228, loss = 0.39991462\n",
      "Iteration 229, loss = 0.39953670\n",
      "Iteration 230, loss = 0.39922400\n",
      "Iteration 231, loss = 0.39885831\n",
      "Iteration 232, loss = 0.39851028\n",
      "Iteration 233, loss = 0.39834355\n",
      "Iteration 234, loss = 0.39866157\n",
      "Iteration 235, loss = 0.39801286\n",
      "Iteration 236, loss = 0.39819679\n",
      "Iteration 237, loss = 0.39802541\n",
      "Iteration 238, loss = 0.39692733\n",
      "Iteration 239, loss = 0.39762479\n",
      "Iteration 240, loss = 0.39766390\n",
      "Iteration 241, loss = 0.39662974\n",
      "Iteration 242, loss = 0.39654446\n",
      "Iteration 243, loss = 0.39650627\n",
      "Iteration 244, loss = 0.39676799\n",
      "Iteration 245, loss = 0.39651786\n",
      "Iteration 246, loss = 0.39644770\n",
      "Iteration 247, loss = 0.39673043\n",
      "Iteration 248, loss = 0.39647028\n",
      "Iteration 249, loss = 0.39544368\n",
      "Iteration 250, loss = 0.39460600\n",
      "Iteration 251, loss = 0.39466816\n",
      "Iteration 252, loss = 0.39523372\n",
      "Iteration 253, loss = 0.39457201\n",
      "Iteration 254, loss = 0.39456359\n",
      "Iteration 255, loss = 0.39411520\n",
      "Iteration 256, loss = 0.39424318\n",
      "Iteration 257, loss = 0.39428971\n",
      "Iteration 258, loss = 0.39394965\n",
      "Iteration 259, loss = 0.39336295\n",
      "Iteration 260, loss = 0.39335392\n",
      "Iteration 261, loss = 0.39316839\n",
      "Iteration 262, loss = 0.39319003\n",
      "Iteration 263, loss = 0.39301031\n",
      "Iteration 264, loss = 0.39251285\n",
      "Iteration 265, loss = 0.39348634\n",
      "Iteration 266, loss = 0.39250691\n",
      "Iteration 267, loss = 0.39214527\n",
      "Iteration 268, loss = 0.39264963\n",
      "Iteration 269, loss = 0.39153157\n",
      "Iteration 270, loss = 0.39164377\n",
      "Iteration 271, loss = 0.39127573\n",
      "Iteration 272, loss = 0.39085184\n",
      "Iteration 273, loss = 0.39105218\n",
      "Iteration 274, loss = 0.39113466\n",
      "Iteration 275, loss = 0.39070285\n",
      "Iteration 276, loss = 0.39039644\n",
      "Iteration 277, loss = 0.39001778\n",
      "Iteration 278, loss = 0.39034734\n",
      "Iteration 279, loss = 0.38969487\n",
      "Iteration 280, loss = 0.38890044\n",
      "Iteration 281, loss = 0.38982063\n",
      "Iteration 282, loss = 0.38944043\n",
      "Iteration 283, loss = 0.38894275\n",
      "Iteration 284, loss = 0.38842494\n",
      "Iteration 285, loss = 0.38850694\n",
      "Iteration 286, loss = 0.38813894\n",
      "Iteration 287, loss = 0.38875134\n",
      "Iteration 288, loss = 0.38778148\n",
      "Iteration 289, loss = 0.38791307\n",
      "Iteration 290, loss = 0.38734244\n",
      "Iteration 291, loss = 0.38726133\n",
      "Iteration 292, loss = 0.38704990\n",
      "Iteration 293, loss = 0.38704061\n",
      "Iteration 294, loss = 0.38658051\n",
      "Iteration 295, loss = 0.38667402\n",
      "Iteration 296, loss = 0.38614188\n",
      "Iteration 297, loss = 0.38599107\n",
      "Iteration 298, loss = 0.38630932\n",
      "Iteration 299, loss = 0.38613108\n",
      "Iteration 300, loss = 0.38600404\n",
      "Iteration 301, loss = 0.38561067\n",
      "Iteration 302, loss = 0.38529995\n",
      "Iteration 303, loss = 0.38575128\n",
      "Iteration 304, loss = 0.38510955\n",
      "Iteration 305, loss = 0.38534349\n",
      "Iteration 306, loss = 0.38445506\n",
      "Iteration 307, loss = 0.38476398\n",
      "Iteration 308, loss = 0.38421604\n",
      "Iteration 309, loss = 0.38367622\n",
      "Iteration 310, loss = 0.38409273\n",
      "Iteration 311, loss = 0.38399267\n",
      "Iteration 312, loss = 0.38328577\n",
      "Iteration 313, loss = 0.38373054\n",
      "Iteration 314, loss = 0.38293202\n",
      "Iteration 315, loss = 0.38286106\n",
      "Iteration 316, loss = 0.38274346\n",
      "Iteration 317, loss = 0.38381294\n",
      "Iteration 318, loss = 0.38196342\n",
      "Iteration 319, loss = 0.38273042\n",
      "Iteration 320, loss = 0.38179201\n",
      "Iteration 321, loss = 0.38217677\n",
      "Iteration 322, loss = 0.38192203\n",
      "Iteration 323, loss = 0.38260258\n",
      "Iteration 324, loss = 0.38117388\n",
      "Iteration 325, loss = 0.38057700\n",
      "Iteration 326, loss = 0.38092718\n",
      "Iteration 327, loss = 0.38057134\n",
      "Iteration 328, loss = 0.38100698\n",
      "Iteration 329, loss = 0.38100612\n",
      "Iteration 330, loss = 0.38041656\n",
      "Iteration 331, loss = 0.38037350\n",
      "Iteration 332, loss = 0.37960526\n",
      "Iteration 333, loss = 0.37998643\n",
      "Iteration 334, loss = 0.37948718\n",
      "Iteration 335, loss = 0.37893029\n",
      "Iteration 336, loss = 0.37950894\n",
      "Iteration 337, loss = 0.37855570\n",
      "Iteration 338, loss = 0.37841393\n",
      "Iteration 339, loss = 0.37927811\n",
      "Iteration 340, loss = 0.37818376\n",
      "Iteration 341, loss = 0.37781735\n",
      "Iteration 342, loss = 0.37914731\n",
      "Iteration 343, loss = 0.37750907\n",
      "Iteration 344, loss = 0.37702844\n",
      "Iteration 345, loss = 0.37776055\n",
      "Iteration 346, loss = 0.37783062\n",
      "Iteration 347, loss = 0.37673858\n",
      "Iteration 348, loss = 0.37762003\n",
      "Iteration 349, loss = 0.37709251\n",
      "Iteration 350, loss = 0.37699685\n",
      "Iteration 351, loss = 0.37593768\n",
      "Iteration 352, loss = 0.37658835\n",
      "Iteration 353, loss = 0.37584500\n",
      "Iteration 354, loss = 0.37618849\n",
      "Iteration 355, loss = 0.37642132\n",
      "Iteration 356, loss = 0.37559223\n",
      "Iteration 357, loss = 0.37535319\n",
      "Iteration 358, loss = 0.37603101\n",
      "Iteration 359, loss = 0.37539167\n",
      "Iteration 360, loss = 0.37619708\n",
      "Iteration 361, loss = 0.37508034\n",
      "Iteration 362, loss = 0.37456552\n",
      "Iteration 363, loss = 0.37474390\n",
      "Iteration 364, loss = 0.37429752\n",
      "Iteration 365, loss = 0.37478014\n",
      "Iteration 366, loss = 0.37468053\n",
      "Iteration 367, loss = 0.37411938\n",
      "Iteration 368, loss = 0.37364493\n",
      "Iteration 369, loss = 0.37407708\n",
      "Iteration 370, loss = 0.37390675\n",
      "Iteration 371, loss = 0.37301202\n",
      "Iteration 372, loss = 0.37284250\n",
      "Iteration 373, loss = 0.37450843\n",
      "Iteration 374, loss = 0.37317380\n",
      "Iteration 375, loss = 0.37364811\n",
      "Iteration 376, loss = 0.37280543\n",
      "Iteration 377, loss = 0.37297336\n",
      "Iteration 378, loss = 0.37150911\n",
      "Iteration 379, loss = 0.37280831\n",
      "Iteration 380, loss = 0.37325667\n",
      "Iteration 381, loss = 0.37158118\n",
      "Iteration 382, loss = 0.37154508\n",
      "Iteration 383, loss = 0.37212852\n",
      "Iteration 384, loss = 0.37186327\n",
      "Iteration 385, loss = 0.37140644\n",
      "Iteration 386, loss = 0.37181315\n",
      "Iteration 387, loss = 0.37171016\n",
      "Iteration 388, loss = 0.37034818\n",
      "Iteration 389, loss = 0.37137149\n",
      "Iteration 390, loss = 0.37051401\n",
      "Iteration 391, loss = 0.37025802\n",
      "Iteration 392, loss = 0.37023288\n",
      "Iteration 393, loss = 0.37091448\n",
      "Iteration 394, loss = 0.36983360\n",
      "Iteration 395, loss = 0.36987325\n",
      "Iteration 396, loss = 0.36922632\n",
      "Iteration 397, loss = 0.36981214\n",
      "Iteration 398, loss = 0.36972828\n",
      "Iteration 399, loss = 0.36888096\n",
      "Iteration 400, loss = 0.37007094\n",
      "Iteration 401, loss = 0.36908427\n",
      "Iteration 402, loss = 0.36828890\n",
      "Iteration 403, loss = 0.36823419\n",
      "Iteration 404, loss = 0.36841862\n",
      "Iteration 405, loss = 0.36883170\n",
      "Iteration 406, loss = 0.36929815\n",
      "Iteration 407, loss = 0.36847141\n",
      "Iteration 408, loss = 0.36792648\n",
      "Iteration 409, loss = 0.36772573\n",
      "Iteration 410, loss = 0.36772995\n",
      "Iteration 411, loss = 0.36737732\n",
      "Iteration 412, loss = 0.36685904\n",
      "Iteration 413, loss = 0.36745851\n",
      "Iteration 414, loss = 0.36665994\n",
      "Iteration 415, loss = 0.36702450\n",
      "Iteration 416, loss = 0.36664302\n",
      "Iteration 417, loss = 0.36640195\n",
      "Iteration 418, loss = 0.36680572\n",
      "Iteration 419, loss = 0.36560630\n",
      "Iteration 420, loss = 0.36711000\n",
      "Iteration 421, loss = 0.36586390\n",
      "Iteration 422, loss = 0.36619873\n",
      "Iteration 423, loss = 0.36626261\n",
      "Iteration 424, loss = 0.36550421\n",
      "Iteration 425, loss = 0.36574180\n",
      "Iteration 426, loss = 0.36502298\n",
      "Iteration 427, loss = 0.36491835\n",
      "Iteration 428, loss = 0.36537711\n",
      "Iteration 429, loss = 0.36452342\n",
      "Iteration 430, loss = 0.36440219\n",
      "Iteration 431, loss = 0.36482601\n",
      "Iteration 432, loss = 0.36434360\n",
      "Iteration 433, loss = 0.36431870\n",
      "Iteration 434, loss = 0.36438679\n",
      "Iteration 435, loss = 0.36441799\n",
      "Iteration 436, loss = 0.36399745\n",
      "Iteration 437, loss = 0.36364966\n",
      "Iteration 438, loss = 0.36379234\n",
      "Iteration 439, loss = 0.36307387\n",
      "Iteration 440, loss = 0.36313003\n",
      "Iteration 441, loss = 0.36256684\n",
      "Iteration 442, loss = 0.36317080\n",
      "Iteration 443, loss = 0.36355867\n",
      "Iteration 444, loss = 0.36285840\n",
      "Iteration 445, loss = 0.36180491\n",
      "Iteration 446, loss = 0.36370337\n",
      "Iteration 447, loss = 0.36254061\n",
      "Iteration 448, loss = 0.36208641\n",
      "Iteration 449, loss = 0.36189179\n",
      "Iteration 450, loss = 0.36222026\n",
      "Iteration 451, loss = 0.36156519\n",
      "Iteration 452, loss = 0.36092308\n",
      "Iteration 453, loss = 0.36187048\n",
      "Iteration 454, loss = 0.36238463\n",
      "Iteration 455, loss = 0.36138129\n",
      "Iteration 456, loss = 0.36141720\n",
      "Iteration 457, loss = 0.36100649\n",
      "Iteration 458, loss = 0.36115532\n",
      "Iteration 459, loss = 0.36095010\n",
      "Iteration 460, loss = 0.36159037\n",
      "Iteration 461, loss = 0.36019031\n",
      "Iteration 462, loss = 0.36061918\n",
      "Iteration 463, loss = 0.36048862\n",
      "Iteration 464, loss = 0.35992842\n",
      "Iteration 465, loss = 0.36034155\n",
      "Iteration 466, loss = 0.36000662\n",
      "Iteration 467, loss = 0.36070716\n",
      "Iteration 468, loss = 0.35930628\n",
      "Iteration 469, loss = 0.36130675\n",
      "Iteration 470, loss = 0.35967648\n",
      "Iteration 471, loss = 0.35963192\n",
      "Iteration 472, loss = 0.35942642\n",
      "Iteration 473, loss = 0.35897446\n",
      "Iteration 474, loss = 0.35944337\n",
      "Iteration 475, loss = 0.35861381\n",
      "Iteration 476, loss = 0.35872124\n",
      "Iteration 477, loss = 0.35828167\n",
      "Iteration 478, loss = 0.35981842\n",
      "Iteration 479, loss = 0.35911644\n",
      "Iteration 480, loss = 0.35812103\n",
      "Iteration 481, loss = 0.35747640\n",
      "Iteration 482, loss = 0.35820927\n",
      "Iteration 483, loss = 0.35803158\n",
      "Iteration 484, loss = 0.35819478\n",
      "Iteration 485, loss = 0.35817762\n",
      "Iteration 486, loss = 0.35764835\n",
      "Iteration 487, loss = 0.35727900\n",
      "Iteration 488, loss = 0.35678384\n",
      "Iteration 489, loss = 0.35770333\n",
      "Iteration 490, loss = 0.35710551\n",
      "Iteration 491, loss = 0.35758512\n",
      "Iteration 492, loss = 0.35712100\n",
      "Iteration 493, loss = 0.35694503\n",
      "Iteration 494, loss = 0.35698643\n",
      "Iteration 495, loss = 0.35666927\n",
      "Iteration 496, loss = 0.35608857\n",
      "Iteration 497, loss = 0.35633119\n",
      "Iteration 498, loss = 0.35742992\n",
      "Iteration 499, loss = 0.35658379\n",
      "Iteration 500, loss = 0.35665158\n",
      "Iteration 501, loss = 0.35526110\n",
      "Iteration 502, loss = 0.35586272\n",
      "Iteration 503, loss = 0.35543002\n",
      "Iteration 504, loss = 0.35608198\n",
      "Iteration 505, loss = 0.35545882\n",
      "Iteration 506, loss = 0.35548346\n",
      "Iteration 507, loss = 0.35556002\n",
      "Iteration 508, loss = 0.35506156\n",
      "Iteration 509, loss = 0.35449842\n",
      "Iteration 510, loss = 0.35563322\n",
      "Iteration 511, loss = 0.35470350\n",
      "Iteration 512, loss = 0.35451440\n",
      "Iteration 513, loss = 0.35474665\n",
      "Iteration 514, loss = 0.35469881\n",
      "Iteration 515, loss = 0.35382376\n",
      "Iteration 516, loss = 0.35423970\n",
      "Iteration 517, loss = 0.35530688\n",
      "Iteration 518, loss = 0.35379598\n",
      "Iteration 519, loss = 0.35360194\n",
      "Iteration 520, loss = 0.35392844\n",
      "Iteration 521, loss = 0.35350373\n",
      "Iteration 522, loss = 0.35507690\n",
      "Iteration 523, loss = 0.35263301\n",
      "Iteration 524, loss = 0.35320410\n",
      "Iteration 525, loss = 0.35254932\n",
      "Iteration 526, loss = 0.35265268\n",
      "Iteration 527, loss = 0.35305724\n",
      "Iteration 528, loss = 0.35328767\n",
      "Iteration 529, loss = 0.35336885\n",
      "Iteration 530, loss = 0.35316568\n",
      "Iteration 531, loss = 0.35214701\n",
      "Iteration 532, loss = 0.35362288\n",
      "Iteration 533, loss = 0.35380115\n",
      "Iteration 534, loss = 0.35181557\n",
      "Iteration 535, loss = 0.35258609\n",
      "Iteration 536, loss = 0.35309009\n",
      "Iteration 537, loss = 0.35222321\n",
      "Iteration 538, loss = 0.35216434\n",
      "Iteration 539, loss = 0.35155303\n",
      "Iteration 540, loss = 0.35212922\n",
      "Iteration 541, loss = 0.35176322\n",
      "Iteration 542, loss = 0.35152633\n",
      "Iteration 543, loss = 0.35172806\n",
      "Iteration 544, loss = 0.35068847\n",
      "Iteration 545, loss = 0.35153177\n",
      "Iteration 546, loss = 0.35148453\n",
      "Iteration 547, loss = 0.35167898\n",
      "Iteration 548, loss = 0.35166183\n",
      "Iteration 549, loss = 0.35054727\n",
      "Iteration 550, loss = 0.35044102\n",
      "Iteration 551, loss = 0.35085426\n",
      "Iteration 552, loss = 0.35065379\n",
      "Iteration 553, loss = 0.35053859\n",
      "Iteration 554, loss = 0.35151043\n",
      "Iteration 555, loss = 0.35097644\n",
      "Iteration 556, loss = 0.35047082\n",
      "Iteration 557, loss = 0.34939278\n",
      "Iteration 558, loss = 0.35045195\n",
      "Iteration 559, loss = 0.34970570\n",
      "Iteration 560, loss = 0.34977303\n",
      "Iteration 561, loss = 0.34926714\n",
      "Iteration 562, loss = 0.34986500\n",
      "Iteration 563, loss = 0.34868623\n",
      "Iteration 564, loss = 0.34898048\n",
      "Iteration 565, loss = 0.34956861\n",
      "Iteration 566, loss = 0.34907305\n",
      "Iteration 567, loss = 0.34954077\n",
      "Iteration 568, loss = 0.34889656\n",
      "Iteration 569, loss = 0.34842757\n",
      "Iteration 570, loss = 0.34843259\n",
      "Iteration 571, loss = 0.34841626\n",
      "Iteration 572, loss = 0.34825125\n",
      "Iteration 573, loss = 0.34844586\n",
      "Iteration 574, loss = 0.34743462\n",
      "Iteration 575, loss = 0.34849108\n",
      "Iteration 576, loss = 0.34720368\n",
      "Iteration 577, loss = 0.34772367\n",
      "Iteration 578, loss = 0.34780650\n",
      "Iteration 579, loss = 0.34803968\n",
      "Iteration 580, loss = 0.34764728\n",
      "Iteration 581, loss = 0.34767468\n",
      "Iteration 582, loss = 0.34681095\n",
      "Iteration 583, loss = 0.34706539\n",
      "Iteration 584, loss = 0.34703609\n",
      "Iteration 585, loss = 0.34631121\n",
      "Iteration 586, loss = 0.34777391\n",
      "Iteration 587, loss = 0.34814947\n",
      "Iteration 588, loss = 0.34703444\n",
      "Iteration 589, loss = 0.34688802\n",
      "Iteration 590, loss = 0.34563362\n",
      "Iteration 591, loss = 0.34708672\n",
      "Iteration 592, loss = 0.34622581\n",
      "Iteration 593, loss = 0.34650377\n",
      "Iteration 594, loss = 0.34611345\n",
      "Iteration 595, loss = 0.34683486\n",
      "Iteration 596, loss = 0.34537428\n",
      "Iteration 597, loss = 0.34596894\n",
      "Iteration 598, loss = 0.34652289\n",
      "Iteration 599, loss = 0.34702740\n",
      "Iteration 600, loss = 0.34598662\n",
      "Iteration 601, loss = 0.34610860\n",
      "Iteration 602, loss = 0.34450178\n",
      "Iteration 603, loss = 0.34559843\n",
      "Iteration 604, loss = 0.34567059\n",
      "Iteration 605, loss = 0.34554598\n",
      "Iteration 606, loss = 0.34459979\n",
      "Iteration 607, loss = 0.34515188\n",
      "Iteration 608, loss = 0.34541972\n",
      "Iteration 609, loss = 0.34506806\n",
      "Iteration 610, loss = 0.34498008\n",
      "Iteration 611, loss = 0.34483686\n",
      "Iteration 612, loss = 0.34443278\n",
      "Iteration 613, loss = 0.34432960\n",
      "Iteration 614, loss = 0.34493101\n",
      "Iteration 615, loss = 0.34404116\n",
      "Iteration 616, loss = 0.34402771\n",
      "Iteration 617, loss = 0.34349342\n",
      "Iteration 618, loss = 0.34529321\n",
      "Iteration 619, loss = 0.34468577\n",
      "Iteration 620, loss = 0.34353549\n",
      "Iteration 621, loss = 0.34463627\n",
      "Iteration 622, loss = 0.34366728\n",
      "Iteration 623, loss = 0.34340631\n",
      "Iteration 624, loss = 0.34362218\n",
      "Iteration 625, loss = 0.34462359\n",
      "Iteration 626, loss = 0.34360855\n",
      "Iteration 627, loss = 0.34279895\n",
      "Iteration 628, loss = 0.34402054\n",
      "Iteration 629, loss = 0.34327507\n",
      "Iteration 630, loss = 0.34316069\n",
      "Iteration 631, loss = 0.34307091\n",
      "Iteration 632, loss = 0.34285914\n",
      "Iteration 633, loss = 0.34240605\n",
      "Iteration 634, loss = 0.34458088\n",
      "Iteration 635, loss = 0.34317101\n",
      "Iteration 636, loss = 0.34183923\n",
      "Iteration 637, loss = 0.34266358\n",
      "Iteration 638, loss = 0.34384026\n",
      "Iteration 639, loss = 0.34202638\n",
      "Iteration 640, loss = 0.34170994\n",
      "Iteration 641, loss = 0.34143227\n",
      "Iteration 642, loss = 0.34215263\n",
      "Iteration 643, loss = 0.34247083\n",
      "Iteration 644, loss = 0.34101019\n",
      "Iteration 645, loss = 0.34210184\n",
      "Iteration 646, loss = 0.34267004\n",
      "Iteration 647, loss = 0.34256795\n",
      "Iteration 648, loss = 0.34264023\n",
      "Iteration 649, loss = 0.34188086\n",
      "Iteration 650, loss = 0.34101526\n",
      "Iteration 651, loss = 0.34103499\n",
      "Iteration 652, loss = 0.34070687\n",
      "Iteration 653, loss = 0.34148197\n",
      "Iteration 654, loss = 0.34096887\n",
      "Iteration 655, loss = 0.34100920\n",
      "Iteration 656, loss = 0.34263540\n",
      "Iteration 657, loss = 0.34073345\n",
      "Iteration 658, loss = 0.34149767\n",
      "Iteration 659, loss = 0.34056053\n",
      "Iteration 660, loss = 0.34002108\n",
      "Iteration 661, loss = 0.34161915\n",
      "Iteration 662, loss = 0.33908482\n",
      "Iteration 663, loss = 0.34060365\n",
      "Iteration 664, loss = 0.33969247\n",
      "Iteration 665, loss = 0.34017732\n",
      "Iteration 666, loss = 0.34054865\n",
      "Iteration 667, loss = 0.34013203\n",
      "Iteration 668, loss = 0.34088768\n",
      "Iteration 669, loss = 0.33966630\n",
      "Iteration 670, loss = 0.34149093\n",
      "Iteration 671, loss = 0.33978118\n",
      "Iteration 672, loss = 0.33996932\n",
      "Iteration 673, loss = 0.33968057\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [03:20<03:52, 25.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.54246893\n",
      "Iteration 2, loss = 0.49527453\n",
      "Iteration 3, loss = 0.47474068\n",
      "Iteration 4, loss = 0.46604518\n",
      "Iteration 5, loss = 0.46203175\n",
      "Iteration 6, loss = 0.45977531\n",
      "Iteration 7, loss = 0.45823305\n",
      "Iteration 8, loss = 0.45651609\n",
      "Iteration 9, loss = 0.45538279\n",
      "Iteration 10, loss = 0.45441021\n",
      "Iteration 11, loss = 0.45360411\n",
      "Iteration 12, loss = 0.45258922\n",
      "Iteration 13, loss = 0.45206259\n",
      "Iteration 14, loss = 0.45117045\n",
      "Iteration 15, loss = 0.45032526\n",
      "Iteration 16, loss = 0.44983718\n",
      "Iteration 17, loss = 0.44928574\n",
      "Iteration 18, loss = 0.44884929\n",
      "Iteration 19, loss = 0.44828543\n",
      "Iteration 20, loss = 0.44803221\n",
      "Iteration 21, loss = 0.44747657\n",
      "Iteration 22, loss = 0.44711153\n",
      "Iteration 23, loss = 0.44674048\n",
      "Iteration 24, loss = 0.44621580\n",
      "Iteration 25, loss = 0.44583493\n",
      "Iteration 26, loss = 0.44568298\n",
      "Iteration 27, loss = 0.44546335\n",
      "Iteration 28, loss = 0.44511072\n",
      "Iteration 29, loss = 0.44449388\n",
      "Iteration 30, loss = 0.44420898\n",
      "Iteration 31, loss = 0.44400211\n",
      "Iteration 32, loss = 0.44357018\n",
      "Iteration 33, loss = 0.44358852\n",
      "Iteration 34, loss = 0.44326930\n",
      "Iteration 35, loss = 0.44291770\n",
      "Iteration 36, loss = 0.44267733\n",
      "Iteration 37, loss = 0.44214568\n",
      "Iteration 38, loss = 0.44197759\n",
      "Iteration 39, loss = 0.44167773\n",
      "Iteration 40, loss = 0.44148435\n",
      "Iteration 41, loss = 0.44106979\n",
      "Iteration 42, loss = 0.44096362\n",
      "Iteration 43, loss = 0.44060807\n",
      "Iteration 44, loss = 0.44039794\n",
      "Iteration 45, loss = 0.44001492\n",
      "Iteration 46, loss = 0.43985606\n",
      "Iteration 47, loss = 0.43963183\n",
      "Iteration 48, loss = 0.43935588\n",
      "Iteration 49, loss = 0.43892256\n",
      "Iteration 50, loss = 0.43856395\n",
      "Iteration 51, loss = 0.43796087\n",
      "Iteration 52, loss = 0.43833708\n",
      "Iteration 53, loss = 0.43762625\n",
      "Iteration 54, loss = 0.43755021\n",
      "Iteration 55, loss = 0.43700606\n",
      "Iteration 56, loss = 0.43695232\n",
      "Iteration 57, loss = 0.43681639\n",
      "Iteration 58, loss = 0.43624873\n",
      "Iteration 59, loss = 0.43614002\n",
      "Iteration 60, loss = 0.43572996\n",
      "Iteration 61, loss = 0.43531642\n",
      "Iteration 62, loss = 0.43495302\n",
      "Iteration 63, loss = 0.43506427\n",
      "Iteration 64, loss = 0.43454202\n",
      "Iteration 65, loss = 0.43438397\n",
      "Iteration 66, loss = 0.43390388\n",
      "Iteration 67, loss = 0.43393458\n",
      "Iteration 68, loss = 0.43353466\n",
      "Iteration 69, loss = 0.43324217\n",
      "Iteration 70, loss = 0.43287539\n",
      "Iteration 71, loss = 0.43290172\n",
      "Iteration 72, loss = 0.43227924\n",
      "Iteration 73, loss = 0.43213983\n",
      "Iteration 74, loss = 0.43212563\n",
      "Iteration 75, loss = 0.43145545\n",
      "Iteration 76, loss = 0.43131005\n",
      "Iteration 77, loss = 0.43086370\n",
      "Iteration 78, loss = 0.43063012\n",
      "Iteration 79, loss = 0.43023752\n",
      "Iteration 80, loss = 0.43032826\n",
      "Iteration 81, loss = 0.42992136\n",
      "Iteration 82, loss = 0.42983141\n",
      "Iteration 83, loss = 0.42924973\n",
      "Iteration 84, loss = 0.42923802\n",
      "Iteration 85, loss = 0.42884740\n",
      "Iteration 86, loss = 0.42864272\n",
      "Iteration 87, loss = 0.42826515\n",
      "Iteration 88, loss = 0.42817067\n",
      "Iteration 89, loss = 0.42769437\n",
      "Iteration 90, loss = 0.42765682\n",
      "Iteration 91, loss = 0.42732572\n",
      "Iteration 92, loss = 0.42700744\n",
      "Iteration 93, loss = 0.42699024\n",
      "Iteration 94, loss = 0.42648501\n",
      "Iteration 95, loss = 0.42636264\n",
      "Iteration 96, loss = 0.42638267\n",
      "Iteration 97, loss = 0.42567166\n",
      "Iteration 98, loss = 0.42542505\n",
      "Iteration 99, loss = 0.42537464\n",
      "Iteration 100, loss = 0.42489234\n",
      "Iteration 101, loss = 0.42492300\n",
      "Iteration 102, loss = 0.42489624\n",
      "Iteration 103, loss = 0.42415059\n",
      "Iteration 104, loss = 0.42380373\n",
      "Iteration 105, loss = 0.42421004\n",
      "Iteration 106, loss = 0.42360013\n",
      "Iteration 107, loss = 0.42395334\n",
      "Iteration 108, loss = 0.42330718\n",
      "Iteration 109, loss = 0.42308654\n",
      "Iteration 110, loss = 0.42289717\n",
      "Iteration 111, loss = 0.42268033\n",
      "Iteration 112, loss = 0.42235769\n",
      "Iteration 113, loss = 0.42191522\n",
      "Iteration 114, loss = 0.42161227\n",
      "Iteration 115, loss = 0.42139501\n",
      "Iteration 116, loss = 0.42142365\n",
      "Iteration 117, loss = 0.42075487\n",
      "Iteration 118, loss = 0.42069859\n",
      "Iteration 119, loss = 0.42050862\n",
      "Iteration 120, loss = 0.42022444\n",
      "Iteration 121, loss = 0.41962297\n",
      "Iteration 122, loss = 0.41996859\n",
      "Iteration 123, loss = 0.42015580\n",
      "Iteration 124, loss = 0.41964946\n",
      "Iteration 125, loss = 0.41908890\n",
      "Iteration 126, loss = 0.41880269\n",
      "Iteration 127, loss = 0.41923698\n",
      "Iteration 128, loss = 0.41862359\n",
      "Iteration 129, loss = 0.41838724\n",
      "Iteration 130, loss = 0.41831021\n",
      "Iteration 131, loss = 0.41763694\n",
      "Iteration 132, loss = 0.41773794\n",
      "Iteration 133, loss = 0.41761657\n",
      "Iteration 134, loss = 0.41701013\n",
      "Iteration 135, loss = 0.41659322\n",
      "Iteration 136, loss = 0.41656230\n",
      "Iteration 137, loss = 0.41634056\n",
      "Iteration 138, loss = 0.41604806\n",
      "Iteration 139, loss = 0.41595473\n",
      "Iteration 140, loss = 0.41575588\n",
      "Iteration 141, loss = 0.41511766\n",
      "Iteration 142, loss = 0.41533564\n",
      "Iteration 143, loss = 0.41474477\n",
      "Iteration 144, loss = 0.41403005\n",
      "Iteration 145, loss = 0.41448191\n",
      "Iteration 146, loss = 0.41448912\n",
      "Iteration 147, loss = 0.41417789\n",
      "Iteration 148, loss = 0.41368396\n",
      "Iteration 149, loss = 0.41370436\n",
      "Iteration 150, loss = 0.41295991\n",
      "Iteration 151, loss = 0.41350376\n",
      "Iteration 152, loss = 0.41264145\n",
      "Iteration 153, loss = 0.41282974\n",
      "Iteration 154, loss = 0.41239890\n",
      "Iteration 155, loss = 0.41225766\n",
      "Iteration 156, loss = 0.41215293\n",
      "Iteration 157, loss = 0.41162179\n",
      "Iteration 158, loss = 0.41129935\n",
      "Iteration 159, loss = 0.41114576\n",
      "Iteration 160, loss = 0.41097127\n",
      "Iteration 161, loss = 0.41109668\n",
      "Iteration 162, loss = 0.41056208\n",
      "Iteration 163, loss = 0.41057208\n",
      "Iteration 164, loss = 0.41021359\n",
      "Iteration 165, loss = 0.40978007\n",
      "Iteration 166, loss = 0.40931036\n",
      "Iteration 167, loss = 0.40996653\n",
      "Iteration 168, loss = 0.40898246\n",
      "Iteration 169, loss = 0.40937617\n",
      "Iteration 170, loss = 0.40895642\n",
      "Iteration 171, loss = 0.40846445\n",
      "Iteration 172, loss = 0.40841389\n",
      "Iteration 173, loss = 0.40834466\n",
      "Iteration 174, loss = 0.40791659\n",
      "Iteration 175, loss = 0.40802138\n",
      "Iteration 176, loss = 0.40809043\n",
      "Iteration 177, loss = 0.40813302\n",
      "Iteration 178, loss = 0.40690900\n",
      "Iteration 179, loss = 0.40689967\n",
      "Iteration 180, loss = 0.40683676\n",
      "Iteration 181, loss = 0.40683545\n",
      "Iteration 182, loss = 0.40702103\n",
      "Iteration 183, loss = 0.40611367\n",
      "Iteration 184, loss = 0.40574752\n",
      "Iteration 185, loss = 0.40525765\n",
      "Iteration 186, loss = 0.40569528\n",
      "Iteration 187, loss = 0.40544886\n",
      "Iteration 188, loss = 0.40524145\n",
      "Iteration 189, loss = 0.40516753\n",
      "Iteration 190, loss = 0.40514828\n",
      "Iteration 191, loss = 0.40431517\n",
      "Iteration 192, loss = 0.40485251\n",
      "Iteration 193, loss = 0.40412193\n",
      "Iteration 194, loss = 0.40373070\n",
      "Iteration 195, loss = 0.40374339\n",
      "Iteration 196, loss = 0.40356963\n",
      "Iteration 197, loss = 0.40342960\n",
      "Iteration 198, loss = 0.40299675\n",
      "Iteration 199, loss = 0.40295175\n",
      "Iteration 200, loss = 0.40257157\n",
      "Iteration 201, loss = 0.40219807\n",
      "Iteration 202, loss = 0.40269030\n",
      "Iteration 203, loss = 0.40264709\n",
      "Iteration 204, loss = 0.40212416\n",
      "Iteration 205, loss = 0.40152126\n",
      "Iteration 206, loss = 0.40105257\n",
      "Iteration 207, loss = 0.40119397\n",
      "Iteration 208, loss = 0.40109692\n",
      "Iteration 209, loss = 0.40105167\n",
      "Iteration 210, loss = 0.40103113\n",
      "Iteration 211, loss = 0.39992577\n",
      "Iteration 212, loss = 0.40005520\n",
      "Iteration 213, loss = 0.40009625\n",
      "Iteration 214, loss = 0.40000795\n",
      "Iteration 215, loss = 0.39979321\n",
      "Iteration 216, loss = 0.39933054\n",
      "Iteration 217, loss = 0.39900318\n",
      "Iteration 218, loss = 0.39948933\n",
      "Iteration 219, loss = 0.39885550\n",
      "Iteration 220, loss = 0.39872094\n",
      "Iteration 221, loss = 0.39844335\n",
      "Iteration 222, loss = 0.39845377\n",
      "Iteration 223, loss = 0.39786833\n",
      "Iteration 224, loss = 0.39762392\n",
      "Iteration 225, loss = 0.39759749\n",
      "Iteration 226, loss = 0.39762985\n",
      "Iteration 227, loss = 0.39776140\n",
      "Iteration 228, loss = 0.39700433\n",
      "Iteration 229, loss = 0.39723130\n",
      "Iteration 230, loss = 0.39673107\n",
      "Iteration 231, loss = 0.39590629\n",
      "Iteration 232, loss = 0.39644177\n",
      "Iteration 233, loss = 0.39643485\n",
      "Iteration 234, loss = 0.39606435\n",
      "Iteration 235, loss = 0.39627848\n",
      "Iteration 236, loss = 0.39523429\n",
      "Iteration 237, loss = 0.39554406\n",
      "Iteration 238, loss = 0.39483133\n",
      "Iteration 239, loss = 0.39484493\n",
      "Iteration 240, loss = 0.39449682\n",
      "Iteration 241, loss = 0.39412427\n",
      "Iteration 242, loss = 0.39416687\n",
      "Iteration 243, loss = 0.39447405\n",
      "Iteration 244, loss = 0.39356398\n",
      "Iteration 245, loss = 0.39418953\n",
      "Iteration 246, loss = 0.39335183\n",
      "Iteration 247, loss = 0.39404903\n",
      "Iteration 248, loss = 0.39387585\n",
      "Iteration 249, loss = 0.39298033\n",
      "Iteration 250, loss = 0.39303163\n",
      "Iteration 251, loss = 0.39275077\n",
      "Iteration 252, loss = 0.39254563\n",
      "Iteration 253, loss = 0.39288897\n",
      "Iteration 254, loss = 0.39256037\n",
      "Iteration 255, loss = 0.39154782\n",
      "Iteration 256, loss = 0.39286542\n",
      "Iteration 257, loss = 0.39192355\n",
      "Iteration 258, loss = 0.39218425\n",
      "Iteration 259, loss = 0.39104769\n",
      "Iteration 260, loss = 0.39132770\n",
      "Iteration 261, loss = 0.39071455\n",
      "Iteration 262, loss = 0.39100989\n",
      "Iteration 263, loss = 0.38960056\n",
      "Iteration 264, loss = 0.39028368\n",
      "Iteration 265, loss = 0.39063969\n",
      "Iteration 266, loss = 0.39003656\n",
      "Iteration 267, loss = 0.39025694\n",
      "Iteration 268, loss = 0.38975869\n",
      "Iteration 269, loss = 0.38914305\n",
      "Iteration 270, loss = 0.38884194\n",
      "Iteration 271, loss = 0.38857805\n",
      "Iteration 272, loss = 0.38828357\n",
      "Iteration 273, loss = 0.38892281\n",
      "Iteration 274, loss = 0.38792166\n",
      "Iteration 275, loss = 0.38868911\n",
      "Iteration 276, loss = 0.38832963\n",
      "Iteration 277, loss = 0.38804297\n",
      "Iteration 278, loss = 0.38751433\n",
      "Iteration 279, loss = 0.38829525\n",
      "Iteration 280, loss = 0.38741962\n",
      "Iteration 281, loss = 0.38763619\n",
      "Iteration 282, loss = 0.38737355\n",
      "Iteration 283, loss = 0.38716302\n",
      "Iteration 284, loss = 0.38657253\n",
      "Iteration 285, loss = 0.38681823\n",
      "Iteration 286, loss = 0.38610590\n",
      "Iteration 287, loss = 0.38603179\n",
      "Iteration 288, loss = 0.38657494\n",
      "Iteration 289, loss = 0.38565218\n",
      "Iteration 290, loss = 0.38608962\n",
      "Iteration 291, loss = 0.38492164\n",
      "Iteration 292, loss = 0.38544982\n",
      "Iteration 293, loss = 0.38511575\n",
      "Iteration 294, loss = 0.38475004\n",
      "Iteration 295, loss = 0.38466942\n",
      "Iteration 296, loss = 0.38418053\n",
      "Iteration 297, loss = 0.38449812\n",
      "Iteration 298, loss = 0.38412855\n",
      "Iteration 299, loss = 0.38474204\n",
      "Iteration 300, loss = 0.38413307\n",
      "Iteration 301, loss = 0.38342559\n",
      "Iteration 302, loss = 0.38343773\n",
      "Iteration 303, loss = 0.38309070\n",
      "Iteration 304, loss = 0.38370715\n",
      "Iteration 305, loss = 0.38281105\n",
      "Iteration 306, loss = 0.38326887\n",
      "Iteration 307, loss = 0.38282874\n",
      "Iteration 308, loss = 0.38294297\n",
      "Iteration 309, loss = 0.38237086\n",
      "Iteration 310, loss = 0.38253125\n",
      "Iteration 311, loss = 0.38160892\n",
      "Iteration 312, loss = 0.38120587\n",
      "Iteration 313, loss = 0.38193962\n",
      "Iteration 314, loss = 0.38119789\n",
      "Iteration 315, loss = 0.38093665\n",
      "Iteration 316, loss = 0.38091455\n",
      "Iteration 317, loss = 0.38107885\n",
      "Iteration 318, loss = 0.38103106\n",
      "Iteration 319, loss = 0.38076413\n",
      "Iteration 320, loss = 0.38023611\n",
      "Iteration 321, loss = 0.38063418\n",
      "Iteration 322, loss = 0.37993972\n",
      "Iteration 323, loss = 0.37982510\n",
      "Iteration 324, loss = 0.38079462\n",
      "Iteration 325, loss = 0.37940036\n",
      "Iteration 326, loss = 0.37998132\n",
      "Iteration 327, loss = 0.37912161\n",
      "Iteration 328, loss = 0.37898651\n",
      "Iteration 329, loss = 0.37912281\n",
      "Iteration 330, loss = 0.37822239\n",
      "Iteration 331, loss = 0.37909670\n",
      "Iteration 332, loss = 0.37865933\n",
      "Iteration 333, loss = 0.37921202\n",
      "Iteration 334, loss = 0.37797310\n",
      "Iteration 335, loss = 0.37917374\n",
      "Iteration 336, loss = 0.37858466\n",
      "Iteration 337, loss = 0.37737766\n",
      "Iteration 338, loss = 0.37750619\n",
      "Iteration 339, loss = 0.37741827\n",
      "Iteration 340, loss = 0.37704433\n",
      "Iteration 341, loss = 0.37732057\n",
      "Iteration 342, loss = 0.37675900\n",
      "Iteration 343, loss = 0.37626317\n",
      "Iteration 344, loss = 0.37662843\n",
      "Iteration 345, loss = 0.37605650\n",
      "Iteration 346, loss = 0.37568803\n",
      "Iteration 347, loss = 0.37640960\n",
      "Iteration 348, loss = 0.37629003\n",
      "Iteration 349, loss = 0.37629074\n",
      "Iteration 350, loss = 0.37639937\n",
      "Iteration 351, loss = 0.37491512\n",
      "Iteration 352, loss = 0.37537979\n",
      "Iteration 353, loss = 0.37595673\n",
      "Iteration 354, loss = 0.37525893\n",
      "Iteration 355, loss = 0.37486159\n",
      "Iteration 356, loss = 0.37465059\n",
      "Iteration 357, loss = 0.37433652\n",
      "Iteration 358, loss = 0.37457032\n",
      "Iteration 359, loss = 0.37447590\n",
      "Iteration 360, loss = 0.37460177\n",
      "Iteration 361, loss = 0.37430761\n",
      "Iteration 362, loss = 0.37483475\n",
      "Iteration 363, loss = 0.37423053\n",
      "Iteration 364, loss = 0.37490279\n",
      "Iteration 365, loss = 0.37272079\n",
      "Iteration 366, loss = 0.37334303\n",
      "Iteration 367, loss = 0.37343614\n",
      "Iteration 368, loss = 0.37374466\n",
      "Iteration 369, loss = 0.37336784\n",
      "Iteration 370, loss = 0.37343385\n",
      "Iteration 371, loss = 0.37212208\n",
      "Iteration 372, loss = 0.37297423\n",
      "Iteration 373, loss = 0.37236642\n",
      "Iteration 374, loss = 0.37283831\n",
      "Iteration 375, loss = 0.37234286\n",
      "Iteration 376, loss = 0.37218853\n",
      "Iteration 377, loss = 0.37139230\n",
      "Iteration 378, loss = 0.37153754\n",
      "Iteration 379, loss = 0.37224820\n",
      "Iteration 380, loss = 0.37161286\n",
      "Iteration 381, loss = 0.37160169\n",
      "Iteration 382, loss = 0.37194320\n",
      "Iteration 383, loss = 0.37143588\n",
      "Iteration 384, loss = 0.37075357\n",
      "Iteration 385, loss = 0.37052488\n",
      "Iteration 386, loss = 0.37077027\n",
      "Iteration 387, loss = 0.37116170\n",
      "Iteration 388, loss = 0.37000329\n",
      "Iteration 389, loss = 0.36965006\n",
      "Iteration 390, loss = 0.36954506\n",
      "Iteration 391, loss = 0.37015099\n",
      "Iteration 392, loss = 0.36886254\n",
      "Iteration 393, loss = 0.36980718\n",
      "Iteration 394, loss = 0.36973579\n",
      "Iteration 395, loss = 0.36853925\n",
      "Iteration 396, loss = 0.36949950\n",
      "Iteration 397, loss = 0.36890207\n",
      "Iteration 398, loss = 0.36857611\n",
      "Iteration 399, loss = 0.36871170\n",
      "Iteration 400, loss = 0.36848786\n",
      "Iteration 401, loss = 0.36783394\n",
      "Iteration 402, loss = 0.36817091\n",
      "Iteration 403, loss = 0.36859077\n",
      "Iteration 404, loss = 0.36776712\n",
      "Iteration 405, loss = 0.36778645\n",
      "Iteration 406, loss = 0.36811351\n",
      "Iteration 407, loss = 0.36689269\n",
      "Iteration 408, loss = 0.36760737\n",
      "Iteration 409, loss = 0.36771747\n",
      "Iteration 410, loss = 0.36669518\n",
      "Iteration 411, loss = 0.36802994\n",
      "Iteration 412, loss = 0.36662457\n",
      "Iteration 413, loss = 0.36715975\n",
      "Iteration 414, loss = 0.36691867\n",
      "Iteration 415, loss = 0.36627189\n",
      "Iteration 416, loss = 0.36661223\n",
      "Iteration 417, loss = 0.36575855\n",
      "Iteration 418, loss = 0.36630779\n",
      "Iteration 419, loss = 0.36562219\n",
      "Iteration 420, loss = 0.36691655\n",
      "Iteration 421, loss = 0.36556917\n",
      "Iteration 422, loss = 0.36645881\n",
      "Iteration 423, loss = 0.36544560\n",
      "Iteration 424, loss = 0.36481818\n",
      "Iteration 425, loss = 0.36534458\n",
      "Iteration 426, loss = 0.36516822\n",
      "Iteration 427, loss = 0.36495060\n",
      "Iteration 428, loss = 0.36480241\n",
      "Iteration 429, loss = 0.36535962\n",
      "Iteration 430, loss = 0.36454782\n",
      "Iteration 431, loss = 0.36425940\n",
      "Iteration 432, loss = 0.36494539\n",
      "Iteration 433, loss = 0.36440084\n",
      "Iteration 434, loss = 0.36360675\n",
      "Iteration 435, loss = 0.36338880\n",
      "Iteration 436, loss = 0.36410212\n",
      "Iteration 437, loss = 0.36347408\n",
      "Iteration 438, loss = 0.36479100\n",
      "Iteration 439, loss = 0.36333263\n",
      "Iteration 440, loss = 0.36373255\n",
      "Iteration 441, loss = 0.36320745\n",
      "Iteration 442, loss = 0.36258306\n",
      "Iteration 443, loss = 0.36324104\n",
      "Iteration 444, loss = 0.36224642\n",
      "Iteration 445, loss = 0.36282107\n",
      "Iteration 446, loss = 0.36200329\n",
      "Iteration 447, loss = 0.36256177\n",
      "Iteration 448, loss = 0.36202355\n",
      "Iteration 449, loss = 0.36220109\n",
      "Iteration 450, loss = 0.36195558\n",
      "Iteration 451, loss = 0.36167421\n",
      "Iteration 452, loss = 0.36158664\n",
      "Iteration 453, loss = 0.36163060\n",
      "Iteration 454, loss = 0.36107470\n",
      "Iteration 455, loss = 0.36092477\n",
      "Iteration 456, loss = 0.36078337\n",
      "Iteration 457, loss = 0.36107372\n",
      "Iteration 458, loss = 0.36086727\n",
      "Iteration 459, loss = 0.36049395\n",
      "Iteration 460, loss = 0.36229902\n",
      "Iteration 461, loss = 0.36104426\n",
      "Iteration 462, loss = 0.36043382\n",
      "Iteration 463, loss = 0.35954473\n",
      "Iteration 464, loss = 0.36025036\n",
      "Iteration 465, loss = 0.35912317\n",
      "Iteration 466, loss = 0.36012699\n",
      "Iteration 467, loss = 0.36007652\n",
      "Iteration 468, loss = 0.35974396\n",
      "Iteration 469, loss = 0.35935435\n",
      "Iteration 470, loss = 0.35917639\n",
      "Iteration 471, loss = 0.35912829\n",
      "Iteration 472, loss = 0.35970340\n",
      "Iteration 473, loss = 0.35901932\n",
      "Iteration 474, loss = 0.35900988\n",
      "Iteration 475, loss = 0.35909332\n",
      "Iteration 476, loss = 0.35873984\n",
      "Iteration 477, loss = 0.35834215\n",
      "Iteration 478, loss = 0.35802401\n",
      "Iteration 479, loss = 0.35753813\n",
      "Iteration 480, loss = 0.35749278\n",
      "Iteration 481, loss = 0.35791630\n",
      "Iteration 482, loss = 0.35849503\n",
      "Iteration 483, loss = 0.35787630\n",
      "Iteration 484, loss = 0.35750340\n",
      "Iteration 485, loss = 0.35772628\n",
      "Iteration 486, loss = 0.35724204\n",
      "Iteration 487, loss = 0.35783528\n",
      "Iteration 488, loss = 0.35743599\n",
      "Iteration 489, loss = 0.35642699\n",
      "Iteration 490, loss = 0.35723258\n",
      "Iteration 491, loss = 0.35610840\n",
      "Iteration 492, loss = 0.35697754\n",
      "Iteration 493, loss = 0.35687708\n",
      "Iteration 494, loss = 0.35610113\n",
      "Iteration 495, loss = 0.35587471\n",
      "Iteration 496, loss = 0.35629041\n",
      "Iteration 497, loss = 0.35583748\n",
      "Iteration 498, loss = 0.35518637\n",
      "Iteration 499, loss = 0.35529058\n",
      "Iteration 500, loss = 0.35549980\n",
      "Iteration 501, loss = 0.35547694\n",
      "Iteration 502, loss = 0.35518712\n",
      "Iteration 503, loss = 0.35628656\n",
      "Iteration 504, loss = 0.35571091\n",
      "Iteration 505, loss = 0.35490056\n",
      "Iteration 506, loss = 0.35526048\n",
      "Iteration 507, loss = 0.35563746\n",
      "Iteration 508, loss = 0.35514892\n",
      "Iteration 509, loss = 0.35503357\n",
      "Iteration 510, loss = 0.35421444\n",
      "Iteration 511, loss = 0.35395031\n",
      "Iteration 512, loss = 0.35495613\n",
      "Iteration 513, loss = 0.35437151\n",
      "Iteration 514, loss = 0.35412239\n",
      "Iteration 515, loss = 0.35451160\n",
      "Iteration 516, loss = 0.35442071\n",
      "Iteration 517, loss = 0.35368847\n",
      "Iteration 518, loss = 0.35338368\n",
      "Iteration 519, loss = 0.35451143\n",
      "Iteration 520, loss = 0.35314351\n",
      "Iteration 521, loss = 0.35280883\n",
      "Iteration 522, loss = 0.35365032\n",
      "Iteration 523, loss = 0.35389888\n",
      "Iteration 524, loss = 0.35237506\n",
      "Iteration 525, loss = 0.35317412\n",
      "Iteration 526, loss = 0.35297283\n",
      "Iteration 527, loss = 0.35207108\n",
      "Iteration 528, loss = 0.35297559\n",
      "Iteration 529, loss = 0.35223816\n",
      "Iteration 530, loss = 0.35217455\n",
      "Iteration 531, loss = 0.35124051\n",
      "Iteration 532, loss = 0.35233780\n",
      "Iteration 533, loss = 0.35221963\n",
      "Iteration 534, loss = 0.35215843\n",
      "Iteration 535, loss = 0.35171900\n",
      "Iteration 536, loss = 0.35222781\n",
      "Iteration 537, loss = 0.35138773\n",
      "Iteration 538, loss = 0.35098213\n",
      "Iteration 539, loss = 0.35226742\n",
      "Iteration 540, loss = 0.35171826\n",
      "Iteration 541, loss = 0.35135012\n",
      "Iteration 542, loss = 0.35127750\n",
      "Iteration 543, loss = 0.35143982\n",
      "Iteration 544, loss = 0.35067034\n",
      "Iteration 545, loss = 0.35030009\n",
      "Iteration 546, loss = 0.35043979\n",
      "Iteration 547, loss = 0.35027597\n",
      "Iteration 548, loss = 0.34977557\n",
      "Iteration 549, loss = 0.35063961\n",
      "Iteration 550, loss = 0.34999151\n",
      "Iteration 551, loss = 0.35108199\n",
      "Iteration 552, loss = 0.34985913\n",
      "Iteration 553, loss = 0.34970601\n",
      "Iteration 554, loss = 0.34946707\n",
      "Iteration 555, loss = 0.34961028\n",
      "Iteration 556, loss = 0.34924153\n",
      "Iteration 557, loss = 0.34929728\n",
      "Iteration 558, loss = 0.34982724\n",
      "Iteration 559, loss = 0.34984316\n",
      "Iteration 560, loss = 0.34994395\n",
      "Iteration 561, loss = 0.34957711\n",
      "Iteration 562, loss = 0.34841810\n",
      "Iteration 563, loss = 0.34925937\n",
      "Iteration 564, loss = 0.34900623\n",
      "Iteration 565, loss = 0.34771456\n",
      "Iteration 566, loss = 0.34839584\n",
      "Iteration 567, loss = 0.34877205\n",
      "Iteration 568, loss = 0.34833021\n",
      "Iteration 569, loss = 0.34877033\n",
      "Iteration 570, loss = 0.34865408\n",
      "Iteration 571, loss = 0.34781222\n",
      "Iteration 572, loss = 0.34818889\n",
      "Iteration 573, loss = 0.34854204\n",
      "Iteration 574, loss = 0.34756124\n",
      "Iteration 575, loss = 0.34736777\n",
      "Iteration 576, loss = 0.34684886\n",
      "Iteration 577, loss = 0.34750155\n",
      "Iteration 578, loss = 0.34675166\n",
      "Iteration 579, loss = 0.34677025\n",
      "Iteration 580, loss = 0.34706003\n",
      "Iteration 581, loss = 0.34702011\n",
      "Iteration 582, loss = 0.34670188\n",
      "Iteration 583, loss = 0.34597226\n",
      "Iteration 584, loss = 0.34629645\n",
      "Iteration 585, loss = 0.34672856\n",
      "Iteration 586, loss = 0.34759626\n",
      "Iteration 587, loss = 0.34756499\n",
      "Iteration 588, loss = 0.34655448\n",
      "Iteration 589, loss = 0.34659644\n",
      "Iteration 590, loss = 0.34633372\n",
      "Iteration 591, loss = 0.34597167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [03:49<03:35, 26.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 592, loss = 0.34648242\n",
      "Iteration 593, loss = 0.34590342\n",
      "Iteration 594, loss = 0.34605411\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56305500\n",
      "Iteration 2, loss = 0.49974938\n",
      "Iteration 3, loss = 0.47872444\n",
      "Iteration 4, loss = 0.46776993\n",
      "Iteration 5, loss = 0.46150140\n",
      "Iteration 6, loss = 0.45777733\n",
      "Iteration 7, loss = 0.45507776\n",
      "Iteration 8, loss = 0.45330569\n",
      "Iteration 9, loss = 0.45212574\n",
      "Iteration 10, loss = 0.45090376\n",
      "Iteration 11, loss = 0.45005708\n",
      "Iteration 12, loss = 0.44925929\n",
      "Iteration 13, loss = 0.44864414\n",
      "Iteration 14, loss = 0.44819970\n",
      "Iteration 15, loss = 0.44726492\n",
      "Iteration 16, loss = 0.44704882\n",
      "Iteration 17, loss = 0.44670453\n",
      "Iteration 18, loss = 0.44623157\n",
      "Iteration 19, loss = 0.44584556\n",
      "Iteration 20, loss = 0.44530960\n",
      "Iteration 21, loss = 0.44517362\n",
      "Iteration 22, loss = 0.44461211\n",
      "Iteration 23, loss = 0.44455574\n",
      "Iteration 24, loss = 0.44408606\n",
      "Iteration 25, loss = 0.44401415\n",
      "Iteration 26, loss = 0.44333172\n",
      "Iteration 27, loss = 0.44305713\n",
      "Iteration 28, loss = 0.44259996\n",
      "Iteration 29, loss = 0.44227293\n",
      "Iteration 30, loss = 0.44215205\n",
      "Iteration 31, loss = 0.44175256\n",
      "Iteration 32, loss = 0.44137777\n",
      "Iteration 33, loss = 0.44127663\n",
      "Iteration 34, loss = 0.44073196\n",
      "Iteration 35, loss = 0.44069239\n",
      "Iteration 36, loss = 0.44013815\n",
      "Iteration 37, loss = 0.43980387\n",
      "Iteration 38, loss = 0.43970731\n",
      "Iteration 39, loss = 0.43923884\n",
      "Iteration 40, loss = 0.43902808\n",
      "Iteration 41, loss = 0.43882202\n",
      "Iteration 42, loss = 0.43848463\n",
      "Iteration 43, loss = 0.43820273\n",
      "Iteration 44, loss = 0.43801273\n",
      "Iteration 45, loss = 0.43778459\n",
      "Iteration 46, loss = 0.43733813\n",
      "Iteration 47, loss = 0.43691891\n",
      "Iteration 48, loss = 0.43667699\n",
      "Iteration 49, loss = 0.43642973\n",
      "Iteration 50, loss = 0.43622451\n",
      "Iteration 51, loss = 0.43586880\n",
      "Iteration 52, loss = 0.43560442\n",
      "Iteration 53, loss = 0.43532789\n",
      "Iteration 54, loss = 0.43501386\n",
      "Iteration 55, loss = 0.43493831\n",
      "Iteration 56, loss = 0.43436433\n",
      "Iteration 57, loss = 0.43396479\n",
      "Iteration 58, loss = 0.43387611\n",
      "Iteration 59, loss = 0.43352067\n",
      "Iteration 60, loss = 0.43328168\n",
      "Iteration 61, loss = 0.43349393\n",
      "Iteration 62, loss = 0.43288408\n",
      "Iteration 63, loss = 0.43251437\n",
      "Iteration 64, loss = 0.43236064\n",
      "Iteration 65, loss = 0.43202147\n",
      "Iteration 66, loss = 0.43177502\n",
      "Iteration 67, loss = 0.43140018\n",
      "Iteration 68, loss = 0.43112886\n",
      "Iteration 69, loss = 0.43083957\n",
      "Iteration 70, loss = 0.43054107\n",
      "Iteration 71, loss = 0.43043890\n",
      "Iteration 72, loss = 0.42993104\n",
      "Iteration 73, loss = 0.42983776\n",
      "Iteration 74, loss = 0.42962423\n",
      "Iteration 75, loss = 0.42930895\n",
      "Iteration 76, loss = 0.42900375\n",
      "Iteration 77, loss = 0.42888828\n",
      "Iteration 78, loss = 0.42857357\n",
      "Iteration 79, loss = 0.42853041\n",
      "Iteration 80, loss = 0.42796726\n",
      "Iteration 81, loss = 0.42777541\n",
      "Iteration 82, loss = 0.42737922\n",
      "Iteration 83, loss = 0.42724411\n",
      "Iteration 84, loss = 0.42693748\n",
      "Iteration 85, loss = 0.42654189\n",
      "Iteration 86, loss = 0.42640458\n",
      "Iteration 87, loss = 0.42607507\n",
      "Iteration 88, loss = 0.42620962\n",
      "Iteration 89, loss = 0.42575917\n",
      "Iteration 90, loss = 0.42560054\n",
      "Iteration 91, loss = 0.42541032\n",
      "Iteration 92, loss = 0.42529495\n",
      "Iteration 93, loss = 0.42457458\n",
      "Iteration 94, loss = 0.42505501\n",
      "Iteration 95, loss = 0.42432715\n",
      "Iteration 96, loss = 0.42393852\n",
      "Iteration 97, loss = 0.42397202\n",
      "Iteration 98, loss = 0.42344930\n",
      "Iteration 99, loss = 0.42331240\n",
      "Iteration 100, loss = 0.42317513\n",
      "Iteration 101, loss = 0.42299847\n",
      "Iteration 102, loss = 0.42273829\n",
      "Iteration 103, loss = 0.42243653\n",
      "Iteration 104, loss = 0.42204170\n",
      "Iteration 105, loss = 0.42213385\n",
      "Iteration 106, loss = 0.42183420\n",
      "Iteration 107, loss = 0.42153191\n",
      "Iteration 108, loss = 0.42090631\n",
      "Iteration 109, loss = 0.42067104\n",
      "Iteration 110, loss = 0.42039685\n",
      "Iteration 111, loss = 0.42033124\n",
      "Iteration 112, loss = 0.42013377\n",
      "Iteration 113, loss = 0.42017247\n",
      "Iteration 114, loss = 0.41976332\n",
      "Iteration 115, loss = 0.41960142\n",
      "Iteration 116, loss = 0.41930585\n",
      "Iteration 117, loss = 0.41898070\n",
      "Iteration 118, loss = 0.41859444\n",
      "Iteration 119, loss = 0.41853698\n",
      "Iteration 120, loss = 0.41881185\n",
      "Iteration 121, loss = 0.41796991\n",
      "Iteration 122, loss = 0.41781577\n",
      "Iteration 123, loss = 0.41759198\n",
      "Iteration 124, loss = 0.41761471\n",
      "Iteration 125, loss = 0.41705207\n",
      "Iteration 126, loss = 0.41664604\n",
      "Iteration 127, loss = 0.41721989\n",
      "Iteration 128, loss = 0.41641659\n",
      "Iteration 129, loss = 0.41663372\n",
      "Iteration 130, loss = 0.41582686\n",
      "Iteration 131, loss = 0.41616638\n",
      "Iteration 132, loss = 0.41577966\n",
      "Iteration 133, loss = 0.41538302\n",
      "Iteration 134, loss = 0.41520846\n",
      "Iteration 135, loss = 0.41489706\n",
      "Iteration 136, loss = 0.41462571\n",
      "Iteration 137, loss = 0.41441868\n",
      "Iteration 138, loss = 0.41420503\n",
      "Iteration 139, loss = 0.41460440\n",
      "Iteration 140, loss = 0.41426131\n",
      "Iteration 141, loss = 0.41357671\n",
      "Iteration 142, loss = 0.41328512\n",
      "Iteration 143, loss = 0.41309193\n",
      "Iteration 144, loss = 0.41289743\n",
      "Iteration 145, loss = 0.41258107\n",
      "Iteration 146, loss = 0.41257283\n",
      "Iteration 147, loss = 0.41220512\n",
      "Iteration 148, loss = 0.41204338\n",
      "Iteration 149, loss = 0.41185847\n",
      "Iteration 150, loss = 0.41154721\n",
      "Iteration 151, loss = 0.41110200\n",
      "Iteration 152, loss = 0.41134606\n",
      "Iteration 153, loss = 0.41037372\n",
      "Iteration 154, loss = 0.41083179\n",
      "Iteration 155, loss = 0.41075692\n",
      "Iteration 156, loss = 0.41020971\n",
      "Iteration 157, loss = 0.40963839\n",
      "Iteration 158, loss = 0.40983745\n",
      "Iteration 159, loss = 0.40921236\n",
      "Iteration 160, loss = 0.40947086\n",
      "Iteration 161, loss = 0.40906612\n",
      "Iteration 162, loss = 0.40848878\n",
      "Iteration 163, loss = 0.40834513\n",
      "Iteration 164, loss = 0.40828871\n",
      "Iteration 165, loss = 0.40841218\n",
      "Iteration 166, loss = 0.40792960\n",
      "Iteration 167, loss = 0.40732638\n",
      "Iteration 168, loss = 0.40722647\n",
      "Iteration 169, loss = 0.40711802\n",
      "Iteration 170, loss = 0.40672972\n",
      "Iteration 171, loss = 0.40671657\n",
      "Iteration 172, loss = 0.40672582\n",
      "Iteration 173, loss = 0.40588576\n",
      "Iteration 174, loss = 0.40606080\n",
      "Iteration 175, loss = 0.40616472\n",
      "Iteration 176, loss = 0.40542107\n",
      "Iteration 177, loss = 0.40521748\n",
      "Iteration 178, loss = 0.40544621\n",
      "Iteration 179, loss = 0.40485711\n",
      "Iteration 180, loss = 0.40437044\n",
      "Iteration 181, loss = 0.40388782\n",
      "Iteration 182, loss = 0.40417450\n",
      "Iteration 183, loss = 0.40422965\n",
      "Iteration 184, loss = 0.40384859\n",
      "Iteration 185, loss = 0.40324816\n",
      "Iteration 186, loss = 0.40286281\n",
      "Iteration 187, loss = 0.40372248\n",
      "Iteration 188, loss = 0.40238078\n",
      "Iteration 189, loss = 0.40218842\n",
      "Iteration 190, loss = 0.40217631\n",
      "Iteration 191, loss = 0.40233192\n",
      "Iteration 192, loss = 0.40192177\n",
      "Iteration 193, loss = 0.40154177\n",
      "Iteration 194, loss = 0.40128232\n",
      "Iteration 195, loss = 0.40121836\n",
      "Iteration 196, loss = 0.40076644\n",
      "Iteration 197, loss = 0.40078660\n",
      "Iteration 198, loss = 0.40020850\n",
      "Iteration 199, loss = 0.40011386\n",
      "Iteration 200, loss = 0.40028891\n",
      "Iteration 201, loss = 0.39956315\n",
      "Iteration 202, loss = 0.39919114\n",
      "Iteration 203, loss = 0.40042591\n",
      "Iteration 204, loss = 0.39885497\n",
      "Iteration 205, loss = 0.39898385\n",
      "Iteration 206, loss = 0.39863640\n",
      "Iteration 207, loss = 0.39820247\n",
      "Iteration 208, loss = 0.39827606\n",
      "Iteration 209, loss = 0.39792823\n",
      "Iteration 210, loss = 0.39804460\n",
      "Iteration 211, loss = 0.39758004\n",
      "Iteration 212, loss = 0.39769565\n",
      "Iteration 213, loss = 0.39776029\n",
      "Iteration 214, loss = 0.39711539\n",
      "Iteration 215, loss = 0.39601928\n",
      "Iteration 216, loss = 0.39650172\n",
      "Iteration 217, loss = 0.39639862\n",
      "Iteration 218, loss = 0.39627298\n",
      "Iteration 219, loss = 0.39552217\n",
      "Iteration 220, loss = 0.39521164\n",
      "Iteration 221, loss = 0.39553382\n",
      "Iteration 222, loss = 0.39516732\n",
      "Iteration 223, loss = 0.39481629\n",
      "Iteration 224, loss = 0.39452705\n",
      "Iteration 225, loss = 0.39412146\n",
      "Iteration 226, loss = 0.39409322\n",
      "Iteration 227, loss = 0.39399116\n",
      "Iteration 228, loss = 0.39411150\n",
      "Iteration 229, loss = 0.39406433\n",
      "Iteration 230, loss = 0.39336363\n",
      "Iteration 231, loss = 0.39337873\n",
      "Iteration 232, loss = 0.39320626\n",
      "Iteration 233, loss = 0.39265424\n",
      "Iteration 234, loss = 0.39261856\n",
      "Iteration 235, loss = 0.39247150\n",
      "Iteration 236, loss = 0.39269412\n",
      "Iteration 237, loss = 0.39199622\n",
      "Iteration 238, loss = 0.39225633\n",
      "Iteration 239, loss = 0.39154219\n",
      "Iteration 240, loss = 0.39131485\n",
      "Iteration 241, loss = 0.39059026\n",
      "Iteration 242, loss = 0.39118147\n",
      "Iteration 243, loss = 0.39057428\n",
      "Iteration 244, loss = 0.39022522\n",
      "Iteration 245, loss = 0.39042586\n",
      "Iteration 246, loss = 0.38957072\n",
      "Iteration 247, loss = 0.38996792\n",
      "Iteration 248, loss = 0.38934728\n",
      "Iteration 249, loss = 0.38981566\n",
      "Iteration 250, loss = 0.38878507\n",
      "Iteration 251, loss = 0.38846234\n",
      "Iteration 252, loss = 0.38896412\n",
      "Iteration 253, loss = 0.38886044\n",
      "Iteration 254, loss = 0.38889503\n",
      "Iteration 255, loss = 0.38791517\n",
      "Iteration 256, loss = 0.38797069\n",
      "Iteration 257, loss = 0.38706492\n",
      "Iteration 258, loss = 0.38729621\n",
      "Iteration 259, loss = 0.38705107\n",
      "Iteration 260, loss = 0.38667777\n",
      "Iteration 261, loss = 0.38638157\n",
      "Iteration 262, loss = 0.38678914\n",
      "Iteration 263, loss = 0.38623908\n",
      "Iteration 264, loss = 0.38600275\n",
      "Iteration 265, loss = 0.38608677\n",
      "Iteration 266, loss = 0.38534650\n",
      "Iteration 267, loss = 0.38517566\n",
      "Iteration 268, loss = 0.38528623\n",
      "Iteration 269, loss = 0.38499184\n",
      "Iteration 270, loss = 0.38467994\n",
      "Iteration 271, loss = 0.38389237\n",
      "Iteration 272, loss = 0.38476283\n",
      "Iteration 273, loss = 0.38368870\n",
      "Iteration 274, loss = 0.38351522\n",
      "Iteration 275, loss = 0.38372748\n",
      "Iteration 276, loss = 0.38378487\n",
      "Iteration 277, loss = 0.38354256\n",
      "Iteration 278, loss = 0.38259302\n",
      "Iteration 279, loss = 0.38313652\n",
      "Iteration 280, loss = 0.38289999\n",
      "Iteration 281, loss = 0.38169685\n",
      "Iteration 282, loss = 0.38215247\n",
      "Iteration 283, loss = 0.38202704\n",
      "Iteration 284, loss = 0.38222435\n",
      "Iteration 285, loss = 0.38159896\n",
      "Iteration 286, loss = 0.38078820\n",
      "Iteration 287, loss = 0.38165817\n",
      "Iteration 288, loss = 0.38106404\n",
      "Iteration 289, loss = 0.38059871\n",
      "Iteration 290, loss = 0.38029894\n",
      "Iteration 291, loss = 0.38003320\n",
      "Iteration 292, loss = 0.38034605\n",
      "Iteration 293, loss = 0.37999126\n",
      "Iteration 294, loss = 0.37946393\n",
      "Iteration 295, loss = 0.37921642\n",
      "Iteration 296, loss = 0.37873910\n",
      "Iteration 297, loss = 0.37864314\n",
      "Iteration 298, loss = 0.37892142\n",
      "Iteration 299, loss = 0.37832017\n",
      "Iteration 300, loss = 0.37817989\n",
      "Iteration 301, loss = 0.37754008\n",
      "Iteration 302, loss = 0.37881022\n",
      "Iteration 303, loss = 0.37684292\n",
      "Iteration 304, loss = 0.37744876\n",
      "Iteration 305, loss = 0.37756549\n",
      "Iteration 306, loss = 0.37665165\n",
      "Iteration 307, loss = 0.37705483\n",
      "Iteration 308, loss = 0.37658346\n",
      "Iteration 309, loss = 0.37626579\n",
      "Iteration 310, loss = 0.37682880\n",
      "Iteration 311, loss = 0.37630459\n",
      "Iteration 312, loss = 0.37564559\n",
      "Iteration 313, loss = 0.37562578\n",
      "Iteration 314, loss = 0.37550836\n",
      "Iteration 315, loss = 0.37495992\n",
      "Iteration 316, loss = 0.37493340\n",
      "Iteration 317, loss = 0.37496367\n",
      "Iteration 318, loss = 0.37423908\n",
      "Iteration 319, loss = 0.37406957\n",
      "Iteration 320, loss = 0.37424224\n",
      "Iteration 321, loss = 0.37431245\n",
      "Iteration 322, loss = 0.37366493\n",
      "Iteration 323, loss = 0.37339281\n",
      "Iteration 324, loss = 0.37299116\n",
      "Iteration 325, loss = 0.37212037\n",
      "Iteration 326, loss = 0.37306541\n",
      "Iteration 327, loss = 0.37266972\n",
      "Iteration 328, loss = 0.37235673\n",
      "Iteration 329, loss = 0.37198998\n",
      "Iteration 330, loss = 0.37145714\n",
      "Iteration 331, loss = 0.37151391\n",
      "Iteration 332, loss = 0.37205063\n",
      "Iteration 333, loss = 0.37185578\n",
      "Iteration 334, loss = 0.37071624\n",
      "Iteration 335, loss = 0.37030063\n",
      "Iteration 336, loss = 0.37029401\n",
      "Iteration 337, loss = 0.36997484\n",
      "Iteration 338, loss = 0.36994977\n",
      "Iteration 339, loss = 0.37020089\n",
      "Iteration 340, loss = 0.36939913\n",
      "Iteration 341, loss = 0.36932205\n",
      "Iteration 342, loss = 0.36951287\n",
      "Iteration 343, loss = 0.36938371\n",
      "Iteration 344, loss = 0.36821433\n",
      "Iteration 345, loss = 0.36826430\n",
      "Iteration 346, loss = 0.36853269\n",
      "Iteration 347, loss = 0.36895202\n",
      "Iteration 348, loss = 0.36838155\n",
      "Iteration 349, loss = 0.36777216\n",
      "Iteration 350, loss = 0.36710241\n",
      "Iteration 351, loss = 0.36676572\n",
      "Iteration 352, loss = 0.36745452\n",
      "Iteration 353, loss = 0.36681966\n",
      "Iteration 354, loss = 0.36673459\n",
      "Iteration 355, loss = 0.36714419\n",
      "Iteration 356, loss = 0.36648832\n",
      "Iteration 357, loss = 0.36644341\n",
      "Iteration 358, loss = 0.36539605\n",
      "Iteration 359, loss = 0.36616020\n",
      "Iteration 360, loss = 0.36597595\n",
      "Iteration 361, loss = 0.36531015\n",
      "Iteration 362, loss = 0.36530810\n",
      "Iteration 363, loss = 0.36507891\n",
      "Iteration 364, loss = 0.36471473\n",
      "Iteration 365, loss = 0.36458194\n",
      "Iteration 366, loss = 0.36432697\n",
      "Iteration 367, loss = 0.36425726\n",
      "Iteration 368, loss = 0.36408974\n",
      "Iteration 369, loss = 0.36419016\n",
      "Iteration 370, loss = 0.36316350\n",
      "Iteration 371, loss = 0.36355912\n",
      "Iteration 372, loss = 0.36355435\n",
      "Iteration 373, loss = 0.36299433\n",
      "Iteration 374, loss = 0.36285332\n",
      "Iteration 375, loss = 0.36231555\n",
      "Iteration 376, loss = 0.36253226\n",
      "Iteration 377, loss = 0.36173699\n",
      "Iteration 378, loss = 0.36234242\n",
      "Iteration 379, loss = 0.36196078\n",
      "Iteration 380, loss = 0.36095927\n",
      "Iteration 381, loss = 0.36137763\n",
      "Iteration 382, loss = 0.36072327\n",
      "Iteration 383, loss = 0.36043667\n",
      "Iteration 384, loss = 0.36064462\n",
      "Iteration 385, loss = 0.36009032\n",
      "Iteration 386, loss = 0.36094347\n",
      "Iteration 387, loss = 0.36018403\n",
      "Iteration 388, loss = 0.35978132\n",
      "Iteration 389, loss = 0.36032687\n",
      "Iteration 390, loss = 0.35963308\n",
      "Iteration 391, loss = 0.35934018\n",
      "Iteration 392, loss = 0.35964956\n",
      "Iteration 393, loss = 0.35883531\n",
      "Iteration 394, loss = 0.35910995\n",
      "Iteration 395, loss = 0.35885958\n",
      "Iteration 396, loss = 0.35868298\n",
      "Iteration 397, loss = 0.35878211\n",
      "Iteration 398, loss = 0.35782832\n",
      "Iteration 399, loss = 0.35821765\n",
      "Iteration 400, loss = 0.35847460\n",
      "Iteration 401, loss = 0.35726968\n",
      "Iteration 402, loss = 0.35785986\n",
      "Iteration 403, loss = 0.35739201\n",
      "Iteration 404, loss = 0.35762980\n",
      "Iteration 405, loss = 0.35690569\n",
      "Iteration 406, loss = 0.35712979\n",
      "Iteration 407, loss = 0.35648098\n",
      "Iteration 408, loss = 0.35692204\n",
      "Iteration 409, loss = 0.35655865\n",
      "Iteration 410, loss = 0.35717924\n",
      "Iteration 411, loss = 0.35631830\n",
      "Iteration 412, loss = 0.35519889\n",
      "Iteration 413, loss = 0.35637432\n",
      "Iteration 414, loss = 0.35536402\n",
      "Iteration 415, loss = 0.35494793\n",
      "Iteration 416, loss = 0.35479416\n",
      "Iteration 417, loss = 0.35457246\n",
      "Iteration 418, loss = 0.35549398\n",
      "Iteration 419, loss = 0.35397791\n",
      "Iteration 420, loss = 0.35473235\n",
      "Iteration 421, loss = 0.35432178\n",
      "Iteration 422, loss = 0.35394070\n",
      "Iteration 423, loss = 0.35307312\n",
      "Iteration 424, loss = 0.35368254\n",
      "Iteration 425, loss = 0.35382765\n",
      "Iteration 426, loss = 0.35312340\n",
      "Iteration 427, loss = 0.35364683\n",
      "Iteration 428, loss = 0.35247040\n",
      "Iteration 429, loss = 0.35291872\n",
      "Iteration 430, loss = 0.35262774\n",
      "Iteration 431, loss = 0.35247631\n",
      "Iteration 432, loss = 0.35206038\n",
      "Iteration 433, loss = 0.35224687\n",
      "Iteration 434, loss = 0.35247433\n",
      "Iteration 435, loss = 0.35238477\n",
      "Iteration 436, loss = 0.35144832\n",
      "Iteration 437, loss = 0.35220044\n",
      "Iteration 438, loss = 0.35194570\n",
      "Iteration 439, loss = 0.35183606\n",
      "Iteration 440, loss = 0.35102226\n",
      "Iteration 441, loss = 0.35201273\n",
      "Iteration 442, loss = 0.35042160\n",
      "Iteration 443, loss = 0.35083445\n",
      "Iteration 444, loss = 0.35037287\n",
      "Iteration 445, loss = 0.35012050\n",
      "Iteration 446, loss = 0.35009696\n",
      "Iteration 447, loss = 0.34960770\n",
      "Iteration 448, loss = 0.35027177\n",
      "Iteration 449, loss = 0.35032190\n",
      "Iteration 450, loss = 0.35003888\n",
      "Iteration 451, loss = 0.34983026\n",
      "Iteration 452, loss = 0.34950705\n",
      "Iteration 453, loss = 0.34836940\n",
      "Iteration 454, loss = 0.34981773\n",
      "Iteration 455, loss = 0.34899133\n",
      "Iteration 456, loss = 0.34901541\n",
      "Iteration 457, loss = 0.34894604\n",
      "Iteration 458, loss = 0.34784856\n",
      "Iteration 459, loss = 0.34823717\n",
      "Iteration 460, loss = 0.34824116\n",
      "Iteration 461, loss = 0.34809878\n",
      "Iteration 462, loss = 0.34852122\n",
      "Iteration 463, loss = 0.34715401\n",
      "Iteration 464, loss = 0.34710652\n",
      "Iteration 465, loss = 0.34769730\n",
      "Iteration 466, loss = 0.34707418\n",
      "Iteration 467, loss = 0.34662950\n",
      "Iteration 468, loss = 0.34756648\n",
      "Iteration 469, loss = 0.34726336\n",
      "Iteration 470, loss = 0.34689523\n",
      "Iteration 471, loss = 0.34679500\n",
      "Iteration 472, loss = 0.34702588\n",
      "Iteration 473, loss = 0.34591380\n",
      "Iteration 474, loss = 0.34656496\n",
      "Iteration 475, loss = 0.34617558\n",
      "Iteration 476, loss = 0.34678306\n",
      "Iteration 477, loss = 0.34572695\n",
      "Iteration 478, loss = 0.34507114\n",
      "Iteration 479, loss = 0.34576853\n",
      "Iteration 480, loss = 0.34544014\n",
      "Iteration 481, loss = 0.34525467\n",
      "Iteration 482, loss = 0.34516638\n",
      "Iteration 483, loss = 0.34527449\n",
      "Iteration 484, loss = 0.34480478\n",
      "Iteration 485, loss = 0.34495145\n",
      "Iteration 486, loss = 0.34469248\n",
      "Iteration 487, loss = 0.34336733\n",
      "Iteration 488, loss = 0.34411099\n",
      "Iteration 489, loss = 0.34343162\n",
      "Iteration 490, loss = 0.34326162\n",
      "Iteration 491, loss = 0.34342667\n",
      "Iteration 492, loss = 0.34361163\n",
      "Iteration 493, loss = 0.34330577\n",
      "Iteration 494, loss = 0.34270850\n",
      "Iteration 495, loss = 0.34286484\n",
      "Iteration 496, loss = 0.34216149\n",
      "Iteration 497, loss = 0.34313363\n",
      "Iteration 498, loss = 0.34228936\n",
      "Iteration 499, loss = 0.34299753\n",
      "Iteration 500, loss = 0.34267588\n",
      "Iteration 501, loss = 0.34229177\n",
      "Iteration 502, loss = 0.34207708\n",
      "Iteration 503, loss = 0.34193357\n",
      "Iteration 504, loss = 0.34213723\n",
      "Iteration 505, loss = 0.34173668\n",
      "Iteration 506, loss = 0.34162704\n",
      "Iteration 507, loss = 0.34219278\n",
      "Iteration 508, loss = 0.34169634\n",
      "Iteration 509, loss = 0.34181862\n",
      "Iteration 510, loss = 0.34137661\n",
      "Iteration 511, loss = 0.34100754\n",
      "Iteration 512, loss = 0.34022870\n",
      "Iteration 513, loss = 0.34058188\n",
      "Iteration 514, loss = 0.34058454\n",
      "Iteration 515, loss = 0.34059182\n",
      "Iteration 516, loss = 0.34054036\n",
      "Iteration 517, loss = 0.34030129\n",
      "Iteration 518, loss = 0.34099439\n",
      "Iteration 519, loss = 0.34001740\n",
      "Iteration 520, loss = 0.34044589\n",
      "Iteration 521, loss = 0.33984881\n",
      "Iteration 522, loss = 0.33984376\n",
      "Iteration 523, loss = 0.33868422\n",
      "Iteration 524, loss = 0.33844095\n",
      "Iteration 525, loss = 0.33886180\n",
      "Iteration 526, loss = 0.33873599\n",
      "Iteration 527, loss = 0.33909871\n",
      "Iteration 528, loss = 0.33893666\n",
      "Iteration 529, loss = 0.33853293\n",
      "Iteration 530, loss = 0.33803378\n",
      "Iteration 531, loss = 0.33778478\n",
      "Iteration 532, loss = 0.33830109\n",
      "Iteration 533, loss = 0.33833208\n",
      "Iteration 534, loss = 0.33728026\n",
      "Iteration 535, loss = 0.33804393\n",
      "Iteration 536, loss = 0.33785559\n",
      "Iteration 537, loss = 0.33794771\n",
      "Iteration 538, loss = 0.33693751\n",
      "Iteration 539, loss = 0.33813322\n",
      "Iteration 540, loss = 0.33734944\n",
      "Iteration 541, loss = 0.33759891\n",
      "Iteration 542, loss = 0.33675738\n",
      "Iteration 543, loss = 0.33663561\n",
      "Iteration 544, loss = 0.33678315\n",
      "Iteration 545, loss = 0.33706373\n",
      "Iteration 546, loss = 0.33648006\n",
      "Iteration 547, loss = 0.33667995\n",
      "Iteration 548, loss = 0.33711158\n",
      "Iteration 549, loss = 0.33467711\n",
      "Iteration 550, loss = 0.33544425\n",
      "Iteration 551, loss = 0.33657355\n",
      "Iteration 552, loss = 0.33498039\n",
      "Iteration 553, loss = 0.33586671\n",
      "Iteration 554, loss = 0.33599668\n",
      "Iteration 555, loss = 0.33523305\n",
      "Iteration 556, loss = 0.33571405\n",
      "Iteration 557, loss = 0.33515913\n",
      "Iteration 558, loss = 0.33523929\n",
      "Iteration 559, loss = 0.33460482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [04:17<03:09, 27.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 560, loss = 0.33539259\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56021905\n",
      "Iteration 2, loss = 0.50461370\n",
      "Iteration 3, loss = 0.48157075\n",
      "Iteration 4, loss = 0.46983313\n",
      "Iteration 5, loss = 0.46361196\n",
      "Iteration 6, loss = 0.45994016\n",
      "Iteration 7, loss = 0.45756833\n",
      "Iteration 8, loss = 0.45552560\n",
      "Iteration 9, loss = 0.45395648\n",
      "Iteration 10, loss = 0.45255133\n",
      "Iteration 11, loss = 0.45179167\n",
      "Iteration 12, loss = 0.45105729\n",
      "Iteration 13, loss = 0.45018935\n",
      "Iteration 14, loss = 0.44965415\n",
      "Iteration 15, loss = 0.44931889\n",
      "Iteration 16, loss = 0.44848920\n",
      "Iteration 17, loss = 0.44817262\n",
      "Iteration 18, loss = 0.44778492\n",
      "Iteration 19, loss = 0.44735760\n",
      "Iteration 20, loss = 0.44689797\n",
      "Iteration 21, loss = 0.44636385\n",
      "Iteration 22, loss = 0.44633658\n",
      "Iteration 23, loss = 0.44566375\n",
      "Iteration 24, loss = 0.44571148\n",
      "Iteration 25, loss = 0.44516508\n",
      "Iteration 26, loss = 0.44485494\n",
      "Iteration 27, loss = 0.44433100\n",
      "Iteration 28, loss = 0.44405959\n",
      "Iteration 29, loss = 0.44400201\n",
      "Iteration 30, loss = 0.44342946\n",
      "Iteration 31, loss = 0.44315571\n",
      "Iteration 32, loss = 0.44318370\n",
      "Iteration 33, loss = 0.44260241\n",
      "Iteration 34, loss = 0.44225344\n",
      "Iteration 35, loss = 0.44245195\n",
      "Iteration 36, loss = 0.44180015\n",
      "Iteration 37, loss = 0.44166452\n",
      "Iteration 38, loss = 0.44132963\n",
      "Iteration 39, loss = 0.44107053\n",
      "Iteration 40, loss = 0.44055461\n",
      "Iteration 41, loss = 0.44027848\n",
      "Iteration 42, loss = 0.44019740\n",
      "Iteration 43, loss = 0.43985031\n",
      "Iteration 44, loss = 0.43953554\n",
      "Iteration 45, loss = 0.43955889\n",
      "Iteration 46, loss = 0.43888433\n",
      "Iteration 47, loss = 0.43859544\n",
      "Iteration 48, loss = 0.43853304\n",
      "Iteration 49, loss = 0.43804052\n",
      "Iteration 50, loss = 0.43791188\n",
      "Iteration 51, loss = 0.43749023\n",
      "Iteration 52, loss = 0.43739037\n",
      "Iteration 53, loss = 0.43710282\n",
      "Iteration 54, loss = 0.43663819\n",
      "Iteration 55, loss = 0.43637576\n",
      "Iteration 56, loss = 0.43640192\n",
      "Iteration 57, loss = 0.43589918\n",
      "Iteration 58, loss = 0.43597968\n",
      "Iteration 59, loss = 0.43555515\n",
      "Iteration 60, loss = 0.43512205\n",
      "Iteration 61, loss = 0.43486695\n",
      "Iteration 62, loss = 0.43459551\n",
      "Iteration 63, loss = 0.43436913\n",
      "Iteration 64, loss = 0.43441192\n",
      "Iteration 65, loss = 0.43411420\n",
      "Iteration 66, loss = 0.43365910\n",
      "Iteration 67, loss = 0.43344812\n",
      "Iteration 68, loss = 0.43323875\n",
      "Iteration 69, loss = 0.43309474\n",
      "Iteration 70, loss = 0.43273872\n",
      "Iteration 71, loss = 0.43244691\n",
      "Iteration 72, loss = 0.43219588\n",
      "Iteration 73, loss = 0.43224008\n",
      "Iteration 74, loss = 0.43210616\n",
      "Iteration 75, loss = 0.43153980\n",
      "Iteration 76, loss = 0.43143747\n",
      "Iteration 77, loss = 0.43105682\n",
      "Iteration 78, loss = 0.43078044\n",
      "Iteration 79, loss = 0.43091862\n",
      "Iteration 80, loss = 0.43038288\n",
      "Iteration 81, loss = 0.43012210\n",
      "Iteration 82, loss = 0.42986252\n",
      "Iteration 83, loss = 0.42992394\n",
      "Iteration 84, loss = 0.42947545\n",
      "Iteration 85, loss = 0.42925331\n",
      "Iteration 86, loss = 0.42901391\n",
      "Iteration 87, loss = 0.42869193\n",
      "Iteration 88, loss = 0.42846643\n",
      "Iteration 89, loss = 0.42772759\n",
      "Iteration 90, loss = 0.42803848\n",
      "Iteration 91, loss = 0.42777752\n",
      "Iteration 92, loss = 0.42809340\n",
      "Iteration 93, loss = 0.42739757\n",
      "Iteration 94, loss = 0.42724697\n",
      "Iteration 95, loss = 0.42684455\n",
      "Iteration 96, loss = 0.42654863\n",
      "Iteration 97, loss = 0.42667557\n",
      "Iteration 98, loss = 0.42644195\n",
      "Iteration 99, loss = 0.42586871\n",
      "Iteration 100, loss = 0.42558398\n",
      "Iteration 101, loss = 0.42559130\n",
      "Iteration 102, loss = 0.42540216\n",
      "Iteration 103, loss = 0.42533098\n",
      "Iteration 104, loss = 0.42491180\n",
      "Iteration 105, loss = 0.42485322\n",
      "Iteration 106, loss = 0.42450684\n",
      "Iteration 107, loss = 0.42417139\n",
      "Iteration 108, loss = 0.42412518\n",
      "Iteration 109, loss = 0.42366802\n",
      "Iteration 110, loss = 0.42370839\n",
      "Iteration 111, loss = 0.42345234\n",
      "Iteration 112, loss = 0.42312563\n",
      "Iteration 113, loss = 0.42270498\n",
      "Iteration 114, loss = 0.42313944\n",
      "Iteration 115, loss = 0.42242464\n",
      "Iteration 116, loss = 0.42234238\n",
      "Iteration 117, loss = 0.42225446\n",
      "Iteration 118, loss = 0.42166791\n",
      "Iteration 119, loss = 0.42157983\n",
      "Iteration 120, loss = 0.42127217\n",
      "Iteration 121, loss = 0.42105693\n",
      "Iteration 122, loss = 0.42106086\n",
      "Iteration 123, loss = 0.42121222\n",
      "Iteration 124, loss = 0.42060377\n",
      "Iteration 125, loss = 0.42093094\n",
      "Iteration 126, loss = 0.42014351\n",
      "Iteration 127, loss = 0.42002029\n",
      "Iteration 128, loss = 0.41978196\n",
      "Iteration 129, loss = 0.41982353\n",
      "Iteration 130, loss = 0.41986518\n",
      "Iteration 131, loss = 0.41905183\n",
      "Iteration 132, loss = 0.41892663\n",
      "Iteration 133, loss = 0.41885905\n",
      "Iteration 134, loss = 0.41890252\n",
      "Iteration 135, loss = 0.41933628\n",
      "Iteration 136, loss = 0.41822685\n",
      "Iteration 137, loss = 0.41800727\n",
      "Iteration 138, loss = 0.41774912\n",
      "Iteration 139, loss = 0.41757382\n",
      "Iteration 140, loss = 0.41754267\n",
      "Iteration 141, loss = 0.41715475\n",
      "Iteration 142, loss = 0.41737918\n",
      "Iteration 143, loss = 0.41689673\n",
      "Iteration 144, loss = 0.41701281\n",
      "Iteration 145, loss = 0.41679382\n",
      "Iteration 146, loss = 0.41654706\n",
      "Iteration 147, loss = 0.41622444\n",
      "Iteration 148, loss = 0.41609351\n",
      "Iteration 149, loss = 0.41599064\n",
      "Iteration 150, loss = 0.41528940\n",
      "Iteration 151, loss = 0.41549267\n",
      "Iteration 152, loss = 0.41504734\n",
      "Iteration 153, loss = 0.41480816\n",
      "Iteration 154, loss = 0.41524281\n",
      "Iteration 155, loss = 0.41433874\n",
      "Iteration 156, loss = 0.41449246\n",
      "Iteration 157, loss = 0.41477692\n",
      "Iteration 158, loss = 0.41377781\n",
      "Iteration 159, loss = 0.41389441\n",
      "Iteration 160, loss = 0.41337650\n",
      "Iteration 161, loss = 0.41335951\n",
      "Iteration 162, loss = 0.41377950\n",
      "Iteration 163, loss = 0.41277568\n",
      "Iteration 164, loss = 0.41314812\n",
      "Iteration 165, loss = 0.41241446\n",
      "Iteration 166, loss = 0.41271148\n",
      "Iteration 167, loss = 0.41222509\n",
      "Iteration 168, loss = 0.41200850\n",
      "Iteration 169, loss = 0.41209372\n",
      "Iteration 170, loss = 0.41183056\n",
      "Iteration 171, loss = 0.41152293\n",
      "Iteration 172, loss = 0.41163247\n",
      "Iteration 173, loss = 0.41088242\n",
      "Iteration 174, loss = 0.41096458\n",
      "Iteration 175, loss = 0.41080288\n",
      "Iteration 176, loss = 0.41030942\n",
      "Iteration 177, loss = 0.41014656\n",
      "Iteration 178, loss = 0.40983951\n",
      "Iteration 179, loss = 0.41023244\n",
      "Iteration 180, loss = 0.40956108\n",
      "Iteration 181, loss = 0.40931201\n",
      "Iteration 182, loss = 0.40898556\n",
      "Iteration 183, loss = 0.40916647\n",
      "Iteration 184, loss = 0.40899518\n",
      "Iteration 185, loss = 0.40825769\n",
      "Iteration 186, loss = 0.40812269\n",
      "Iteration 187, loss = 0.40825370\n",
      "Iteration 188, loss = 0.40797647\n",
      "Iteration 189, loss = 0.40748355\n",
      "Iteration 190, loss = 0.40774242\n",
      "Iteration 191, loss = 0.40735771\n",
      "Iteration 192, loss = 0.40692803\n",
      "Iteration 193, loss = 0.40686504\n",
      "Iteration 194, loss = 0.40648395\n",
      "Iteration 195, loss = 0.40663375\n",
      "Iteration 196, loss = 0.40587206\n",
      "Iteration 197, loss = 0.40609240\n",
      "Iteration 198, loss = 0.40595352\n",
      "Iteration 199, loss = 0.40521728\n",
      "Iteration 200, loss = 0.40530322\n",
      "Iteration 201, loss = 0.40481917\n",
      "Iteration 202, loss = 0.40446570\n",
      "Iteration 203, loss = 0.40451761\n",
      "Iteration 204, loss = 0.40408098\n",
      "Iteration 205, loss = 0.40437392\n",
      "Iteration 206, loss = 0.40394773\n",
      "Iteration 207, loss = 0.40351662\n",
      "Iteration 208, loss = 0.40293141\n",
      "Iteration 209, loss = 0.40336900\n",
      "Iteration 210, loss = 0.40259536\n",
      "Iteration 211, loss = 0.40282650\n",
      "Iteration 212, loss = 0.40231849\n",
      "Iteration 213, loss = 0.40205248\n",
      "Iteration 214, loss = 0.40212704\n",
      "Iteration 215, loss = 0.40242646\n",
      "Iteration 216, loss = 0.40162093\n",
      "Iteration 217, loss = 0.40137448\n",
      "Iteration 218, loss = 0.40103125\n",
      "Iteration 219, loss = 0.40096258\n",
      "Iteration 220, loss = 0.40054368\n",
      "Iteration 221, loss = 0.40019550\n",
      "Iteration 222, loss = 0.40012292\n",
      "Iteration 223, loss = 0.39985790\n",
      "Iteration 224, loss = 0.39982768\n",
      "Iteration 225, loss = 0.39912526\n",
      "Iteration 226, loss = 0.39922922\n",
      "Iteration 227, loss = 0.39895390\n",
      "Iteration 228, loss = 0.39938313\n",
      "Iteration 229, loss = 0.39856506\n",
      "Iteration 230, loss = 0.39819218\n",
      "Iteration 231, loss = 0.39843721\n",
      "Iteration 232, loss = 0.39773157\n",
      "Iteration 233, loss = 0.39794422\n",
      "Iteration 234, loss = 0.39722938\n",
      "Iteration 235, loss = 0.39750375\n",
      "Iteration 236, loss = 0.39692168\n",
      "Iteration 237, loss = 0.39664776\n",
      "Iteration 238, loss = 0.39712628\n",
      "Iteration 239, loss = 0.39697690\n",
      "Iteration 240, loss = 0.39591777\n",
      "Iteration 241, loss = 0.39580879\n",
      "Iteration 242, loss = 0.39584069\n",
      "Iteration 243, loss = 0.39528831\n",
      "Iteration 244, loss = 0.39530542\n",
      "Iteration 245, loss = 0.39543945\n",
      "Iteration 246, loss = 0.39472481\n",
      "Iteration 247, loss = 0.39480616\n",
      "Iteration 248, loss = 0.39471575\n",
      "Iteration 249, loss = 0.39393174\n",
      "Iteration 250, loss = 0.39406590\n",
      "Iteration 251, loss = 0.39314635\n",
      "Iteration 252, loss = 0.39330740\n",
      "Iteration 253, loss = 0.39308930\n",
      "Iteration 254, loss = 0.39276191\n",
      "Iteration 255, loss = 0.39245622\n",
      "Iteration 256, loss = 0.39218928\n",
      "Iteration 257, loss = 0.39242165\n",
      "Iteration 258, loss = 0.39182453\n",
      "Iteration 259, loss = 0.39193936\n",
      "Iteration 260, loss = 0.39153939\n",
      "Iteration 261, loss = 0.39064261\n",
      "Iteration 262, loss = 0.39072074\n",
      "Iteration 263, loss = 0.39062642\n",
      "Iteration 264, loss = 0.39021283\n",
      "Iteration 265, loss = 0.38929186\n",
      "Iteration 266, loss = 0.39022328\n",
      "Iteration 267, loss = 0.39003527\n",
      "Iteration 268, loss = 0.38974551\n",
      "Iteration 269, loss = 0.38916065\n",
      "Iteration 270, loss = 0.38865236\n",
      "Iteration 271, loss = 0.38899389\n",
      "Iteration 272, loss = 0.38859348\n",
      "Iteration 273, loss = 0.38863646\n",
      "Iteration 274, loss = 0.38808529\n",
      "Iteration 275, loss = 0.38783478\n",
      "Iteration 276, loss = 0.38776557\n",
      "Iteration 277, loss = 0.38786050\n",
      "Iteration 278, loss = 0.38684279\n",
      "Iteration 279, loss = 0.38709267\n",
      "Iteration 280, loss = 0.38626678\n",
      "Iteration 281, loss = 0.38596960\n",
      "Iteration 282, loss = 0.38602670\n",
      "Iteration 283, loss = 0.38581535\n",
      "Iteration 284, loss = 0.38592397\n",
      "Iteration 285, loss = 0.38633502\n",
      "Iteration 286, loss = 0.38528759\n",
      "Iteration 287, loss = 0.38507321\n",
      "Iteration 288, loss = 0.38449192\n",
      "Iteration 289, loss = 0.38447206\n",
      "Iteration 290, loss = 0.38454619\n",
      "Iteration 291, loss = 0.38416374\n",
      "Iteration 292, loss = 0.38407772\n",
      "Iteration 293, loss = 0.38404204\n",
      "Iteration 294, loss = 0.38306373\n",
      "Iteration 295, loss = 0.38392048\n",
      "Iteration 296, loss = 0.38299040\n",
      "Iteration 297, loss = 0.38285470\n",
      "Iteration 298, loss = 0.38315359\n",
      "Iteration 299, loss = 0.38297628\n",
      "Iteration 300, loss = 0.38221911\n",
      "Iteration 301, loss = 0.38189203\n",
      "Iteration 302, loss = 0.38169909\n",
      "Iteration 303, loss = 0.38101895\n",
      "Iteration 304, loss = 0.38119462\n",
      "Iteration 305, loss = 0.38132132\n",
      "Iteration 306, loss = 0.38054405\n",
      "Iteration 307, loss = 0.38077864\n",
      "Iteration 308, loss = 0.38055225\n",
      "Iteration 309, loss = 0.37977063\n",
      "Iteration 310, loss = 0.38035736\n",
      "Iteration 311, loss = 0.37947154\n",
      "Iteration 312, loss = 0.37999159\n",
      "Iteration 313, loss = 0.37884592\n",
      "Iteration 314, loss = 0.37968826\n",
      "Iteration 315, loss = 0.37851961\n",
      "Iteration 316, loss = 0.37811500\n",
      "Iteration 317, loss = 0.37926719\n",
      "Iteration 318, loss = 0.37851053\n",
      "Iteration 319, loss = 0.37768079\n",
      "Iteration 320, loss = 0.37748444\n",
      "Iteration 321, loss = 0.37787725\n",
      "Iteration 322, loss = 0.37705318\n",
      "Iteration 323, loss = 0.37794635\n",
      "Iteration 324, loss = 0.37760195\n",
      "Iteration 325, loss = 0.37677614\n",
      "Iteration 326, loss = 0.37654023\n",
      "Iteration 327, loss = 0.37693750\n",
      "Iteration 328, loss = 0.37610751\n",
      "Iteration 329, loss = 0.37533378\n",
      "Iteration 330, loss = 0.37573539\n",
      "Iteration 331, loss = 0.37519992\n",
      "Iteration 332, loss = 0.37543697\n",
      "Iteration 333, loss = 0.37619387\n",
      "Iteration 334, loss = 0.37471910\n",
      "Iteration 335, loss = 0.37381885\n",
      "Iteration 336, loss = 0.37481561\n",
      "Iteration 337, loss = 0.37445473\n",
      "Iteration 338, loss = 0.37410027\n",
      "Iteration 339, loss = 0.37346078\n",
      "Iteration 340, loss = 0.37483905\n",
      "Iteration 341, loss = 0.37306824\n",
      "Iteration 342, loss = 0.37337761\n",
      "Iteration 343, loss = 0.37290024\n",
      "Iteration 344, loss = 0.37270031\n",
      "Iteration 345, loss = 0.37272246\n",
      "Iteration 346, loss = 0.37179363\n",
      "Iteration 347, loss = 0.37285055\n",
      "Iteration 348, loss = 0.37151931\n",
      "Iteration 349, loss = 0.37132481\n",
      "Iteration 350, loss = 0.37203161\n",
      "Iteration 351, loss = 0.37098685\n",
      "Iteration 352, loss = 0.37117213\n",
      "Iteration 353, loss = 0.37127670\n",
      "Iteration 354, loss = 0.37058839\n",
      "Iteration 355, loss = 0.37050468\n",
      "Iteration 356, loss = 0.36972927\n",
      "Iteration 357, loss = 0.36982641\n",
      "Iteration 358, loss = 0.37046036\n",
      "Iteration 359, loss = 0.37035673\n",
      "Iteration 360, loss = 0.36935250\n",
      "Iteration 361, loss = 0.36913392\n",
      "Iteration 362, loss = 0.36837372\n",
      "Iteration 363, loss = 0.36890427\n",
      "Iteration 364, loss = 0.36943236\n",
      "Iteration 365, loss = 0.36872292\n",
      "Iteration 366, loss = 0.36853391\n",
      "Iteration 367, loss = 0.36805379\n",
      "Iteration 368, loss = 0.36754443\n",
      "Iteration 369, loss = 0.36789901\n",
      "Iteration 370, loss = 0.36761260\n",
      "Iteration 371, loss = 0.36750231\n",
      "Iteration 372, loss = 0.36701969\n",
      "Iteration 373, loss = 0.36661326\n",
      "Iteration 374, loss = 0.36625691\n",
      "Iteration 375, loss = 0.36740724\n",
      "Iteration 376, loss = 0.36645224\n",
      "Iteration 377, loss = 0.36642118\n",
      "Iteration 378, loss = 0.36623803\n",
      "Iteration 379, loss = 0.36630150\n",
      "Iteration 380, loss = 0.36537980\n",
      "Iteration 381, loss = 0.36587780\n",
      "Iteration 382, loss = 0.36646236\n",
      "Iteration 383, loss = 0.36541542\n",
      "Iteration 384, loss = 0.36510184\n",
      "Iteration 385, loss = 0.36497212\n",
      "Iteration 386, loss = 0.36520820\n",
      "Iteration 387, loss = 0.36436670\n",
      "Iteration 388, loss = 0.36469836\n",
      "Iteration 389, loss = 0.36390094\n",
      "Iteration 390, loss = 0.36427641\n",
      "Iteration 391, loss = 0.36401545\n",
      "Iteration 392, loss = 0.36384129\n",
      "Iteration 393, loss = 0.36361344\n",
      "Iteration 394, loss = 0.36302562\n",
      "Iteration 395, loss = 0.36341702\n",
      "Iteration 396, loss = 0.36206937\n",
      "Iteration 397, loss = 0.36284585\n",
      "Iteration 398, loss = 0.36267105\n",
      "Iteration 399, loss = 0.36240099\n",
      "Iteration 400, loss = 0.36242438\n",
      "Iteration 401, loss = 0.36142510\n",
      "Iteration 402, loss = 0.36131333\n",
      "Iteration 403, loss = 0.36141604\n",
      "Iteration 404, loss = 0.36174250\n",
      "Iteration 405, loss = 0.36054977\n",
      "Iteration 406, loss = 0.36095678\n",
      "Iteration 407, loss = 0.36124638\n",
      "Iteration 408, loss = 0.36073444\n",
      "Iteration 409, loss = 0.36007936\n",
      "Iteration 410, loss = 0.35979183\n",
      "Iteration 411, loss = 0.36059360\n",
      "Iteration 412, loss = 0.35882947\n",
      "Iteration 413, loss = 0.35979539\n",
      "Iteration 414, loss = 0.35916837\n",
      "Iteration 415, loss = 0.35886488\n",
      "Iteration 416, loss = 0.35956881\n",
      "Iteration 417, loss = 0.35820379\n",
      "Iteration 418, loss = 0.35843464\n",
      "Iteration 419, loss = 0.35875032\n",
      "Iteration 420, loss = 0.35877475\n",
      "Iteration 421, loss = 0.35800710\n",
      "Iteration 422, loss = 0.35862394\n",
      "Iteration 423, loss = 0.35762325\n",
      "Iteration 424, loss = 0.35721940\n",
      "Iteration 425, loss = 0.35742728\n",
      "Iteration 426, loss = 0.35714057\n",
      "Iteration 427, loss = 0.35708958\n",
      "Iteration 428, loss = 0.35616719\n",
      "Iteration 429, loss = 0.35763211\n",
      "Iteration 430, loss = 0.35730658\n",
      "Iteration 431, loss = 0.35694785\n",
      "Iteration 432, loss = 0.35647543\n",
      "Iteration 433, loss = 0.35619652\n",
      "Iteration 434, loss = 0.35514010\n",
      "Iteration 435, loss = 0.35555644\n",
      "Iteration 436, loss = 0.35593549\n",
      "Iteration 437, loss = 0.35558199\n",
      "Iteration 438, loss = 0.35508295\n",
      "Iteration 439, loss = 0.35526778\n",
      "Iteration 440, loss = 0.35468954\n",
      "Iteration 441, loss = 0.35481340\n",
      "Iteration 442, loss = 0.35364102\n",
      "Iteration 443, loss = 0.35372108\n",
      "Iteration 444, loss = 0.35354401\n",
      "Iteration 445, loss = 0.35397313\n",
      "Iteration 446, loss = 0.35441336\n",
      "Iteration 447, loss = 0.35397834\n",
      "Iteration 448, loss = 0.35265619\n",
      "Iteration 449, loss = 0.35303518\n",
      "Iteration 450, loss = 0.35294545\n",
      "Iteration 451, loss = 0.35382069\n",
      "Iteration 452, loss = 0.35239303\n",
      "Iteration 453, loss = 0.35312792\n",
      "Iteration 454, loss = 0.35182912\n",
      "Iteration 455, loss = 0.35241562\n",
      "Iteration 456, loss = 0.35188224\n",
      "Iteration 457, loss = 0.35253896\n",
      "Iteration 458, loss = 0.35175865\n",
      "Iteration 459, loss = 0.35181370\n",
      "Iteration 460, loss = 0.35130978\n",
      "Iteration 461, loss = 0.35121716\n",
      "Iteration 462, loss = 0.35211437\n",
      "Iteration 463, loss = 0.35094085\n",
      "Iteration 464, loss = 0.35135582\n",
      "Iteration 465, loss = 0.35086123\n",
      "Iteration 466, loss = 0.35032294\n",
      "Iteration 467, loss = 0.35120084\n",
      "Iteration 468, loss = 0.35045265\n",
      "Iteration 469, loss = 0.35093765\n",
      "Iteration 470, loss = 0.35007390\n",
      "Iteration 471, loss = 0.35041119\n",
      "Iteration 472, loss = 0.34905188\n",
      "Iteration 473, loss = 0.34976641\n",
      "Iteration 474, loss = 0.34867334\n",
      "Iteration 475, loss = 0.34977653\n",
      "Iteration 476, loss = 0.34958885\n",
      "Iteration 477, loss = 0.34922182\n",
      "Iteration 478, loss = 0.34930183\n",
      "Iteration 479, loss = 0.34844832\n",
      "Iteration 480, loss = 0.34875037\n",
      "Iteration 481, loss = 0.34854325\n",
      "Iteration 482, loss = 0.34853564\n",
      "Iteration 483, loss = 0.34806926\n",
      "Iteration 484, loss = 0.34798168\n",
      "Iteration 485, loss = 0.34841302\n",
      "Iteration 486, loss = 0.34870788\n",
      "Iteration 487, loss = 0.34683983\n",
      "Iteration 488, loss = 0.34777767\n",
      "Iteration 489, loss = 0.34794307\n",
      "Iteration 490, loss = 0.34722952\n",
      "Iteration 491, loss = 0.34743838\n",
      "Iteration 492, loss = 0.34628611\n",
      "Iteration 493, loss = 0.34638132\n",
      "Iteration 494, loss = 0.34650350\n",
      "Iteration 495, loss = 0.34615462\n",
      "Iteration 496, loss = 0.34677314\n",
      "Iteration 497, loss = 0.34608864\n",
      "Iteration 498, loss = 0.34541583\n",
      "Iteration 499, loss = 0.34602879\n",
      "Iteration 500, loss = 0.34586199\n",
      "Iteration 501, loss = 0.34447668\n",
      "Iteration 502, loss = 0.34602037\n",
      "Iteration 503, loss = 0.34573640\n",
      "Iteration 504, loss = 0.34544110\n",
      "Iteration 505, loss = 0.34495243\n",
      "Iteration 506, loss = 0.34465954\n",
      "Iteration 507, loss = 0.34401234\n",
      "Iteration 508, loss = 0.34438666\n",
      "Iteration 509, loss = 0.34464917\n",
      "Iteration 510, loss = 0.34418135\n",
      "Iteration 511, loss = 0.34552877\n",
      "Iteration 512, loss = 0.34396763\n",
      "Iteration 513, loss = 0.34395014\n",
      "Iteration 514, loss = 0.34313131\n",
      "Iteration 515, loss = 0.34344357\n",
      "Iteration 516, loss = 0.34273352\n",
      "Iteration 517, loss = 0.34279639\n",
      "Iteration 518, loss = 0.34306417\n",
      "Iteration 519, loss = 0.34312846\n",
      "Iteration 520, loss = 0.34246927\n",
      "Iteration 521, loss = 0.34301505\n",
      "Iteration 522, loss = 0.34335048\n",
      "Iteration 523, loss = 0.34326576\n",
      "Iteration 524, loss = 0.34304117\n",
      "Iteration 525, loss = 0.34239426\n",
      "Iteration 526, loss = 0.34199302\n",
      "Iteration 527, loss = 0.34128193\n",
      "Iteration 528, loss = 0.34168520\n",
      "Iteration 529, loss = 0.34193095\n",
      "Iteration 530, loss = 0.34174079\n",
      "Iteration 531, loss = 0.34144161\n",
      "Iteration 532, loss = 0.34105670\n",
      "Iteration 533, loss = 0.34169037\n",
      "Iteration 534, loss = 0.34118719\n",
      "Iteration 535, loss = 0.34137132\n",
      "Iteration 536, loss = 0.34036346\n",
      "Iteration 537, loss = 0.34077859\n",
      "Iteration 538, loss = 0.34087896\n",
      "Iteration 539, loss = 0.34135834\n",
      "Iteration 540, loss = 0.34090033\n",
      "Iteration 541, loss = 0.33959713\n",
      "Iteration 542, loss = 0.33980247\n",
      "Iteration 543, loss = 0.34072660\n",
      "Iteration 544, loss = 0.34022234\n",
      "Iteration 545, loss = 0.34084801\n",
      "Iteration 546, loss = 0.33992471\n",
      "Iteration 547, loss = 0.33991305\n",
      "Iteration 548, loss = 0.34009139\n",
      "Iteration 549, loss = 0.33890758\n",
      "Iteration 550, loss = 0.33988090\n",
      "Iteration 551, loss = 0.33870158\n",
      "Iteration 552, loss = 0.33970498\n",
      "Iteration 553, loss = 0.33857365\n",
      "Iteration 554, loss = 0.33889057\n",
      "Iteration 555, loss = 0.33855868\n",
      "Iteration 556, loss = 0.33910377\n",
      "Iteration 557, loss = 0.33822758\n",
      "Iteration 558, loss = 0.33795646\n",
      "Iteration 559, loss = 0.33805255\n",
      "Iteration 560, loss = 0.33693439\n",
      "Iteration 561, loss = 0.33804385\n",
      "Iteration 562, loss = 0.33717344\n",
      "Iteration 563, loss = 0.33788386\n",
      "Iteration 564, loss = 0.33820807\n",
      "Iteration 565, loss = 0.33640902\n",
      "Iteration 566, loss = 0.33735660\n",
      "Iteration 567, loss = 0.33747685\n",
      "Iteration 568, loss = 0.33727819\n",
      "Iteration 569, loss = 0.33695345\n",
      "Iteration 570, loss = 0.33626853\n",
      "Iteration 571, loss = 0.33683745\n",
      "Iteration 572, loss = 0.33735720\n",
      "Iteration 573, loss = 0.33584984\n",
      "Iteration 574, loss = 0.33687127\n",
      "Iteration 575, loss = 0.33589836\n",
      "Iteration 576, loss = 0.33631214\n",
      "Iteration 577, loss = 0.33586234\n",
      "Iteration 578, loss = 0.33676803\n",
      "Iteration 579, loss = 0.33582775\n",
      "Iteration 580, loss = 0.33533390\n",
      "Iteration 581, loss = 0.33572401\n",
      "Iteration 582, loss = 0.33547267\n",
      "Iteration 583, loss = 0.33589125\n",
      "Iteration 584, loss = 0.33466180\n",
      "Iteration 585, loss = 0.33525229\n",
      "Iteration 586, loss = 0.33469028\n",
      "Iteration 587, loss = 0.33479158\n",
      "Iteration 588, loss = 0.33563423\n",
      "Iteration 589, loss = 0.33469774\n",
      "Iteration 590, loss = 0.33408335\n",
      "Iteration 591, loss = 0.33496370\n",
      "Iteration 592, loss = 0.33486430\n",
      "Iteration 593, loss = 0.33508327\n",
      "Iteration 594, loss = 0.33453369\n",
      "Iteration 595, loss = 0.33379546\n",
      "Iteration 596, loss = 0.33462243\n",
      "Iteration 597, loss = 0.33435066\n",
      "Iteration 598, loss = 0.33306101\n",
      "Iteration 599, loss = 0.33489388\n",
      "Iteration 600, loss = 0.33317924\n",
      "Iteration 601, loss = 0.33338950\n",
      "Iteration 602, loss = 0.33311443\n",
      "Iteration 603, loss = 0.33411787\n",
      "Iteration 604, loss = 0.33221423\n",
      "Iteration 605, loss = 0.33370594\n",
      "Iteration 606, loss = 0.33355394\n",
      "Iteration 607, loss = 0.33297788\n",
      "Iteration 608, loss = 0.33296388\n",
      "Iteration 609, loss = 0.33249129\n",
      "Iteration 610, loss = 0.33300155\n",
      "Iteration 611, loss = 0.33241425\n",
      "Iteration 612, loss = 0.33265434\n",
      "Iteration 613, loss = 0.33260492\n",
      "Iteration 614, loss = 0.33302463\n",
      "Iteration 615, loss = 0.33159116\n",
      "Iteration 616, loss = 0.33288864\n",
      "Iteration 617, loss = 0.33207947\n",
      "Iteration 618, loss = 0.33237974\n",
      "Iteration 619, loss = 0.33116775\n",
      "Iteration 620, loss = 0.33279911\n",
      "Iteration 621, loss = 0.33113968\n",
      "Iteration 622, loss = 0.33182244\n",
      "Iteration 623, loss = 0.33041344\n",
      "Iteration 624, loss = 0.33249149\n",
      "Iteration 625, loss = 0.33074041\n",
      "Iteration 626, loss = 0.33141185\n",
      "Iteration 627, loss = 0.33091917\n",
      "Iteration 628, loss = 0.33085295\n",
      "Iteration 629, loss = 0.32962676\n",
      "Iteration 630, loss = 0.33014095\n",
      "Iteration 631, loss = 0.32975427\n",
      "Iteration 632, loss = 0.33031306\n",
      "Iteration 633, loss = 0.33024756\n",
      "Iteration 634, loss = 0.33107740\n",
      "Iteration 635, loss = 0.33042287\n",
      "Iteration 636, loss = 0.32860609\n",
      "Iteration 637, loss = 0.33001149\n",
      "Iteration 638, loss = 0.32947285\n",
      "Iteration 639, loss = 0.33012398\n",
      "Iteration 640, loss = 0.32878505\n",
      "Iteration 641, loss = 0.32916889\n",
      "Iteration 642, loss = 0.32808606\n",
      "Iteration 643, loss = 0.32979561\n",
      "Iteration 644, loss = 0.32953390\n",
      "Iteration 645, loss = 0.32949615\n",
      "Iteration 646, loss = 0.32832981\n",
      "Iteration 647, loss = 0.32832842\n",
      "Iteration 648, loss = 0.32886066\n",
      "Iteration 649, loss = 0.32944687\n",
      "Iteration 650, loss = 0.32858035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [04:49<02:53, 28.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 651, loss = 0.32831911\n",
      "Iteration 652, loss = 0.32841924\n",
      "Iteration 653, loss = 0.32828534\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.52579404\n",
      "Iteration 2, loss = 0.48507113\n",
      "Iteration 3, loss = 0.47102931\n",
      "Iteration 4, loss = 0.46520817\n",
      "Iteration 5, loss = 0.46187033\n",
      "Iteration 6, loss = 0.45924642\n",
      "Iteration 7, loss = 0.45753901\n",
      "Iteration 8, loss = 0.45601011\n",
      "Iteration 9, loss = 0.45485678\n",
      "Iteration 10, loss = 0.45358427\n",
      "Iteration 11, loss = 0.45252224\n",
      "Iteration 12, loss = 0.45144067\n",
      "Iteration 13, loss = 0.45036962\n",
      "Iteration 14, loss = 0.44994529\n",
      "Iteration 15, loss = 0.44908362\n",
      "Iteration 16, loss = 0.44827224\n",
      "Iteration 17, loss = 0.44784906\n",
      "Iteration 18, loss = 0.44703026\n",
      "Iteration 19, loss = 0.44678813\n",
      "Iteration 20, loss = 0.44608753\n",
      "Iteration 21, loss = 0.44561452\n",
      "Iteration 22, loss = 0.44518375\n",
      "Iteration 23, loss = 0.44459043\n",
      "Iteration 24, loss = 0.44422435\n",
      "Iteration 25, loss = 0.44392031\n",
      "Iteration 26, loss = 0.44328733\n",
      "Iteration 27, loss = 0.44312338\n",
      "Iteration 28, loss = 0.44253581\n",
      "Iteration 29, loss = 0.44199134\n",
      "Iteration 30, loss = 0.44172447\n",
      "Iteration 31, loss = 0.44129962\n",
      "Iteration 32, loss = 0.44094904\n",
      "Iteration 33, loss = 0.44063981\n",
      "Iteration 34, loss = 0.44018799\n",
      "Iteration 35, loss = 0.44014678\n",
      "Iteration 36, loss = 0.43943601\n",
      "Iteration 37, loss = 0.43915890\n",
      "Iteration 38, loss = 0.43881324\n",
      "Iteration 39, loss = 0.43864200\n",
      "Iteration 40, loss = 0.43823146\n",
      "Iteration 41, loss = 0.43773488\n",
      "Iteration 42, loss = 0.43750028\n",
      "Iteration 43, loss = 0.43725655\n",
      "Iteration 44, loss = 0.43699483\n",
      "Iteration 45, loss = 0.43662065\n",
      "Iteration 46, loss = 0.43577752\n",
      "Iteration 47, loss = 0.43599143\n",
      "Iteration 48, loss = 0.43569698\n",
      "Iteration 49, loss = 0.43517868\n",
      "Iteration 50, loss = 0.43472987\n",
      "Iteration 51, loss = 0.43449671\n",
      "Iteration 52, loss = 0.43418365\n",
      "Iteration 53, loss = 0.43395658\n",
      "Iteration 54, loss = 0.43336891\n",
      "Iteration 55, loss = 0.43354200\n",
      "Iteration 56, loss = 0.43287804\n",
      "Iteration 57, loss = 0.43255158\n",
      "Iteration 58, loss = 0.43207260\n",
      "Iteration 59, loss = 0.43166274\n",
      "Iteration 60, loss = 0.43162545\n",
      "Iteration 61, loss = 0.43103903\n",
      "Iteration 62, loss = 0.43085914\n",
      "Iteration 63, loss = 0.43089043\n",
      "Iteration 64, loss = 0.43014355\n",
      "Iteration 65, loss = 0.42991632\n",
      "Iteration 66, loss = 0.42957706\n",
      "Iteration 67, loss = 0.42933853\n",
      "Iteration 68, loss = 0.42891176\n",
      "Iteration 69, loss = 0.42912123\n",
      "Iteration 70, loss = 0.42859442\n",
      "Iteration 71, loss = 0.42849290\n",
      "Iteration 72, loss = 0.42811506\n",
      "Iteration 73, loss = 0.42772077\n",
      "Iteration 74, loss = 0.42729528\n",
      "Iteration 75, loss = 0.42751027\n",
      "Iteration 76, loss = 0.42694803\n",
      "Iteration 77, loss = 0.42639405\n",
      "Iteration 78, loss = 0.42603387\n",
      "Iteration 79, loss = 0.42595897\n",
      "Iteration 80, loss = 0.42545605\n",
      "Iteration 81, loss = 0.42526608\n",
      "Iteration 82, loss = 0.42527007\n",
      "Iteration 83, loss = 0.42467659\n",
      "Iteration 84, loss = 0.42456010\n",
      "Iteration 85, loss = 0.42473151\n",
      "Iteration 86, loss = 0.42407866\n",
      "Iteration 87, loss = 0.42381824\n",
      "Iteration 88, loss = 0.42367063\n",
      "Iteration 89, loss = 0.42307020\n",
      "Iteration 90, loss = 0.42296309\n",
      "Iteration 91, loss = 0.42283600\n",
      "Iteration 92, loss = 0.42264291\n",
      "Iteration 93, loss = 0.42228089\n",
      "Iteration 94, loss = 0.42157590\n",
      "Iteration 95, loss = 0.42147748\n",
      "Iteration 96, loss = 0.42130796\n",
      "Iteration 97, loss = 0.42152038\n",
      "Iteration 98, loss = 0.42063304\n",
      "Iteration 99, loss = 0.42104487\n",
      "Iteration 100, loss = 0.42047004\n",
      "Iteration 101, loss = 0.42023825\n",
      "Iteration 102, loss = 0.41998293\n",
      "Iteration 103, loss = 0.41962685\n",
      "Iteration 104, loss = 0.41955924\n",
      "Iteration 105, loss = 0.41893288\n",
      "Iteration 106, loss = 0.41924912\n",
      "Iteration 107, loss = 0.41843713\n",
      "Iteration 108, loss = 0.41825072\n",
      "Iteration 109, loss = 0.41784030\n",
      "Iteration 110, loss = 0.41750817\n",
      "Iteration 111, loss = 0.41722541\n",
      "Iteration 112, loss = 0.41723216\n",
      "Iteration 113, loss = 0.41708444\n",
      "Iteration 114, loss = 0.41665049\n",
      "Iteration 115, loss = 0.41636682\n",
      "Iteration 116, loss = 0.41603845\n",
      "Iteration 117, loss = 0.41622813\n",
      "Iteration 118, loss = 0.41591180\n",
      "Iteration 119, loss = 0.41544186\n",
      "Iteration 120, loss = 0.41520439\n",
      "Iteration 121, loss = 0.41470852\n",
      "Iteration 122, loss = 0.41469275\n",
      "Iteration 123, loss = 0.41440040\n",
      "Iteration 124, loss = 0.41410058\n",
      "Iteration 125, loss = 0.41363640\n",
      "Iteration 126, loss = 0.41344759\n",
      "Iteration 127, loss = 0.41318875\n",
      "Iteration 128, loss = 0.41304980\n",
      "Iteration 129, loss = 0.41313775\n",
      "Iteration 130, loss = 0.41313678\n",
      "Iteration 131, loss = 0.41251608\n",
      "Iteration 132, loss = 0.41199317\n",
      "Iteration 133, loss = 0.41163680\n",
      "Iteration 134, loss = 0.41147109\n",
      "Iteration 135, loss = 0.41167283\n",
      "Iteration 136, loss = 0.41074463\n",
      "Iteration 137, loss = 0.41096224\n",
      "Iteration 138, loss = 0.41041277\n",
      "Iteration 139, loss = 0.41056202\n",
      "Iteration 140, loss = 0.40986275\n",
      "Iteration 141, loss = 0.41031752\n",
      "Iteration 142, loss = 0.40929680\n",
      "Iteration 143, loss = 0.40944687\n",
      "Iteration 144, loss = 0.40893428\n",
      "Iteration 145, loss = 0.40853076\n",
      "Iteration 146, loss = 0.40844899\n",
      "Iteration 147, loss = 0.40837491\n",
      "Iteration 148, loss = 0.40795958\n",
      "Iteration 149, loss = 0.40739350\n",
      "Iteration 150, loss = 0.40730089\n",
      "Iteration 151, loss = 0.40766021\n",
      "Iteration 152, loss = 0.40683899\n",
      "Iteration 153, loss = 0.40668158\n",
      "Iteration 154, loss = 0.40631848\n",
      "Iteration 155, loss = 0.40627072\n",
      "Iteration 156, loss = 0.40625002\n",
      "Iteration 157, loss = 0.40606403\n",
      "Iteration 158, loss = 0.40575912\n",
      "Iteration 159, loss = 0.40496943\n",
      "Iteration 160, loss = 0.40553809\n",
      "Iteration 161, loss = 0.40506413\n",
      "Iteration 162, loss = 0.40432061\n",
      "Iteration 163, loss = 0.40399954\n",
      "Iteration 164, loss = 0.40392108\n",
      "Iteration 165, loss = 0.40393793\n",
      "Iteration 166, loss = 0.40319126\n",
      "Iteration 167, loss = 0.40314638\n",
      "Iteration 168, loss = 0.40286532\n",
      "Iteration 169, loss = 0.40277132\n",
      "Iteration 170, loss = 0.40255950\n",
      "Iteration 171, loss = 0.40192035\n",
      "Iteration 172, loss = 0.40215292\n",
      "Iteration 173, loss = 0.40196514\n",
      "Iteration 174, loss = 0.40101845\n",
      "Iteration 175, loss = 0.40108167\n",
      "Iteration 176, loss = 0.40127315\n",
      "Iteration 177, loss = 0.40028020\n",
      "Iteration 178, loss = 0.40084321\n",
      "Iteration 179, loss = 0.39996311\n",
      "Iteration 180, loss = 0.39961865\n",
      "Iteration 181, loss = 0.39913513\n",
      "Iteration 182, loss = 0.39894845\n",
      "Iteration 183, loss = 0.39925976\n",
      "Iteration 184, loss = 0.39915778\n",
      "Iteration 185, loss = 0.39799089\n",
      "Iteration 186, loss = 0.39807076\n",
      "Iteration 187, loss = 0.39831144\n",
      "Iteration 188, loss = 0.39793509\n",
      "Iteration 189, loss = 0.39736801\n",
      "Iteration 190, loss = 0.39787055\n",
      "Iteration 191, loss = 0.39672881\n",
      "Iteration 192, loss = 0.39687086\n",
      "Iteration 193, loss = 0.39756134\n",
      "Iteration 194, loss = 0.39588176\n",
      "Iteration 195, loss = 0.39616646\n",
      "Iteration 196, loss = 0.39524866\n",
      "Iteration 197, loss = 0.39583178\n",
      "Iteration 198, loss = 0.39496854\n",
      "Iteration 199, loss = 0.39530798\n",
      "Iteration 200, loss = 0.39492471\n",
      "Iteration 201, loss = 0.39449462\n",
      "Iteration 202, loss = 0.39460952\n",
      "Iteration 203, loss = 0.39431273\n",
      "Iteration 204, loss = 0.39376775\n",
      "Iteration 205, loss = 0.39426813\n",
      "Iteration 206, loss = 0.39370177\n",
      "Iteration 207, loss = 0.39326477\n",
      "Iteration 208, loss = 0.39284686\n",
      "Iteration 209, loss = 0.39270361\n",
      "Iteration 210, loss = 0.39219100\n",
      "Iteration 211, loss = 0.39215848\n",
      "Iteration 212, loss = 0.39180101\n",
      "Iteration 213, loss = 0.39133083\n",
      "Iteration 214, loss = 0.39205876\n",
      "Iteration 215, loss = 0.39138679\n",
      "Iteration 216, loss = 0.39094317\n",
      "Iteration 217, loss = 0.38998262\n",
      "Iteration 218, loss = 0.39031860\n",
      "Iteration 219, loss = 0.39017734\n",
      "Iteration 220, loss = 0.38961517\n",
      "Iteration 221, loss = 0.38957728\n",
      "Iteration 222, loss = 0.38940027\n",
      "Iteration 223, loss = 0.39000526\n",
      "Iteration 224, loss = 0.38903062\n",
      "Iteration 225, loss = 0.38896480\n",
      "Iteration 226, loss = 0.38866712\n",
      "Iteration 227, loss = 0.38826606\n",
      "Iteration 228, loss = 0.38796384\n",
      "Iteration 229, loss = 0.38785071\n",
      "Iteration 230, loss = 0.38817940\n",
      "Iteration 231, loss = 0.38795558\n",
      "Iteration 232, loss = 0.38677831\n",
      "Iteration 233, loss = 0.38748798\n",
      "Iteration 234, loss = 0.38607209\n",
      "Iteration 235, loss = 0.38648644\n",
      "Iteration 236, loss = 0.38620894\n",
      "Iteration 237, loss = 0.38602658\n",
      "Iteration 238, loss = 0.38568351\n",
      "Iteration 239, loss = 0.38618644\n",
      "Iteration 240, loss = 0.38491323\n",
      "Iteration 241, loss = 0.38518423\n",
      "Iteration 242, loss = 0.38527445\n",
      "Iteration 243, loss = 0.38454367\n",
      "Iteration 244, loss = 0.38404448\n",
      "Iteration 245, loss = 0.38383647\n",
      "Iteration 246, loss = 0.38341948\n",
      "Iteration 247, loss = 0.38362322\n",
      "Iteration 248, loss = 0.38417989\n",
      "Iteration 249, loss = 0.38355243\n",
      "Iteration 250, loss = 0.38263051\n",
      "Iteration 251, loss = 0.38250264\n",
      "Iteration 252, loss = 0.38189495\n",
      "Iteration 253, loss = 0.38211241\n",
      "Iteration 254, loss = 0.38230495\n",
      "Iteration 255, loss = 0.38129692\n",
      "Iteration 256, loss = 0.38187277\n",
      "Iteration 257, loss = 0.38175900\n",
      "Iteration 258, loss = 0.38131724\n",
      "Iteration 259, loss = 0.38123822\n",
      "Iteration 260, loss = 0.38011125\n",
      "Iteration 261, loss = 0.38032640\n",
      "Iteration 262, loss = 0.38023658\n",
      "Iteration 263, loss = 0.37982936\n",
      "Iteration 264, loss = 0.37937477\n",
      "Iteration 265, loss = 0.38018447\n",
      "Iteration 266, loss = 0.37849698\n",
      "Iteration 267, loss = 0.37936102\n",
      "Iteration 268, loss = 0.37948953\n",
      "Iteration 269, loss = 0.37843305\n",
      "Iteration 270, loss = 0.37916052\n",
      "Iteration 271, loss = 0.37783181\n",
      "Iteration 272, loss = 0.37754489\n",
      "Iteration 273, loss = 0.37808333\n",
      "Iteration 274, loss = 0.37733744\n",
      "Iteration 275, loss = 0.37762472\n",
      "Iteration 276, loss = 0.37765017\n",
      "Iteration 277, loss = 0.37703321\n",
      "Iteration 278, loss = 0.37734348\n",
      "Iteration 279, loss = 0.37593185\n",
      "Iteration 280, loss = 0.37642428\n",
      "Iteration 281, loss = 0.37587823\n",
      "Iteration 282, loss = 0.37582701\n",
      "Iteration 283, loss = 0.37513963\n",
      "Iteration 284, loss = 0.37622579\n",
      "Iteration 285, loss = 0.37468476\n",
      "Iteration 286, loss = 0.37496153\n",
      "Iteration 287, loss = 0.37484821\n",
      "Iteration 288, loss = 0.37479106\n",
      "Iteration 289, loss = 0.37396728\n",
      "Iteration 290, loss = 0.37418372\n",
      "Iteration 291, loss = 0.37389683\n",
      "Iteration 292, loss = 0.37335627\n",
      "Iteration 293, loss = 0.37328144\n",
      "Iteration 294, loss = 0.37312836\n",
      "Iteration 295, loss = 0.37202712\n",
      "Iteration 296, loss = 0.37348256\n",
      "Iteration 297, loss = 0.37231059\n",
      "Iteration 298, loss = 0.37207624\n",
      "Iteration 299, loss = 0.37188006\n",
      "Iteration 300, loss = 0.37256463\n",
      "Iteration 301, loss = 0.37243646\n",
      "Iteration 302, loss = 0.37184077\n",
      "Iteration 303, loss = 0.37116666\n",
      "Iteration 304, loss = 0.37032375\n",
      "Iteration 305, loss = 0.37071233\n",
      "Iteration 306, loss = 0.37066627\n",
      "Iteration 307, loss = 0.37085182\n",
      "Iteration 308, loss = 0.36979992\n",
      "Iteration 309, loss = 0.37020549\n",
      "Iteration 310, loss = 0.37039217\n",
      "Iteration 311, loss = 0.36977719\n",
      "Iteration 312, loss = 0.36970433\n",
      "Iteration 313, loss = 0.36878161\n",
      "Iteration 314, loss = 0.36909824\n",
      "Iteration 315, loss = 0.36920278\n",
      "Iteration 316, loss = 0.36869624\n",
      "Iteration 317, loss = 0.36913377\n",
      "Iteration 318, loss = 0.36750075\n",
      "Iteration 319, loss = 0.36792269\n",
      "Iteration 320, loss = 0.36767942\n",
      "Iteration 321, loss = 0.36703544\n",
      "Iteration 322, loss = 0.36782910\n",
      "Iteration 323, loss = 0.36706603\n",
      "Iteration 324, loss = 0.36675003\n",
      "Iteration 325, loss = 0.36672651\n",
      "Iteration 326, loss = 0.36720726\n",
      "Iteration 327, loss = 0.36587713\n",
      "Iteration 328, loss = 0.36586374\n",
      "Iteration 329, loss = 0.36583539\n",
      "Iteration 330, loss = 0.36619975\n",
      "Iteration 331, loss = 0.36545818\n",
      "Iteration 332, loss = 0.36494185\n",
      "Iteration 333, loss = 0.36593156\n",
      "Iteration 334, loss = 0.36475640\n",
      "Iteration 335, loss = 0.36495196\n",
      "Iteration 336, loss = 0.36447973\n",
      "Iteration 337, loss = 0.36409394\n",
      "Iteration 338, loss = 0.36404180\n",
      "Iteration 339, loss = 0.36393320\n",
      "Iteration 340, loss = 0.36301709\n",
      "Iteration 341, loss = 0.36363277\n",
      "Iteration 342, loss = 0.36348993\n",
      "Iteration 343, loss = 0.36267072\n",
      "Iteration 344, loss = 0.36240816\n",
      "Iteration 345, loss = 0.36287869\n",
      "Iteration 346, loss = 0.36268470\n",
      "Iteration 347, loss = 0.36210673\n",
      "Iteration 348, loss = 0.36214787\n",
      "Iteration 349, loss = 0.36132144\n",
      "Iteration 350, loss = 0.36223597\n",
      "Iteration 351, loss = 0.36175753\n",
      "Iteration 352, loss = 0.36130539\n",
      "Iteration 353, loss = 0.36104705\n",
      "Iteration 354, loss = 0.36110151\n",
      "Iteration 355, loss = 0.36043240\n",
      "Iteration 356, loss = 0.36009464\n",
      "Iteration 357, loss = 0.36055395\n",
      "Iteration 358, loss = 0.36026763\n",
      "Iteration 359, loss = 0.35974629\n",
      "Iteration 360, loss = 0.35992513\n",
      "Iteration 361, loss = 0.35973507\n",
      "Iteration 362, loss = 0.35920853\n",
      "Iteration 363, loss = 0.35892714\n",
      "Iteration 364, loss = 0.35922508\n",
      "Iteration 365, loss = 0.35919637\n",
      "Iteration 366, loss = 0.35879566\n",
      "Iteration 367, loss = 0.35893211\n",
      "Iteration 368, loss = 0.35819402\n",
      "Iteration 369, loss = 0.35766829\n",
      "Iteration 370, loss = 0.35785522\n",
      "Iteration 371, loss = 0.35726871\n",
      "Iteration 372, loss = 0.35851869\n",
      "Iteration 373, loss = 0.35680117\n",
      "Iteration 374, loss = 0.35801018\n",
      "Iteration 375, loss = 0.35678029\n",
      "Iteration 376, loss = 0.35658813\n",
      "Iteration 377, loss = 0.35641923\n",
      "Iteration 378, loss = 0.35681578\n",
      "Iteration 379, loss = 0.35598595\n",
      "Iteration 380, loss = 0.35592266\n",
      "Iteration 381, loss = 0.35545559\n",
      "Iteration 382, loss = 0.35568518\n",
      "Iteration 383, loss = 0.35442419\n",
      "Iteration 384, loss = 0.35595123\n",
      "Iteration 385, loss = 0.35624606\n",
      "Iteration 386, loss = 0.35477036\n",
      "Iteration 387, loss = 0.35500875\n",
      "Iteration 388, loss = 0.35567817\n",
      "Iteration 389, loss = 0.35439580\n",
      "Iteration 390, loss = 0.35435315\n",
      "Iteration 391, loss = 0.35577063\n",
      "Iteration 392, loss = 0.35511994\n",
      "Iteration 393, loss = 0.35370992\n",
      "Iteration 394, loss = 0.35514939\n",
      "Iteration 395, loss = 0.35484005\n",
      "Iteration 396, loss = 0.35354542\n",
      "Iteration 397, loss = 0.35308927\n",
      "Iteration 398, loss = 0.35490683\n",
      "Iteration 399, loss = 0.35264208\n",
      "Iteration 400, loss = 0.35252816\n",
      "Iteration 401, loss = 0.35251336\n",
      "Iteration 402, loss = 0.35247266\n",
      "Iteration 403, loss = 0.35213241\n",
      "Iteration 404, loss = 0.35261738\n",
      "Iteration 405, loss = 0.35319630\n",
      "Iteration 406, loss = 0.35255294\n",
      "Iteration 407, loss = 0.35132292\n",
      "Iteration 408, loss = 0.35235123\n",
      "Iteration 409, loss = 0.35217173\n",
      "Iteration 410, loss = 0.35141806\n",
      "Iteration 411, loss = 0.35148693\n",
      "Iteration 412, loss = 0.35091545\n",
      "Iteration 413, loss = 0.35156819\n",
      "Iteration 414, loss = 0.35171007\n",
      "Iteration 415, loss = 0.35059473\n",
      "Iteration 416, loss = 0.35098018\n",
      "Iteration 417, loss = 0.35022857\n",
      "Iteration 418, loss = 0.34934345\n",
      "Iteration 419, loss = 0.35005557\n",
      "Iteration 420, loss = 0.35006377\n",
      "Iteration 421, loss = 0.35073578\n",
      "Iteration 422, loss = 0.34943274\n",
      "Iteration 423, loss = 0.34892094\n",
      "Iteration 424, loss = 0.34896176\n",
      "Iteration 425, loss = 0.34883745\n",
      "Iteration 426, loss = 0.34931261\n",
      "Iteration 427, loss = 0.34878469\n",
      "Iteration 428, loss = 0.34808280\n",
      "Iteration 429, loss = 0.34876638\n",
      "Iteration 430, loss = 0.34744722\n",
      "Iteration 431, loss = 0.34830504\n",
      "Iteration 432, loss = 0.34651998\n",
      "Iteration 433, loss = 0.34754179\n",
      "Iteration 434, loss = 0.34771201\n",
      "Iteration 435, loss = 0.34673479\n",
      "Iteration 436, loss = 0.34684616\n",
      "Iteration 437, loss = 0.34704582\n",
      "Iteration 438, loss = 0.34779748\n",
      "Iteration 439, loss = 0.34708802\n",
      "Iteration 440, loss = 0.34580006\n",
      "Iteration 441, loss = 0.34606766\n",
      "Iteration 442, loss = 0.34599752\n",
      "Iteration 443, loss = 0.34587538\n",
      "Iteration 444, loss = 0.34645471\n",
      "Iteration 445, loss = 0.34521762\n",
      "Iteration 446, loss = 0.34586678\n",
      "Iteration 447, loss = 0.34513817\n",
      "Iteration 448, loss = 0.34569636\n",
      "Iteration 449, loss = 0.34439691\n",
      "Iteration 450, loss = 0.34482806\n",
      "Iteration 451, loss = 0.34451484\n",
      "Iteration 452, loss = 0.34492736\n",
      "Iteration 453, loss = 0.34416548\n",
      "Iteration 454, loss = 0.34481381\n",
      "Iteration 455, loss = 0.34306688\n",
      "Iteration 456, loss = 0.34399349\n",
      "Iteration 457, loss = 0.34497598\n",
      "Iteration 458, loss = 0.34339191\n",
      "Iteration 459, loss = 0.34279121\n",
      "Iteration 460, loss = 0.34275500\n",
      "Iteration 461, loss = 0.34345452\n",
      "Iteration 462, loss = 0.34228177\n",
      "Iteration 463, loss = 0.34240336\n",
      "Iteration 464, loss = 0.34252913\n",
      "Iteration 465, loss = 0.34243966\n",
      "Iteration 466, loss = 0.34395415\n",
      "Iteration 467, loss = 0.34132217\n",
      "Iteration 468, loss = 0.34386156\n",
      "Iteration 469, loss = 0.34197314\n",
      "Iteration 470, loss = 0.34162053\n",
      "Iteration 471, loss = 0.34083018\n",
      "Iteration 472, loss = 0.34135916\n",
      "Iteration 473, loss = 0.34079714\n",
      "Iteration 474, loss = 0.34129747\n",
      "Iteration 475, loss = 0.34124066\n",
      "Iteration 476, loss = 0.34083151\n",
      "Iteration 477, loss = 0.34047358\n",
      "Iteration 478, loss = 0.34123545\n",
      "Iteration 479, loss = 0.34072532\n",
      "Iteration 480, loss = 0.34059285\n",
      "Iteration 481, loss = 0.34078899\n",
      "Iteration 482, loss = 0.33926838\n",
      "Iteration 483, loss = 0.33938789\n",
      "Iteration 484, loss = 0.34009997\n",
      "Iteration 485, loss = 0.33878378\n",
      "Iteration 486, loss = 0.33881574\n",
      "Iteration 487, loss = 0.34001401\n",
      "Iteration 488, loss = 0.33956376\n",
      "Iteration 489, loss = 0.33858698\n",
      "Iteration 490, loss = 0.33811166\n",
      "Iteration 491, loss = 0.33840126\n",
      "Iteration 492, loss = 0.33894672\n",
      "Iteration 493, loss = 0.33742837\n",
      "Iteration 494, loss = 0.33790132\n",
      "Iteration 495, loss = 0.33754820\n",
      "Iteration 496, loss = 0.33840398\n",
      "Iteration 497, loss = 0.33766903\n",
      "Iteration 498, loss = 0.33802260\n",
      "Iteration 499, loss = 0.33631915\n",
      "Iteration 500, loss = 0.33701396\n",
      "Iteration 501, loss = 0.33691929\n",
      "Iteration 502, loss = 0.33609153\n",
      "Iteration 503, loss = 0.33698203\n",
      "Iteration 504, loss = 0.33591660\n",
      "Iteration 505, loss = 0.33604007\n",
      "Iteration 506, loss = 0.33738723\n",
      "Iteration 507, loss = 0.33689446\n",
      "Iteration 508, loss = 0.33638478\n",
      "Iteration 509, loss = 0.33607428\n",
      "Iteration 510, loss = 0.33606179\n",
      "Iteration 511, loss = 0.33663852\n",
      "Iteration 512, loss = 0.33486334\n",
      "Iteration 513, loss = 0.33532830\n",
      "Iteration 514, loss = 0.33476725\n",
      "Iteration 515, loss = 0.33541857\n",
      "Iteration 516, loss = 0.33473669\n",
      "Iteration 517, loss = 0.33533646\n",
      "Iteration 518, loss = 0.33508115\n",
      "Iteration 519, loss = 0.33547094\n",
      "Iteration 520, loss = 0.33450446\n",
      "Iteration 521, loss = 0.33327214\n",
      "Iteration 522, loss = 0.33442309\n",
      "Iteration 523, loss = 0.33418192\n",
      "Iteration 524, loss = 0.33421730\n",
      "Iteration 525, loss = 0.33409978\n",
      "Iteration 526, loss = 0.33355144\n",
      "Iteration 527, loss = 0.33343202\n",
      "Iteration 528, loss = 0.33264112\n",
      "Iteration 529, loss = 0.33446808\n",
      "Iteration 530, loss = 0.33346176\n",
      "Iteration 531, loss = 0.33398155\n",
      "Iteration 532, loss = 0.33244206\n",
      "Iteration 533, loss = 0.33286643\n",
      "Iteration 534, loss = 0.33188422\n",
      "Iteration 535, loss = 0.33327452\n",
      "Iteration 536, loss = 0.33328276\n",
      "Iteration 537, loss = 0.33185103\n",
      "Iteration 538, loss = 0.33183509\n",
      "Iteration 539, loss = 0.33122605\n",
      "Iteration 540, loss = 0.33247476\n",
      "Iteration 541, loss = 0.33166630\n",
      "Iteration 542, loss = 0.33150625\n",
      "Iteration 543, loss = 0.33157861\n",
      "Iteration 544, loss = 0.33053535\n",
      "Iteration 545, loss = 0.33052204\n",
      "Iteration 546, loss = 0.33179066\n",
      "Iteration 547, loss = 0.33141569\n",
      "Iteration 548, loss = 0.33118109\n",
      "Iteration 549, loss = 0.33211569\n",
      "Iteration 550, loss = 0.33021227\n",
      "Iteration 551, loss = 0.32923988\n",
      "Iteration 552, loss = 0.33082127\n",
      "Iteration 553, loss = 0.32900974\n",
      "Iteration 554, loss = 0.33018069\n",
      "Iteration 555, loss = 0.32923098\n",
      "Iteration 556, loss = 0.32858127\n",
      "Iteration 557, loss = 0.32896577\n",
      "Iteration 558, loss = 0.32945914\n",
      "Iteration 559, loss = 0.33127399\n",
      "Iteration 560, loss = 0.32874283\n",
      "Iteration 561, loss = 0.32905106\n",
      "Iteration 562, loss = 0.33004752\n",
      "Iteration 563, loss = 0.32907126\n",
      "Iteration 564, loss = 0.32914107\n",
      "Iteration 565, loss = 0.32825631\n",
      "Iteration 566, loss = 0.32981252\n",
      "Iteration 567, loss = 0.32912021\n",
      "Iteration 568, loss = 0.32865637\n",
      "Iteration 569, loss = 0.32833366\n",
      "Iteration 570, loss = 0.32852011\n",
      "Iteration 571, loss = 0.32743108\n",
      "Iteration 572, loss = 0.32723749\n",
      "Iteration 573, loss = 0.32784777\n",
      "Iteration 574, loss = 0.32714736\n",
      "Iteration 575, loss = 0.32794716\n",
      "Iteration 576, loss = 0.32622303\n",
      "Iteration 577, loss = 0.32718107\n",
      "Iteration 578, loss = 0.32692465\n",
      "Iteration 579, loss = 0.32696394\n",
      "Iteration 580, loss = 0.32638500\n",
      "Iteration 581, loss = 0.32642484\n",
      "Iteration 582, loss = 0.32628065\n",
      "Iteration 583, loss = 0.32717554\n",
      "Iteration 584, loss = 0.32607340\n",
      "Iteration 585, loss = 0.32769649\n",
      "Iteration 586, loss = 0.32720569\n",
      "Iteration 587, loss = 0.32550442\n",
      "Iteration 588, loss = 0.32500862\n",
      "Iteration 589, loss = 0.32539897\n",
      "Iteration 590, loss = 0.32609093\n",
      "Iteration 591, loss = 0.32604142\n",
      "Iteration 592, loss = 0.32592705\n",
      "Iteration 593, loss = 0.32568435\n",
      "Iteration 594, loss = 0.32473071\n",
      "Iteration 595, loss = 0.32689612\n",
      "Iteration 596, loss = 0.32567183\n",
      "Iteration 597, loss = 0.32446160\n",
      "Iteration 598, loss = 0.32418156\n",
      "Iteration 599, loss = 0.32622437\n",
      "Iteration 600, loss = 0.32544757\n",
      "Iteration 601, loss = 0.32390565\n",
      "Iteration 602, loss = 0.32414290\n",
      "Iteration 603, loss = 0.32339702\n",
      "Iteration 604, loss = 0.32422550\n",
      "Iteration 605, loss = 0.32391273\n",
      "Iteration 606, loss = 0.32334037\n",
      "Iteration 607, loss = 0.32400708\n",
      "Iteration 608, loss = 0.32300927\n",
      "Iteration 609, loss = 0.32381009\n",
      "Iteration 610, loss = 0.32354841\n",
      "Iteration 611, loss = 0.32356121\n",
      "Iteration 612, loss = 0.32445455\n",
      "Iteration 613, loss = 0.32206953\n",
      "Iteration 614, loss = 0.32348151\n",
      "Iteration 615, loss = 0.32241883\n",
      "Iteration 616, loss = 0.32189186\n",
      "Iteration 617, loss = 0.32244764\n",
      "Iteration 618, loss = 0.32381603\n",
      "Iteration 619, loss = 0.32258599\n",
      "Iteration 620, loss = 0.32259202\n",
      "Iteration 621, loss = 0.32235349\n",
      "Iteration 622, loss = 0.32252907\n",
      "Iteration 623, loss = 0.32176625\n",
      "Iteration 624, loss = 0.32276721\n",
      "Iteration 625, loss = 0.32149413\n",
      "Iteration 626, loss = 0.32275107\n",
      "Iteration 627, loss = 0.32125894\n",
      "Iteration 628, loss = 0.32120889\n",
      "Iteration 629, loss = 0.32087525\n",
      "Iteration 630, loss = 0.32021932\n",
      "Iteration 631, loss = 0.32192967\n",
      "Iteration 632, loss = 0.32226354\n",
      "Iteration 633, loss = 0.32073361\n",
      "Iteration 634, loss = 0.31994934\n",
      "Iteration 635, loss = 0.32029977\n",
      "Iteration 636, loss = 0.31979565\n",
      "Iteration 637, loss = 0.32006247\n",
      "Iteration 638, loss = 0.32182668\n",
      "Iteration 639, loss = 0.31939242\n",
      "Iteration 640, loss = 0.32087839\n",
      "Iteration 641, loss = 0.31923899\n",
      "Iteration 642, loss = 0.32074320\n",
      "Iteration 643, loss = 0.32044451\n",
      "Iteration 644, loss = 0.31888011\n",
      "Iteration 645, loss = 0.32009943\n",
      "Iteration 646, loss = 0.31833484\n",
      "Iteration 647, loss = 0.31986319\n",
      "Iteration 648, loss = 0.32024400\n",
      "Iteration 649, loss = 0.31958161\n",
      "Iteration 650, loss = 0.31878473\n",
      "Iteration 651, loss = 0.31999013\n",
      "Iteration 652, loss = 0.31905564\n",
      "Iteration 653, loss = 0.31817584\n",
      "Iteration 654, loss = 0.31944051\n",
      "Iteration 655, loss = 0.31708832\n",
      "Iteration 656, loss = 0.31899932\n",
      "Iteration 657, loss = 0.31915395\n",
      "Iteration 658, loss = 0.31675428\n",
      "Iteration 659, loss = 0.31691935\n",
      "Iteration 660, loss = 0.31724686\n",
      "Iteration 661, loss = 0.31791825\n",
      "Iteration 662, loss = 0.31776290\n",
      "Iteration 663, loss = 0.31741473\n",
      "Iteration 664, loss = 0.31590410\n",
      "Iteration 665, loss = 0.31766673\n",
      "Iteration 666, loss = 0.31741218\n",
      "Iteration 667, loss = 0.31704620\n",
      "Iteration 668, loss = 0.31694030\n",
      "Iteration 669, loss = 0.31647648\n",
      "Iteration 670, loss = 0.31758720\n",
      "Iteration 671, loss = 0.31600048\n",
      "Iteration 672, loss = 0.31577502\n",
      "Iteration 673, loss = 0.31803000\n",
      "Iteration 674, loss = 0.31780909\n",
      "Iteration 675, loss = 0.31684647\n",
      "Iteration 676, loss = 0.31475258\n",
      "Iteration 677, loss = 0.31551038\n",
      "Iteration 678, loss = 0.31554173\n",
      "Iteration 679, loss = 0.31658502\n",
      "Iteration 680, loss = 0.31728845\n",
      "Iteration 681, loss = 0.31584769\n",
      "Iteration 682, loss = 0.31561727\n",
      "Iteration 683, loss = 0.31602853\n",
      "Iteration 684, loss = 0.31645108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [05:24<02:33, 30.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 685, loss = 0.31578171\n",
      "Iteration 686, loss = 0.31537279\n",
      "Iteration 687, loss = 0.31678356\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56110451\n",
      "Iteration 2, loss = 0.50012015\n",
      "Iteration 3, loss = 0.47942359\n",
      "Iteration 4, loss = 0.46818371\n",
      "Iteration 5, loss = 0.46228914\n",
      "Iteration 6, loss = 0.45887157\n",
      "Iteration 7, loss = 0.45663908\n",
      "Iteration 8, loss = 0.45487463\n",
      "Iteration 9, loss = 0.45343973\n",
      "Iteration 10, loss = 0.45252180\n",
      "Iteration 11, loss = 0.45153991\n",
      "Iteration 12, loss = 0.45070914\n",
      "Iteration 13, loss = 0.45003870\n",
      "Iteration 14, loss = 0.44950677\n",
      "Iteration 15, loss = 0.44851034\n",
      "Iteration 16, loss = 0.44803062\n",
      "Iteration 17, loss = 0.44765339\n",
      "Iteration 18, loss = 0.44698729\n",
      "Iteration 19, loss = 0.44668335\n",
      "Iteration 20, loss = 0.44596498\n",
      "Iteration 21, loss = 0.44554525\n",
      "Iteration 22, loss = 0.44578758\n",
      "Iteration 23, loss = 0.44486463\n",
      "Iteration 24, loss = 0.44459339\n",
      "Iteration 25, loss = 0.44389142\n",
      "Iteration 26, loss = 0.44369232\n",
      "Iteration 27, loss = 0.44309268\n",
      "Iteration 28, loss = 0.44305637\n",
      "Iteration 29, loss = 0.44238143\n",
      "Iteration 30, loss = 0.44200727\n",
      "Iteration 31, loss = 0.44158068\n",
      "Iteration 32, loss = 0.44091481\n",
      "Iteration 33, loss = 0.44101344\n",
      "Iteration 34, loss = 0.44057768\n",
      "Iteration 35, loss = 0.44019078\n",
      "Iteration 36, loss = 0.43969139\n",
      "Iteration 37, loss = 0.43944986\n",
      "Iteration 38, loss = 0.43917103\n",
      "Iteration 39, loss = 0.43884952\n",
      "Iteration 40, loss = 0.43843989\n",
      "Iteration 41, loss = 0.43818100\n",
      "Iteration 42, loss = 0.43783282\n",
      "Iteration 43, loss = 0.43743278\n",
      "Iteration 44, loss = 0.43713727\n",
      "Iteration 45, loss = 0.43677137\n",
      "Iteration 46, loss = 0.43661385\n",
      "Iteration 47, loss = 0.43653750\n",
      "Iteration 48, loss = 0.43604399\n",
      "Iteration 49, loss = 0.43578708\n",
      "Iteration 50, loss = 0.43518588\n",
      "Iteration 51, loss = 0.43503763\n",
      "Iteration 52, loss = 0.43465950\n",
      "Iteration 53, loss = 0.43453613\n",
      "Iteration 54, loss = 0.43420915\n",
      "Iteration 55, loss = 0.43381686\n",
      "Iteration 56, loss = 0.43359804\n",
      "Iteration 57, loss = 0.43330961\n",
      "Iteration 58, loss = 0.43281555\n",
      "Iteration 59, loss = 0.43267405\n",
      "Iteration 60, loss = 0.43221145\n",
      "Iteration 61, loss = 0.43193823\n",
      "Iteration 62, loss = 0.43173074\n",
      "Iteration 63, loss = 0.43155824\n",
      "Iteration 64, loss = 0.43108040\n",
      "Iteration 65, loss = 0.43085443\n",
      "Iteration 66, loss = 0.43045764\n",
      "Iteration 67, loss = 0.43023738\n",
      "Iteration 68, loss = 0.43021573\n",
      "Iteration 69, loss = 0.42973316\n",
      "Iteration 70, loss = 0.42919790\n",
      "Iteration 71, loss = 0.42909571\n",
      "Iteration 72, loss = 0.42888745\n",
      "Iteration 73, loss = 0.42854845\n",
      "Iteration 74, loss = 0.42808718\n",
      "Iteration 75, loss = 0.42804889\n",
      "Iteration 76, loss = 0.42779716\n",
      "Iteration 77, loss = 0.42758774\n",
      "Iteration 78, loss = 0.42699012\n",
      "Iteration 79, loss = 0.42680315\n",
      "Iteration 80, loss = 0.42650998\n",
      "Iteration 81, loss = 0.42618839\n",
      "Iteration 82, loss = 0.42606165\n",
      "Iteration 83, loss = 0.42579183\n",
      "Iteration 84, loss = 0.42517195\n",
      "Iteration 85, loss = 0.42547158\n",
      "Iteration 86, loss = 0.42465209\n",
      "Iteration 87, loss = 0.42482578\n",
      "Iteration 88, loss = 0.42424020\n",
      "Iteration 89, loss = 0.42416877\n",
      "Iteration 90, loss = 0.42398282\n",
      "Iteration 91, loss = 0.42349168\n",
      "Iteration 92, loss = 0.42330513\n",
      "Iteration 93, loss = 0.42303460\n",
      "Iteration 94, loss = 0.42284417\n",
      "Iteration 95, loss = 0.42227442\n",
      "Iteration 96, loss = 0.42234038\n",
      "Iteration 97, loss = 0.42172791\n",
      "Iteration 98, loss = 0.42148046\n",
      "Iteration 99, loss = 0.42130360\n",
      "Iteration 100, loss = 0.42120570\n",
      "Iteration 101, loss = 0.42111526\n",
      "Iteration 102, loss = 0.42062328\n",
      "Iteration 103, loss = 0.42016378\n",
      "Iteration 104, loss = 0.42006020\n",
      "Iteration 105, loss = 0.42023844\n",
      "Iteration 106, loss = 0.41939114\n",
      "Iteration 107, loss = 0.41934744\n",
      "Iteration 108, loss = 0.41877871\n",
      "Iteration 109, loss = 0.41892296\n",
      "Iteration 110, loss = 0.41831498\n",
      "Iteration 111, loss = 0.41810402\n",
      "Iteration 112, loss = 0.41786303\n",
      "Iteration 113, loss = 0.41754911\n",
      "Iteration 114, loss = 0.41714489\n",
      "Iteration 115, loss = 0.41729285\n",
      "Iteration 116, loss = 0.41691345\n",
      "Iteration 117, loss = 0.41664343\n",
      "Iteration 118, loss = 0.41615406\n",
      "Iteration 119, loss = 0.41581263\n",
      "Iteration 120, loss = 0.41563240\n",
      "Iteration 121, loss = 0.41582895\n",
      "Iteration 122, loss = 0.41499511\n",
      "Iteration 123, loss = 0.41484131\n",
      "Iteration 124, loss = 0.41523844\n",
      "Iteration 125, loss = 0.41445896\n",
      "Iteration 126, loss = 0.41462564\n",
      "Iteration 127, loss = 0.41423620\n",
      "Iteration 128, loss = 0.41322923\n",
      "Iteration 129, loss = 0.41356143\n",
      "Iteration 130, loss = 0.41312604\n",
      "Iteration 131, loss = 0.41348824\n",
      "Iteration 132, loss = 0.41306635\n",
      "Iteration 133, loss = 0.41242702\n",
      "Iteration 134, loss = 0.41219350\n",
      "Iteration 135, loss = 0.41208047\n",
      "Iteration 136, loss = 0.41198049\n",
      "Iteration 137, loss = 0.41143760\n",
      "Iteration 138, loss = 0.41113256\n",
      "Iteration 139, loss = 0.41110331\n",
      "Iteration 140, loss = 0.41058059\n",
      "Iteration 141, loss = 0.41032609\n",
      "Iteration 142, loss = 0.41008059\n",
      "Iteration 143, loss = 0.40992189\n",
      "Iteration 144, loss = 0.40936285\n",
      "Iteration 145, loss = 0.40896500\n",
      "Iteration 146, loss = 0.40852960\n",
      "Iteration 147, loss = 0.40855388\n",
      "Iteration 148, loss = 0.40839012\n",
      "Iteration 149, loss = 0.40809708\n",
      "Iteration 150, loss = 0.40773267\n",
      "Iteration 151, loss = 0.40788897\n",
      "Iteration 152, loss = 0.40696280\n",
      "Iteration 153, loss = 0.40720421\n",
      "Iteration 154, loss = 0.40695186\n",
      "Iteration 155, loss = 0.40645318\n",
      "Iteration 156, loss = 0.40629293\n",
      "Iteration 157, loss = 0.40574088\n",
      "Iteration 158, loss = 0.40596692\n",
      "Iteration 159, loss = 0.40526017\n",
      "Iteration 160, loss = 0.40517772\n",
      "Iteration 161, loss = 0.40508994\n",
      "Iteration 162, loss = 0.40439108\n",
      "Iteration 163, loss = 0.40419692\n",
      "Iteration 164, loss = 0.40388503\n",
      "Iteration 165, loss = 0.40370802\n",
      "Iteration 166, loss = 0.40353863\n",
      "Iteration 167, loss = 0.40306904\n",
      "Iteration 168, loss = 0.40313807\n",
      "Iteration 169, loss = 0.40282689\n",
      "Iteration 170, loss = 0.40300291\n",
      "Iteration 171, loss = 0.40178455\n",
      "Iteration 172, loss = 0.40213678\n",
      "Iteration 173, loss = 0.40199936\n",
      "Iteration 174, loss = 0.40165845\n",
      "Iteration 175, loss = 0.40087938\n",
      "Iteration 176, loss = 0.40116594\n",
      "Iteration 177, loss = 0.40038560\n",
      "Iteration 178, loss = 0.40015293\n",
      "Iteration 179, loss = 0.40009012\n",
      "Iteration 180, loss = 0.39967798\n",
      "Iteration 181, loss = 0.39980284\n",
      "Iteration 182, loss = 0.39962960\n",
      "Iteration 183, loss = 0.39902240\n",
      "Iteration 184, loss = 0.39847128\n",
      "Iteration 185, loss = 0.39883831\n",
      "Iteration 186, loss = 0.39820744\n",
      "Iteration 187, loss = 0.39806822\n",
      "Iteration 188, loss = 0.39784786\n",
      "Iteration 189, loss = 0.39738121\n",
      "Iteration 190, loss = 0.39759114\n",
      "Iteration 191, loss = 0.39663896\n",
      "Iteration 192, loss = 0.39648812\n",
      "Iteration 193, loss = 0.39641749\n",
      "Iteration 194, loss = 0.39621921\n",
      "Iteration 195, loss = 0.39608306\n",
      "Iteration 196, loss = 0.39577855\n",
      "Iteration 197, loss = 0.39525271\n",
      "Iteration 198, loss = 0.39541378\n",
      "Iteration 199, loss = 0.39482670\n",
      "Iteration 200, loss = 0.39468866\n",
      "Iteration 201, loss = 0.39430149\n",
      "Iteration 202, loss = 0.39435707\n",
      "Iteration 203, loss = 0.39414093\n",
      "Iteration 204, loss = 0.39351362\n",
      "Iteration 205, loss = 0.39293057\n",
      "Iteration 206, loss = 0.39355010\n",
      "Iteration 207, loss = 0.39257515\n",
      "Iteration 208, loss = 0.39229940\n",
      "Iteration 209, loss = 0.39248632\n",
      "Iteration 210, loss = 0.39187593\n",
      "Iteration 211, loss = 0.39136351\n",
      "Iteration 212, loss = 0.39170553\n",
      "Iteration 213, loss = 0.39078513\n",
      "Iteration 214, loss = 0.39110106\n",
      "Iteration 215, loss = 0.39040651\n",
      "Iteration 216, loss = 0.39027551\n",
      "Iteration 217, loss = 0.39009924\n",
      "Iteration 218, loss = 0.38967560\n",
      "Iteration 219, loss = 0.38971934\n",
      "Iteration 220, loss = 0.38953096\n",
      "Iteration 221, loss = 0.38936971\n",
      "Iteration 222, loss = 0.38936879\n",
      "Iteration 223, loss = 0.38885987\n",
      "Iteration 224, loss = 0.38876046\n",
      "Iteration 225, loss = 0.38835157\n",
      "Iteration 226, loss = 0.38824279\n",
      "Iteration 227, loss = 0.38711961\n",
      "Iteration 228, loss = 0.38762734\n",
      "Iteration 229, loss = 0.38663112\n",
      "Iteration 230, loss = 0.38687284\n",
      "Iteration 231, loss = 0.38641445\n",
      "Iteration 232, loss = 0.38637223\n",
      "Iteration 233, loss = 0.38610014\n",
      "Iteration 234, loss = 0.38594531\n",
      "Iteration 235, loss = 0.38553101\n",
      "Iteration 236, loss = 0.38510240\n",
      "Iteration 237, loss = 0.38565638\n",
      "Iteration 238, loss = 0.38415231\n",
      "Iteration 239, loss = 0.38413368\n",
      "Iteration 240, loss = 0.38393233\n",
      "Iteration 241, loss = 0.38357020\n",
      "Iteration 242, loss = 0.38363486\n",
      "Iteration 243, loss = 0.38336541\n",
      "Iteration 244, loss = 0.38349572\n",
      "Iteration 245, loss = 0.38326442\n",
      "Iteration 246, loss = 0.38283160\n",
      "Iteration 247, loss = 0.38295985\n",
      "Iteration 248, loss = 0.38211022\n",
      "Iteration 249, loss = 0.38179140\n",
      "Iteration 250, loss = 0.38179794\n",
      "Iteration 251, loss = 0.38158231\n",
      "Iteration 252, loss = 0.38103517\n",
      "Iteration 253, loss = 0.38096555\n",
      "Iteration 254, loss = 0.38073613\n",
      "Iteration 255, loss = 0.38121136\n",
      "Iteration 256, loss = 0.37984080\n",
      "Iteration 257, loss = 0.37966117\n",
      "Iteration 258, loss = 0.37950825\n",
      "Iteration 259, loss = 0.37945035\n",
      "Iteration 260, loss = 0.37951697\n",
      "Iteration 261, loss = 0.37894989\n",
      "Iteration 262, loss = 0.37981593\n",
      "Iteration 263, loss = 0.37888288\n",
      "Iteration 264, loss = 0.37834604\n",
      "Iteration 265, loss = 0.37781708\n",
      "Iteration 266, loss = 0.37793728\n",
      "Iteration 267, loss = 0.37730706\n",
      "Iteration 268, loss = 0.37723915\n",
      "Iteration 269, loss = 0.37668738\n",
      "Iteration 270, loss = 0.37625806\n",
      "Iteration 271, loss = 0.37714497\n",
      "Iteration 272, loss = 0.37584412\n",
      "Iteration 273, loss = 0.37586364\n",
      "Iteration 274, loss = 0.37533301\n",
      "Iteration 275, loss = 0.37568858\n",
      "Iteration 276, loss = 0.37508327\n",
      "Iteration 277, loss = 0.37543219\n",
      "Iteration 278, loss = 0.37478264\n",
      "Iteration 279, loss = 0.37420767\n",
      "Iteration 280, loss = 0.37386225\n",
      "Iteration 281, loss = 0.37375752\n",
      "Iteration 282, loss = 0.37323898\n",
      "Iteration 283, loss = 0.37355943\n",
      "Iteration 284, loss = 0.37352919\n",
      "Iteration 285, loss = 0.37310891\n",
      "Iteration 286, loss = 0.37278046\n",
      "Iteration 287, loss = 0.37269447\n",
      "Iteration 288, loss = 0.37295904\n",
      "Iteration 289, loss = 0.37281243\n",
      "Iteration 290, loss = 0.37202685\n",
      "Iteration 291, loss = 0.37160038\n",
      "Iteration 292, loss = 0.37192699\n",
      "Iteration 293, loss = 0.37119905\n",
      "Iteration 294, loss = 0.37113496\n",
      "Iteration 295, loss = 0.37056065\n",
      "Iteration 296, loss = 0.36970911\n",
      "Iteration 297, loss = 0.37002606\n",
      "Iteration 298, loss = 0.37000510\n",
      "Iteration 299, loss = 0.36979558\n",
      "Iteration 300, loss = 0.36950979\n",
      "Iteration 301, loss = 0.36895362\n",
      "Iteration 302, loss = 0.36936051\n",
      "Iteration 303, loss = 0.36890250\n",
      "Iteration 304, loss = 0.36959006\n",
      "Iteration 305, loss = 0.36764977\n",
      "Iteration 306, loss = 0.36806039\n",
      "Iteration 307, loss = 0.36766590\n",
      "Iteration 308, loss = 0.36793611\n",
      "Iteration 309, loss = 0.36760025\n",
      "Iteration 310, loss = 0.36714479\n",
      "Iteration 311, loss = 0.36692734\n",
      "Iteration 312, loss = 0.36693184\n",
      "Iteration 313, loss = 0.36603439\n",
      "Iteration 314, loss = 0.36614465\n",
      "Iteration 315, loss = 0.36605094\n",
      "Iteration 316, loss = 0.36541061\n",
      "Iteration 317, loss = 0.36593346\n",
      "Iteration 318, loss = 0.36656274\n",
      "Iteration 319, loss = 0.36475754\n",
      "Iteration 320, loss = 0.36522872\n",
      "Iteration 321, loss = 0.36476513\n",
      "Iteration 322, loss = 0.36442369\n",
      "Iteration 323, loss = 0.36381170\n",
      "Iteration 324, loss = 0.36422696\n",
      "Iteration 325, loss = 0.36379728\n",
      "Iteration 326, loss = 0.36420031\n",
      "Iteration 327, loss = 0.36338997\n",
      "Iteration 328, loss = 0.36335034\n",
      "Iteration 329, loss = 0.36323962\n",
      "Iteration 330, loss = 0.36285727\n",
      "Iteration 331, loss = 0.36255804\n",
      "Iteration 332, loss = 0.36279491\n",
      "Iteration 333, loss = 0.36286061\n",
      "Iteration 334, loss = 0.36246714\n",
      "Iteration 335, loss = 0.36130146\n",
      "Iteration 336, loss = 0.36177813\n",
      "Iteration 337, loss = 0.36155419\n",
      "Iteration 338, loss = 0.36166559\n",
      "Iteration 339, loss = 0.36069236\n",
      "Iteration 340, loss = 0.36084744\n",
      "Iteration 341, loss = 0.36007482\n",
      "Iteration 342, loss = 0.36033897\n",
      "Iteration 343, loss = 0.35924754\n",
      "Iteration 344, loss = 0.36083202\n",
      "Iteration 345, loss = 0.35925176\n",
      "Iteration 346, loss = 0.35923503\n",
      "Iteration 347, loss = 0.35952977\n",
      "Iteration 348, loss = 0.35872265\n",
      "Iteration 349, loss = 0.35886362\n",
      "Iteration 350, loss = 0.35943090\n",
      "Iteration 351, loss = 0.35876931\n",
      "Iteration 352, loss = 0.35787640\n",
      "Iteration 353, loss = 0.35861010\n",
      "Iteration 354, loss = 0.35772253\n",
      "Iteration 355, loss = 0.35787232\n",
      "Iteration 356, loss = 0.35672732\n",
      "Iteration 357, loss = 0.35678224\n",
      "Iteration 358, loss = 0.35759908\n",
      "Iteration 359, loss = 0.35601663\n",
      "Iteration 360, loss = 0.35692653\n",
      "Iteration 361, loss = 0.35678849\n",
      "Iteration 362, loss = 0.35614817\n",
      "Iteration 363, loss = 0.35617212\n",
      "Iteration 364, loss = 0.35564292\n",
      "Iteration 365, loss = 0.35528993\n",
      "Iteration 366, loss = 0.35497450\n",
      "Iteration 367, loss = 0.35477932\n",
      "Iteration 368, loss = 0.35451500\n",
      "Iteration 369, loss = 0.35522905\n",
      "Iteration 370, loss = 0.35455454\n",
      "Iteration 371, loss = 0.35391311\n",
      "Iteration 372, loss = 0.35466089\n",
      "Iteration 373, loss = 0.35414752\n",
      "Iteration 374, loss = 0.35267717\n",
      "Iteration 375, loss = 0.35323108\n",
      "Iteration 376, loss = 0.35305367\n",
      "Iteration 377, loss = 0.35274892\n",
      "Iteration 378, loss = 0.35269690\n",
      "Iteration 379, loss = 0.35238243\n",
      "Iteration 380, loss = 0.35255149\n",
      "Iteration 381, loss = 0.35279437\n",
      "Iteration 382, loss = 0.35190409\n",
      "Iteration 383, loss = 0.35260442\n",
      "Iteration 384, loss = 0.35206788\n",
      "Iteration 385, loss = 0.35128967\n",
      "Iteration 386, loss = 0.35155345\n",
      "Iteration 387, loss = 0.35066752\n",
      "Iteration 388, loss = 0.35054468\n",
      "Iteration 389, loss = 0.35089155\n",
      "Iteration 390, loss = 0.35022129\n",
      "Iteration 391, loss = 0.34991019\n",
      "Iteration 392, loss = 0.35046094\n",
      "Iteration 393, loss = 0.35040443\n",
      "Iteration 394, loss = 0.34896961\n",
      "Iteration 395, loss = 0.34967067\n",
      "Iteration 396, loss = 0.34946578\n",
      "Iteration 397, loss = 0.34899548\n",
      "Iteration 398, loss = 0.34871617\n",
      "Iteration 399, loss = 0.34898674\n",
      "Iteration 400, loss = 0.34893185\n",
      "Iteration 401, loss = 0.34949794\n",
      "Iteration 402, loss = 0.34824518\n",
      "Iteration 403, loss = 0.34854521\n",
      "Iteration 404, loss = 0.34813766\n",
      "Iteration 405, loss = 0.34770155\n",
      "Iteration 406, loss = 0.34742457\n",
      "Iteration 407, loss = 0.34719951\n",
      "Iteration 408, loss = 0.34656026\n",
      "Iteration 409, loss = 0.34668234\n",
      "Iteration 410, loss = 0.34669996\n",
      "Iteration 411, loss = 0.34696601\n",
      "Iteration 412, loss = 0.34547539\n",
      "Iteration 413, loss = 0.34597029\n",
      "Iteration 414, loss = 0.34673046\n",
      "Iteration 415, loss = 0.34595626\n",
      "Iteration 416, loss = 0.34542947\n",
      "Iteration 417, loss = 0.34523827\n",
      "Iteration 418, loss = 0.34546515\n",
      "Iteration 419, loss = 0.34508504\n",
      "Iteration 420, loss = 0.34554677\n",
      "Iteration 421, loss = 0.34534856\n",
      "Iteration 422, loss = 0.34410380\n",
      "Iteration 423, loss = 0.34358589\n",
      "Iteration 424, loss = 0.34340662\n",
      "Iteration 425, loss = 0.34415670\n",
      "Iteration 426, loss = 0.34379309\n",
      "Iteration 427, loss = 0.34380553\n",
      "Iteration 428, loss = 0.34280042\n",
      "Iteration 429, loss = 0.34251361\n",
      "Iteration 430, loss = 0.34272212\n",
      "Iteration 431, loss = 0.34338567\n",
      "Iteration 432, loss = 0.34265486\n",
      "Iteration 433, loss = 0.34266202\n",
      "Iteration 434, loss = 0.34249746\n",
      "Iteration 435, loss = 0.34210886\n",
      "Iteration 436, loss = 0.34224513\n",
      "Iteration 437, loss = 0.34195231\n",
      "Iteration 438, loss = 0.34126961\n",
      "Iteration 439, loss = 0.34171394\n",
      "Iteration 440, loss = 0.34067142\n",
      "Iteration 441, loss = 0.34148777\n",
      "Iteration 442, loss = 0.34147041\n",
      "Iteration 443, loss = 0.34060551\n",
      "Iteration 444, loss = 0.34022645\n",
      "Iteration 445, loss = 0.34007526\n",
      "Iteration 446, loss = 0.34001902\n",
      "Iteration 447, loss = 0.33993245\n",
      "Iteration 448, loss = 0.33987783\n",
      "Iteration 449, loss = 0.34004192\n",
      "Iteration 450, loss = 0.33972418\n",
      "Iteration 451, loss = 0.33937341\n",
      "Iteration 452, loss = 0.33853480\n",
      "Iteration 453, loss = 0.33955246\n",
      "Iteration 454, loss = 0.33825892\n",
      "Iteration 455, loss = 0.33871175\n",
      "Iteration 456, loss = 0.33848428\n",
      "Iteration 457, loss = 0.33802706\n",
      "Iteration 458, loss = 0.33838882\n",
      "Iteration 459, loss = 0.33823039\n",
      "Iteration 460, loss = 0.33796873\n",
      "Iteration 461, loss = 0.33719981\n",
      "Iteration 462, loss = 0.33715817\n",
      "Iteration 463, loss = 0.33762975\n",
      "Iteration 464, loss = 0.33777069\n",
      "Iteration 465, loss = 0.33748455\n",
      "Iteration 466, loss = 0.33703347\n",
      "Iteration 467, loss = 0.33721110\n",
      "Iteration 468, loss = 0.33576223\n",
      "Iteration 469, loss = 0.33650442\n",
      "Iteration 470, loss = 0.33657492\n",
      "Iteration 471, loss = 0.33670208\n",
      "Iteration 472, loss = 0.33636107\n",
      "Iteration 473, loss = 0.33590014\n",
      "Iteration 474, loss = 0.33593528\n",
      "Iteration 475, loss = 0.33516488\n",
      "Iteration 476, loss = 0.33446597\n",
      "Iteration 477, loss = 0.33546834\n",
      "Iteration 478, loss = 0.33545659\n",
      "Iteration 479, loss = 0.33507910\n",
      "Iteration 480, loss = 0.33546791\n",
      "Iteration 481, loss = 0.33450295\n",
      "Iteration 482, loss = 0.33472282\n",
      "Iteration 483, loss = 0.33380579\n",
      "Iteration 484, loss = 0.33425779\n",
      "Iteration 485, loss = 0.33343729\n",
      "Iteration 486, loss = 0.33462662\n",
      "Iteration 487, loss = 0.33296658\n",
      "Iteration 488, loss = 0.33423497\n",
      "Iteration 489, loss = 0.33302471\n",
      "Iteration 490, loss = 0.33229819\n",
      "Iteration 491, loss = 0.33205611\n",
      "Iteration 492, loss = 0.33441578\n",
      "Iteration 493, loss = 0.33236274\n",
      "Iteration 494, loss = 0.33329459\n",
      "Iteration 495, loss = 0.33225847\n",
      "Iteration 496, loss = 0.33222299\n",
      "Iteration 497, loss = 0.33244859\n",
      "Iteration 498, loss = 0.33220719\n",
      "Iteration 499, loss = 0.33249497\n",
      "Iteration 500, loss = 0.33128023\n",
      "Iteration 501, loss = 0.33014250\n",
      "Iteration 502, loss = 0.33030727\n",
      "Iteration 503, loss = 0.33077748\n",
      "Iteration 504, loss = 0.32973056\n",
      "Iteration 505, loss = 0.33217237\n",
      "Iteration 506, loss = 0.33012130\n",
      "Iteration 507, loss = 0.33134362\n",
      "Iteration 508, loss = 0.32952716\n",
      "Iteration 509, loss = 0.33009040\n",
      "Iteration 510, loss = 0.32993719\n",
      "Iteration 511, loss = 0.32976354\n",
      "Iteration 512, loss = 0.32934004\n",
      "Iteration 513, loss = 0.32930607\n",
      "Iteration 514, loss = 0.32901198\n",
      "Iteration 515, loss = 0.32935673\n",
      "Iteration 516, loss = 0.32989462\n",
      "Iteration 517, loss = 0.32830413\n",
      "Iteration 518, loss = 0.32881193\n",
      "Iteration 519, loss = 0.32824872\n",
      "Iteration 520, loss = 0.32795705\n",
      "Iteration 521, loss = 0.32812500\n",
      "Iteration 522, loss = 0.32769591\n",
      "Iteration 523, loss = 0.32745527\n",
      "Iteration 524, loss = 0.32767955\n",
      "Iteration 525, loss = 0.32750615\n",
      "Iteration 526, loss = 0.32803646\n",
      "Iteration 527, loss = 0.32713398\n",
      "Iteration 528, loss = 0.32806392\n",
      "Iteration 529, loss = 0.32701269\n",
      "Iteration 530, loss = 0.32757777\n",
      "Iteration 531, loss = 0.32722852\n",
      "Iteration 532, loss = 0.32705179\n",
      "Iteration 533, loss = 0.32733442\n",
      "Iteration 534, loss = 0.32733926\n",
      "Iteration 535, loss = 0.32671530\n",
      "Iteration 536, loss = 0.32573463\n",
      "Iteration 537, loss = 0.32518211\n",
      "Iteration 538, loss = 0.32620652\n",
      "Iteration 539, loss = 0.32611887\n",
      "Iteration 540, loss = 0.32542494\n",
      "Iteration 541, loss = 0.32583085\n",
      "Iteration 542, loss = 0.32472947\n",
      "Iteration 543, loss = 0.32587123\n",
      "Iteration 544, loss = 0.32499488\n",
      "Iteration 545, loss = 0.32526344\n",
      "Iteration 546, loss = 0.32425197\n",
      "Iteration 547, loss = 0.32486777\n",
      "Iteration 548, loss = 0.32455690\n",
      "Iteration 549, loss = 0.32414836\n",
      "Iteration 550, loss = 0.32346965\n",
      "Iteration 551, loss = 0.32375871\n",
      "Iteration 552, loss = 0.32393305\n",
      "Iteration 553, loss = 0.32449413\n",
      "Iteration 554, loss = 0.32453210\n",
      "Iteration 555, loss = 0.32435725\n",
      "Iteration 556, loss = 0.32302625\n",
      "Iteration 557, loss = 0.32328523\n",
      "Iteration 558, loss = 0.32384847\n",
      "Iteration 559, loss = 0.32198533\n",
      "Iteration 560, loss = 0.32363269\n",
      "Iteration 561, loss = 0.32178316\n",
      "Iteration 562, loss = 0.32288605\n",
      "Iteration 563, loss = 0.32084971\n",
      "Iteration 564, loss = 0.32196499\n",
      "Iteration 565, loss = 0.32166452\n",
      "Iteration 566, loss = 0.32298058\n",
      "Iteration 567, loss = 0.32170143\n",
      "Iteration 568, loss = 0.32213545\n",
      "Iteration 569, loss = 0.32264153\n",
      "Iteration 570, loss = 0.32134407\n",
      "Iteration 571, loss = 0.32144875\n",
      "Iteration 572, loss = 0.32184294\n",
      "Iteration 573, loss = 0.32041571\n",
      "Iteration 574, loss = 0.32106548\n",
      "Iteration 575, loss = 0.32001461\n",
      "Iteration 576, loss = 0.32058682\n",
      "Iteration 577, loss = 0.32044934\n",
      "Iteration 578, loss = 0.32050808\n",
      "Iteration 579, loss = 0.32154167\n",
      "Iteration 580, loss = 0.32101575\n",
      "Iteration 581, loss = 0.32110893\n",
      "Iteration 582, loss = 0.31975594\n",
      "Iteration 583, loss = 0.32024683\n",
      "Iteration 584, loss = 0.31955459\n",
      "Iteration 585, loss = 0.31855156\n",
      "Iteration 586, loss = 0.32084380\n",
      "Iteration 587, loss = 0.31958670\n",
      "Iteration 588, loss = 0.31979195\n",
      "Iteration 589, loss = 0.31867138\n",
      "Iteration 590, loss = 0.31884417\n",
      "Iteration 591, loss = 0.31810729\n",
      "Iteration 592, loss = 0.31894186\n",
      "Iteration 593, loss = 0.31810034\n",
      "Iteration 594, loss = 0.31927805\n",
      "Iteration 595, loss = 0.31789814\n",
      "Iteration 596, loss = 0.31852375\n",
      "Iteration 597, loss = 0.31817938\n",
      "Iteration 598, loss = 0.31866548\n",
      "Iteration 599, loss = 0.31825965\n",
      "Iteration 600, loss = 0.31875795\n",
      "Iteration 601, loss = 0.31840646\n",
      "Iteration 602, loss = 0.31744821\n",
      "Iteration 603, loss = 0.31757785\n",
      "Iteration 604, loss = 0.31677227\n",
      "Iteration 605, loss = 0.31797761\n",
      "Iteration 606, loss = 0.31810315\n",
      "Iteration 607, loss = 0.31658565\n",
      "Iteration 608, loss = 0.31736086\n",
      "Iteration 609, loss = 0.31664251\n",
      "Iteration 610, loss = 0.31642430\n",
      "Iteration 611, loss = 0.31671551\n",
      "Iteration 612, loss = 0.31628236\n",
      "Iteration 613, loss = 0.31701805\n",
      "Iteration 614, loss = 0.31603397\n",
      "Iteration 615, loss = 0.31680426\n",
      "Iteration 616, loss = 0.31686012\n",
      "Iteration 617, loss = 0.31620236\n",
      "Iteration 618, loss = 0.31573418\n",
      "Iteration 619, loss = 0.31603258\n",
      "Iteration 620, loss = 0.31608019\n",
      "Iteration 621, loss = 0.31536340\n",
      "Iteration 622, loss = 0.31467670\n",
      "Iteration 623, loss = 0.31454311\n",
      "Iteration 624, loss = 0.31507207\n",
      "Iteration 625, loss = 0.31471136\n",
      "Iteration 626, loss = 0.31373601\n",
      "Iteration 627, loss = 0.31460214\n",
      "Iteration 628, loss = 0.31368483\n",
      "Iteration 629, loss = 0.31446607\n",
      "Iteration 630, loss = 0.31489869\n",
      "Iteration 631, loss = 0.31435316\n",
      "Iteration 632, loss = 0.31476612\n",
      "Iteration 633, loss = 0.31407467\n",
      "Iteration 634, loss = 0.31468657\n",
      "Iteration 635, loss = 0.31451260\n",
      "Iteration 636, loss = 0.31344511\n",
      "Iteration 637, loss = 0.31236680\n",
      "Iteration 638, loss = 0.31343452\n",
      "Iteration 639, loss = 0.31345265\n",
      "Iteration 640, loss = 0.31373261\n",
      "Iteration 641, loss = 0.31278831\n",
      "Iteration 642, loss = 0.31278812\n",
      "Iteration 643, loss = 0.31117861\n",
      "Iteration 644, loss = 0.31184773\n",
      "Iteration 645, loss = 0.31244189\n",
      "Iteration 646, loss = 0.31260697\n",
      "Iteration 647, loss = 0.31253734\n",
      "Iteration 648, loss = 0.31246451\n",
      "Iteration 649, loss = 0.31134252\n",
      "Iteration 650, loss = 0.31022172\n",
      "Iteration 651, loss = 0.31079375\n",
      "Iteration 652, loss = 0.31229467\n",
      "Iteration 653, loss = 0.31071421\n",
      "Iteration 654, loss = 0.31322451\n",
      "Iteration 655, loss = 0.31108559\n",
      "Iteration 656, loss = 0.31034505\n",
      "Iteration 657, loss = 0.31136747\n",
      "Iteration 658, loss = 0.30972668\n",
      "Iteration 659, loss = 0.31066325\n",
      "Iteration 660, loss = 0.31059426\n",
      "Iteration 661, loss = 0.31041720\n",
      "Iteration 662, loss = 0.31105980\n",
      "Iteration 663, loss = 0.31051715\n",
      "Iteration 664, loss = 0.31002423\n",
      "Iteration 665, loss = 0.30974097\n",
      "Iteration 666, loss = 0.30986319\n",
      "Iteration 667, loss = 0.30962136\n",
      "Iteration 668, loss = 0.30932222\n",
      "Iteration 669, loss = 0.30933677\n",
      "Iteration 670, loss = 0.31038015\n",
      "Iteration 671, loss = 0.30956150\n",
      "Iteration 672, loss = 0.30818957\n",
      "Iteration 673, loss = 0.30894871\n",
      "Iteration 674, loss = 0.30814645\n",
      "Iteration 675, loss = 0.30887926\n",
      "Iteration 676, loss = 0.30866338\n",
      "Iteration 677, loss = 0.30728197\n",
      "Iteration 678, loss = 0.30867795\n",
      "Iteration 679, loss = 0.30867853\n",
      "Iteration 680, loss = 0.30806630\n",
      "Iteration 681, loss = 0.30845362\n",
      "Iteration 682, loss = 0.30790759\n",
      "Iteration 683, loss = 0.30786727\n",
      "Iteration 684, loss = 0.30812764\n",
      "Iteration 685, loss = 0.30698608\n",
      "Iteration 686, loss = 0.30889242\n",
      "Iteration 687, loss = 0.30682651\n",
      "Iteration 688, loss = 0.30712653\n",
      "Iteration 689, loss = 0.30739275\n",
      "Iteration 690, loss = 0.30743967\n",
      "Iteration 691, loss = 0.30709433\n",
      "Iteration 692, loss = 0.30780545\n",
      "Iteration 693, loss = 0.30766163\n",
      "Iteration 694, loss = 0.30682928\n",
      "Iteration 695, loss = 0.30702979\n",
      "Iteration 696, loss = 0.30555566\n",
      "Iteration 697, loss = 0.30542244\n",
      "Iteration 698, loss = 0.30593534\n",
      "Iteration 699, loss = 0.30728446\n",
      "Iteration 700, loss = 0.30563857\n",
      "Iteration 701, loss = 0.30543036\n",
      "Iteration 702, loss = 0.30566944\n",
      "Iteration 703, loss = 0.30771805\n",
      "Iteration 704, loss = 0.30429051\n",
      "Iteration 705, loss = 0.30500751\n",
      "Iteration 706, loss = 0.30538442\n",
      "Iteration 707, loss = 0.30535839\n",
      "Iteration 708, loss = 0.30530083\n",
      "Iteration 709, loss = 0.30501175\n",
      "Iteration 710, loss = 0.30465545\n",
      "Iteration 711, loss = 0.30494486\n",
      "Iteration 712, loss = 0.30529779\n",
      "Iteration 713, loss = 0.30527082\n",
      "Iteration 714, loss = 0.30484066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [06:03<02:11, 32.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 715, loss = 0.30453215\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57435595\n",
      "Iteration 2, loss = 0.50904551\n",
      "Iteration 3, loss = 0.48384771\n",
      "Iteration 4, loss = 0.47092431\n",
      "Iteration 5, loss = 0.46449134\n",
      "Iteration 6, loss = 0.46075133\n",
      "Iteration 7, loss = 0.45848382\n",
      "Iteration 8, loss = 0.45652167\n",
      "Iteration 9, loss = 0.45507916\n",
      "Iteration 10, loss = 0.45379013\n",
      "Iteration 11, loss = 0.45277421\n",
      "Iteration 12, loss = 0.45201757\n",
      "Iteration 13, loss = 0.45100159\n",
      "Iteration 14, loss = 0.45027510\n",
      "Iteration 15, loss = 0.44953909\n",
      "Iteration 16, loss = 0.44902229\n",
      "Iteration 17, loss = 0.44804778\n",
      "Iteration 18, loss = 0.44787452\n",
      "Iteration 19, loss = 0.44704856\n",
      "Iteration 20, loss = 0.44662590\n",
      "Iteration 21, loss = 0.44598081\n",
      "Iteration 22, loss = 0.44562239\n",
      "Iteration 23, loss = 0.44520314\n",
      "Iteration 24, loss = 0.44476689\n",
      "Iteration 25, loss = 0.44421667\n",
      "Iteration 26, loss = 0.44403079\n",
      "Iteration 27, loss = 0.44342540\n",
      "Iteration 28, loss = 0.44304835\n",
      "Iteration 29, loss = 0.44244289\n",
      "Iteration 30, loss = 0.44217827\n",
      "Iteration 31, loss = 0.44174452\n",
      "Iteration 32, loss = 0.44113945\n",
      "Iteration 33, loss = 0.44108524\n",
      "Iteration 34, loss = 0.44074061\n",
      "Iteration 35, loss = 0.44021619\n",
      "Iteration 36, loss = 0.43997451\n",
      "Iteration 37, loss = 0.43944215\n",
      "Iteration 38, loss = 0.43934149\n",
      "Iteration 39, loss = 0.43880388\n",
      "Iteration 40, loss = 0.43843834\n",
      "Iteration 41, loss = 0.43793738\n",
      "Iteration 42, loss = 0.43770502\n",
      "Iteration 43, loss = 0.43741610\n",
      "Iteration 44, loss = 0.43704885\n",
      "Iteration 45, loss = 0.43691210\n",
      "Iteration 46, loss = 0.43614398\n",
      "Iteration 47, loss = 0.43619544\n",
      "Iteration 48, loss = 0.43585703\n",
      "Iteration 49, loss = 0.43562392\n",
      "Iteration 50, loss = 0.43501222\n",
      "Iteration 51, loss = 0.43465123\n",
      "Iteration 52, loss = 0.43484968\n",
      "Iteration 53, loss = 0.43451418\n",
      "Iteration 54, loss = 0.43397262\n",
      "Iteration 55, loss = 0.43380320\n",
      "Iteration 56, loss = 0.43330944\n",
      "Iteration 57, loss = 0.43293492\n",
      "Iteration 58, loss = 0.43266724\n",
      "Iteration 59, loss = 0.43260002\n",
      "Iteration 60, loss = 0.43208604\n",
      "Iteration 61, loss = 0.43201040\n",
      "Iteration 62, loss = 0.43171734\n",
      "Iteration 63, loss = 0.43135553\n",
      "Iteration 64, loss = 0.43105708\n",
      "Iteration 65, loss = 0.43060131\n",
      "Iteration 66, loss = 0.43071900\n",
      "Iteration 67, loss = 0.43008076\n",
      "Iteration 68, loss = 0.42982549\n",
      "Iteration 69, loss = 0.42941921\n",
      "Iteration 70, loss = 0.42908821\n",
      "Iteration 71, loss = 0.42881402\n",
      "Iteration 72, loss = 0.42860959\n",
      "Iteration 73, loss = 0.42829020\n",
      "Iteration 74, loss = 0.42817732\n",
      "Iteration 75, loss = 0.42794187\n",
      "Iteration 76, loss = 0.42763621\n",
      "Iteration 77, loss = 0.42748543\n",
      "Iteration 78, loss = 0.42707997\n",
      "Iteration 79, loss = 0.42683976\n",
      "Iteration 80, loss = 0.42649855\n",
      "Iteration 81, loss = 0.42626663\n",
      "Iteration 82, loss = 0.42593426\n",
      "Iteration 83, loss = 0.42591014\n",
      "Iteration 84, loss = 0.42546513\n",
      "Iteration 85, loss = 0.42497917\n",
      "Iteration 86, loss = 0.42506482\n",
      "Iteration 87, loss = 0.42469700\n",
      "Iteration 88, loss = 0.42416893\n",
      "Iteration 89, loss = 0.42413902\n",
      "Iteration 90, loss = 0.42415261\n",
      "Iteration 91, loss = 0.42335646\n",
      "Iteration 92, loss = 0.42387752\n",
      "Iteration 93, loss = 0.42299116\n",
      "Iteration 94, loss = 0.42360106\n",
      "Iteration 95, loss = 0.42234075\n",
      "Iteration 96, loss = 0.42224580\n",
      "Iteration 97, loss = 0.42205603\n",
      "Iteration 98, loss = 0.42164321\n",
      "Iteration 99, loss = 0.42150932\n",
      "Iteration 100, loss = 0.42105311\n",
      "Iteration 101, loss = 0.42115286\n",
      "Iteration 102, loss = 0.42067298\n",
      "Iteration 103, loss = 0.42027458\n",
      "Iteration 104, loss = 0.42015129\n",
      "Iteration 105, loss = 0.42023933\n",
      "Iteration 106, loss = 0.41958318\n",
      "Iteration 107, loss = 0.41948261\n",
      "Iteration 108, loss = 0.41895694\n",
      "Iteration 109, loss = 0.41893771\n",
      "Iteration 110, loss = 0.41839923\n",
      "Iteration 111, loss = 0.41851623\n",
      "Iteration 112, loss = 0.41808597\n",
      "Iteration 113, loss = 0.41761815\n",
      "Iteration 114, loss = 0.41737726\n",
      "Iteration 115, loss = 0.41708634\n",
      "Iteration 116, loss = 0.41677196\n",
      "Iteration 117, loss = 0.41676832\n",
      "Iteration 118, loss = 0.41644730\n",
      "Iteration 119, loss = 0.41597849\n",
      "Iteration 120, loss = 0.41571711\n",
      "Iteration 121, loss = 0.41557396\n",
      "Iteration 122, loss = 0.41498121\n",
      "Iteration 123, loss = 0.41500474\n",
      "Iteration 124, loss = 0.41530453\n",
      "Iteration 125, loss = 0.41454886\n",
      "Iteration 126, loss = 0.41404903\n",
      "Iteration 127, loss = 0.41442494\n",
      "Iteration 128, loss = 0.41363951\n",
      "Iteration 129, loss = 0.41340727\n",
      "Iteration 130, loss = 0.41312269\n",
      "Iteration 131, loss = 0.41266075\n",
      "Iteration 132, loss = 0.41278264\n",
      "Iteration 133, loss = 0.41304199\n",
      "Iteration 134, loss = 0.41266715\n",
      "Iteration 135, loss = 0.41215350\n",
      "Iteration 136, loss = 0.41116724\n",
      "Iteration 137, loss = 0.41153452\n",
      "Iteration 138, loss = 0.41067942\n",
      "Iteration 139, loss = 0.41057134\n",
      "Iteration 140, loss = 0.41046825\n",
      "Iteration 141, loss = 0.40992822\n",
      "Iteration 142, loss = 0.40989727\n",
      "Iteration 143, loss = 0.41022499\n",
      "Iteration 144, loss = 0.40914366\n",
      "Iteration 145, loss = 0.40909754\n",
      "Iteration 146, loss = 0.40889271\n",
      "Iteration 147, loss = 0.40855944\n",
      "Iteration 148, loss = 0.40853571\n",
      "Iteration 149, loss = 0.40817553\n",
      "Iteration 150, loss = 0.40900550\n",
      "Iteration 151, loss = 0.40713615\n",
      "Iteration 152, loss = 0.40778123\n",
      "Iteration 153, loss = 0.40747474\n",
      "Iteration 154, loss = 0.40705609\n",
      "Iteration 155, loss = 0.40703622\n",
      "Iteration 156, loss = 0.40580276\n",
      "Iteration 157, loss = 0.40554204\n",
      "Iteration 158, loss = 0.40594541\n",
      "Iteration 159, loss = 0.40534703\n",
      "Iteration 160, loss = 0.40506919\n",
      "Iteration 161, loss = 0.40465114\n",
      "Iteration 162, loss = 0.40429365\n",
      "Iteration 163, loss = 0.40489881\n",
      "Iteration 164, loss = 0.40408322\n",
      "Iteration 165, loss = 0.40392520\n",
      "Iteration 166, loss = 0.40425776\n",
      "Iteration 167, loss = 0.40352476\n",
      "Iteration 168, loss = 0.40302305\n",
      "Iteration 169, loss = 0.40287199\n",
      "Iteration 170, loss = 0.40291806\n",
      "Iteration 171, loss = 0.40210895\n",
      "Iteration 172, loss = 0.40215865\n",
      "Iteration 173, loss = 0.40178284\n",
      "Iteration 174, loss = 0.40223376\n",
      "Iteration 175, loss = 0.40112303\n",
      "Iteration 176, loss = 0.40153432\n",
      "Iteration 177, loss = 0.40071819\n",
      "Iteration 178, loss = 0.40036418\n",
      "Iteration 179, loss = 0.39977219\n",
      "Iteration 180, loss = 0.40031631\n",
      "Iteration 181, loss = 0.39938302\n",
      "Iteration 182, loss = 0.39901655\n",
      "Iteration 183, loss = 0.39869537\n",
      "Iteration 184, loss = 0.39923086\n",
      "Iteration 185, loss = 0.39996280\n",
      "Iteration 186, loss = 0.39827842\n",
      "Iteration 187, loss = 0.39747986\n",
      "Iteration 188, loss = 0.39775759\n",
      "Iteration 189, loss = 0.39739650\n",
      "Iteration 190, loss = 0.39715141\n",
      "Iteration 191, loss = 0.39697782\n",
      "Iteration 192, loss = 0.39623821\n",
      "Iteration 193, loss = 0.39624621\n",
      "Iteration 194, loss = 0.39638200\n",
      "Iteration 195, loss = 0.39576578\n",
      "Iteration 196, loss = 0.39568249\n",
      "Iteration 197, loss = 0.39542033\n",
      "Iteration 198, loss = 0.39502264\n",
      "Iteration 199, loss = 0.39471887\n",
      "Iteration 200, loss = 0.39384439\n",
      "Iteration 201, loss = 0.39457345\n",
      "Iteration 202, loss = 0.39422296\n",
      "Iteration 203, loss = 0.39361505\n",
      "Iteration 204, loss = 0.39329405\n",
      "Iteration 205, loss = 0.39321475\n",
      "Iteration 206, loss = 0.39284047\n",
      "Iteration 207, loss = 0.39203899\n",
      "Iteration 208, loss = 0.39199570\n",
      "Iteration 209, loss = 0.39201001\n",
      "Iteration 210, loss = 0.39147155\n",
      "Iteration 211, loss = 0.39083246\n",
      "Iteration 212, loss = 0.39111794\n",
      "Iteration 213, loss = 0.39089749\n",
      "Iteration 214, loss = 0.39120690\n",
      "Iteration 215, loss = 0.39016199\n",
      "Iteration 216, loss = 0.38969884\n",
      "Iteration 217, loss = 0.39001116\n",
      "Iteration 218, loss = 0.38953281\n",
      "Iteration 219, loss = 0.38983806\n",
      "Iteration 220, loss = 0.38868525\n",
      "Iteration 221, loss = 0.38840191\n",
      "Iteration 222, loss = 0.38787194\n",
      "Iteration 223, loss = 0.38842361\n",
      "Iteration 224, loss = 0.38725781\n",
      "Iteration 225, loss = 0.38771004\n",
      "Iteration 226, loss = 0.38720696\n",
      "Iteration 227, loss = 0.38744105\n",
      "Iteration 228, loss = 0.38637537\n",
      "Iteration 229, loss = 0.38610268\n",
      "Iteration 230, loss = 0.38667910\n",
      "Iteration 231, loss = 0.38566469\n",
      "Iteration 232, loss = 0.38566773\n",
      "Iteration 233, loss = 0.38564038\n",
      "Iteration 234, loss = 0.38539895\n",
      "Iteration 235, loss = 0.38565742\n",
      "Iteration 236, loss = 0.38442523\n",
      "Iteration 237, loss = 0.38426722\n",
      "Iteration 238, loss = 0.38386286\n",
      "Iteration 239, loss = 0.38405692\n",
      "Iteration 240, loss = 0.38362193\n",
      "Iteration 241, loss = 0.38333595\n",
      "Iteration 242, loss = 0.38319592\n",
      "Iteration 243, loss = 0.38298246\n",
      "Iteration 244, loss = 0.38260521\n",
      "Iteration 245, loss = 0.38153709\n",
      "Iteration 246, loss = 0.38267833\n",
      "Iteration 247, loss = 0.38159175\n",
      "Iteration 248, loss = 0.38221484\n",
      "Iteration 249, loss = 0.38121350\n",
      "Iteration 250, loss = 0.38078493\n",
      "Iteration 251, loss = 0.38066011\n",
      "Iteration 252, loss = 0.37994214\n",
      "Iteration 253, loss = 0.37973677\n",
      "Iteration 254, loss = 0.37978036\n",
      "Iteration 255, loss = 0.37950156\n",
      "Iteration 256, loss = 0.37922790\n",
      "Iteration 257, loss = 0.37876244\n",
      "Iteration 258, loss = 0.37821458\n",
      "Iteration 259, loss = 0.37862325\n",
      "Iteration 260, loss = 0.37795446\n",
      "Iteration 261, loss = 0.37765102\n",
      "Iteration 262, loss = 0.37763127\n",
      "Iteration 263, loss = 0.37751193\n",
      "Iteration 264, loss = 0.37731474\n",
      "Iteration 265, loss = 0.37710360\n",
      "Iteration 266, loss = 0.37659979\n",
      "Iteration 267, loss = 0.37619952\n",
      "Iteration 268, loss = 0.37660270\n",
      "Iteration 269, loss = 0.37595373\n",
      "Iteration 270, loss = 0.37533517\n",
      "Iteration 271, loss = 0.37482050\n",
      "Iteration 272, loss = 0.37484580\n",
      "Iteration 273, loss = 0.37428550\n",
      "Iteration 274, loss = 0.37519836\n",
      "Iteration 275, loss = 0.37367258\n",
      "Iteration 276, loss = 0.37425532\n",
      "Iteration 277, loss = 0.37350020\n",
      "Iteration 278, loss = 0.37378418\n",
      "Iteration 279, loss = 0.37309839\n",
      "Iteration 280, loss = 0.37296023\n",
      "Iteration 281, loss = 0.37271459\n",
      "Iteration 282, loss = 0.37248718\n",
      "Iteration 283, loss = 0.37199688\n",
      "Iteration 284, loss = 0.37217765\n",
      "Iteration 285, loss = 0.37171248\n",
      "Iteration 286, loss = 0.37221424\n",
      "Iteration 287, loss = 0.37138013\n",
      "Iteration 288, loss = 0.37080944\n",
      "Iteration 289, loss = 0.37034327\n",
      "Iteration 290, loss = 0.37113731\n",
      "Iteration 291, loss = 0.37033140\n",
      "Iteration 292, loss = 0.36983662\n",
      "Iteration 293, loss = 0.36963412\n",
      "Iteration 294, loss = 0.36951892\n",
      "Iteration 295, loss = 0.36921940\n",
      "Iteration 296, loss = 0.36980071\n",
      "Iteration 297, loss = 0.36939846\n",
      "Iteration 298, loss = 0.36809088\n",
      "Iteration 299, loss = 0.36792556\n",
      "Iteration 300, loss = 0.36785760\n",
      "Iteration 301, loss = 0.36821163\n",
      "Iteration 302, loss = 0.36715014\n",
      "Iteration 303, loss = 0.36701992\n",
      "Iteration 304, loss = 0.36633044\n",
      "Iteration 305, loss = 0.36714804\n",
      "Iteration 306, loss = 0.36674911\n",
      "Iteration 307, loss = 0.36557701\n",
      "Iteration 308, loss = 0.36566844\n",
      "Iteration 309, loss = 0.36556928\n",
      "Iteration 310, loss = 0.36538831\n",
      "Iteration 311, loss = 0.36559719\n",
      "Iteration 312, loss = 0.36574919\n",
      "Iteration 313, loss = 0.36453680\n",
      "Iteration 314, loss = 0.36453608\n",
      "Iteration 315, loss = 0.36460536\n",
      "Iteration 316, loss = 0.36408658\n",
      "Iteration 317, loss = 0.36417306\n",
      "Iteration 318, loss = 0.36411849\n",
      "Iteration 319, loss = 0.36329918\n",
      "Iteration 320, loss = 0.36299152\n",
      "Iteration 321, loss = 0.36281917\n",
      "Iteration 322, loss = 0.36218068\n",
      "Iteration 323, loss = 0.36271086\n",
      "Iteration 324, loss = 0.36173080\n",
      "Iteration 325, loss = 0.36176498\n",
      "Iteration 326, loss = 0.36141371\n",
      "Iteration 327, loss = 0.36117500\n",
      "Iteration 328, loss = 0.36094266\n",
      "Iteration 329, loss = 0.36061666\n",
      "Iteration 330, loss = 0.36129068\n",
      "Iteration 331, loss = 0.36062252\n",
      "Iteration 332, loss = 0.35917459\n",
      "Iteration 333, loss = 0.35904945\n",
      "Iteration 334, loss = 0.35940951\n",
      "Iteration 335, loss = 0.35898063\n",
      "Iteration 336, loss = 0.35961437\n",
      "Iteration 337, loss = 0.35852356\n",
      "Iteration 338, loss = 0.35837448\n",
      "Iteration 339, loss = 0.35725713\n",
      "Iteration 340, loss = 0.35787151\n",
      "Iteration 341, loss = 0.35724789\n",
      "Iteration 342, loss = 0.35724838\n",
      "Iteration 343, loss = 0.35615786\n",
      "Iteration 344, loss = 0.35663093\n",
      "Iteration 345, loss = 0.35634072\n",
      "Iteration 346, loss = 0.35709809\n",
      "Iteration 347, loss = 0.35620507\n",
      "Iteration 348, loss = 0.35518019\n",
      "Iteration 349, loss = 0.35600030\n",
      "Iteration 350, loss = 0.35516850\n",
      "Iteration 351, loss = 0.35508646\n",
      "Iteration 352, loss = 0.35504147\n",
      "Iteration 353, loss = 0.35530883\n",
      "Iteration 354, loss = 0.35447782\n",
      "Iteration 355, loss = 0.35356862\n",
      "Iteration 356, loss = 0.35394569\n",
      "Iteration 357, loss = 0.35410084\n",
      "Iteration 358, loss = 0.35352202\n",
      "Iteration 359, loss = 0.35310383\n",
      "Iteration 360, loss = 0.35266294\n",
      "Iteration 361, loss = 0.35292311\n",
      "Iteration 362, loss = 0.35237893\n",
      "Iteration 363, loss = 0.35226804\n",
      "Iteration 364, loss = 0.35129347\n",
      "Iteration 365, loss = 0.35179825\n",
      "Iteration 366, loss = 0.35202743\n",
      "Iteration 367, loss = 0.35144764\n",
      "Iteration 368, loss = 0.35115481\n",
      "Iteration 369, loss = 0.35079662\n",
      "Iteration 370, loss = 0.35087709\n",
      "Iteration 371, loss = 0.35152151\n",
      "Iteration 372, loss = 0.34974123\n",
      "Iteration 373, loss = 0.35154206\n",
      "Iteration 374, loss = 0.35023133\n",
      "Iteration 375, loss = 0.35089026\n",
      "Iteration 376, loss = 0.34879256\n",
      "Iteration 377, loss = 0.34947373\n",
      "Iteration 378, loss = 0.34912846\n",
      "Iteration 379, loss = 0.34821403\n",
      "Iteration 380, loss = 0.34910368\n",
      "Iteration 381, loss = 0.34852370\n",
      "Iteration 382, loss = 0.34884909\n",
      "Iteration 383, loss = 0.34783667\n",
      "Iteration 384, loss = 0.34726975\n",
      "Iteration 385, loss = 0.34786452\n",
      "Iteration 386, loss = 0.34697899\n",
      "Iteration 387, loss = 0.34612990\n",
      "Iteration 388, loss = 0.34664839\n",
      "Iteration 389, loss = 0.34578053\n",
      "Iteration 390, loss = 0.34702039\n",
      "Iteration 391, loss = 0.34545928\n",
      "Iteration 392, loss = 0.34535642\n",
      "Iteration 393, loss = 0.34473486\n",
      "Iteration 394, loss = 0.34550807\n",
      "Iteration 395, loss = 0.34425289\n",
      "Iteration 396, loss = 0.34518764\n",
      "Iteration 397, loss = 0.34423064\n",
      "Iteration 398, loss = 0.34416309\n",
      "Iteration 399, loss = 0.34399286\n",
      "Iteration 400, loss = 0.34392605\n",
      "Iteration 401, loss = 0.34430114\n",
      "Iteration 402, loss = 0.34392191\n",
      "Iteration 403, loss = 0.34280355\n",
      "Iteration 404, loss = 0.34315934\n",
      "Iteration 405, loss = 0.34301023\n",
      "Iteration 406, loss = 0.34278786\n",
      "Iteration 407, loss = 0.34275988\n",
      "Iteration 408, loss = 0.34289052\n",
      "Iteration 409, loss = 0.34156766\n",
      "Iteration 410, loss = 0.34233680\n",
      "Iteration 411, loss = 0.34200244\n",
      "Iteration 412, loss = 0.34169873\n",
      "Iteration 413, loss = 0.34209717\n",
      "Iteration 414, loss = 0.34164102\n",
      "Iteration 415, loss = 0.34126107\n",
      "Iteration 416, loss = 0.34069333\n",
      "Iteration 417, loss = 0.34028486\n",
      "Iteration 418, loss = 0.34030688\n",
      "Iteration 419, loss = 0.34093839\n",
      "Iteration 420, loss = 0.33995303\n",
      "Iteration 421, loss = 0.33915743\n",
      "Iteration 422, loss = 0.33962735\n",
      "Iteration 423, loss = 0.33977964\n",
      "Iteration 424, loss = 0.33861781\n",
      "Iteration 425, loss = 0.33849775\n",
      "Iteration 426, loss = 0.33928434\n",
      "Iteration 427, loss = 0.33805013\n",
      "Iteration 428, loss = 0.33845009\n",
      "Iteration 429, loss = 0.33906461\n",
      "Iteration 430, loss = 0.33795049\n",
      "Iteration 431, loss = 0.33770841\n",
      "Iteration 432, loss = 0.33795894\n",
      "Iteration 433, loss = 0.33624768\n",
      "Iteration 434, loss = 0.33734356\n",
      "Iteration 435, loss = 0.33659690\n",
      "Iteration 436, loss = 0.33664173\n",
      "Iteration 437, loss = 0.33704819\n",
      "Iteration 438, loss = 0.33666037\n",
      "Iteration 439, loss = 0.33611604\n",
      "Iteration 440, loss = 0.33621601\n",
      "Iteration 441, loss = 0.33549754\n",
      "Iteration 442, loss = 0.33524205\n",
      "Iteration 443, loss = 0.33654281\n",
      "Iteration 444, loss = 0.33483070\n",
      "Iteration 445, loss = 0.33499904\n",
      "Iteration 446, loss = 0.33630767\n",
      "Iteration 447, loss = 0.33448339\n",
      "Iteration 448, loss = 0.33475282\n",
      "Iteration 449, loss = 0.33422428\n",
      "Iteration 450, loss = 0.33368062\n",
      "Iteration 451, loss = 0.33320123\n",
      "Iteration 452, loss = 0.33414578\n",
      "Iteration 453, loss = 0.33390382\n",
      "Iteration 454, loss = 0.33351728\n",
      "Iteration 455, loss = 0.33408961\n",
      "Iteration 456, loss = 0.33317148\n",
      "Iteration 457, loss = 0.33188668\n",
      "Iteration 458, loss = 0.33355708\n",
      "Iteration 459, loss = 0.33195131\n",
      "Iteration 460, loss = 0.33152187\n",
      "Iteration 461, loss = 0.33071908\n",
      "Iteration 462, loss = 0.33171543\n",
      "Iteration 463, loss = 0.33270577\n",
      "Iteration 464, loss = 0.33168425\n",
      "Iteration 465, loss = 0.33098412\n",
      "Iteration 466, loss = 0.33116166\n",
      "Iteration 467, loss = 0.33088924\n",
      "Iteration 468, loss = 0.33076909\n",
      "Iteration 469, loss = 0.33110434\n",
      "Iteration 470, loss = 0.33042738\n",
      "Iteration 471, loss = 0.32969438\n",
      "Iteration 472, loss = 0.32950794\n",
      "Iteration 473, loss = 0.32989007\n",
      "Iteration 474, loss = 0.32992993\n",
      "Iteration 475, loss = 0.33038647\n",
      "Iteration 476, loss = 0.32922072\n",
      "Iteration 477, loss = 0.32883293\n",
      "Iteration 478, loss = 0.32894506\n",
      "Iteration 479, loss = 0.32869051\n",
      "Iteration 480, loss = 0.32849457\n",
      "Iteration 481, loss = 0.32878923\n",
      "Iteration 482, loss = 0.32878537\n",
      "Iteration 483, loss = 0.32778983\n",
      "Iteration 484, loss = 0.32792671\n",
      "Iteration 485, loss = 0.32722606\n",
      "Iteration 486, loss = 0.32727794\n",
      "Iteration 487, loss = 0.32754152\n",
      "Iteration 488, loss = 0.32629443\n",
      "Iteration 489, loss = 0.32679933\n",
      "Iteration 490, loss = 0.32645078\n",
      "Iteration 491, loss = 0.32736901\n",
      "Iteration 492, loss = 0.32617809\n",
      "Iteration 493, loss = 0.32629496\n",
      "Iteration 494, loss = 0.32653888\n",
      "Iteration 495, loss = 0.32688736\n",
      "Iteration 496, loss = 0.32649427\n",
      "Iteration 497, loss = 0.32620585\n",
      "Iteration 498, loss = 0.32589338\n",
      "Iteration 499, loss = 0.32507282\n",
      "Iteration 500, loss = 0.32497032\n",
      "Iteration 501, loss = 0.32552017\n",
      "Iteration 502, loss = 0.32472638\n",
      "Iteration 503, loss = 0.32401797\n",
      "Iteration 504, loss = 0.32446789\n",
      "Iteration 505, loss = 0.32484439\n",
      "Iteration 506, loss = 0.32453266\n",
      "Iteration 507, loss = 0.32369096\n",
      "Iteration 508, loss = 0.32361931\n",
      "Iteration 509, loss = 0.32402492\n",
      "Iteration 510, loss = 0.32290965\n",
      "Iteration 511, loss = 0.32334263\n",
      "Iteration 512, loss = 0.32339909\n",
      "Iteration 513, loss = 0.32423007\n",
      "Iteration 514, loss = 0.32313068\n",
      "Iteration 515, loss = 0.32294972\n",
      "Iteration 516, loss = 0.32282776\n",
      "Iteration 517, loss = 0.32221483\n",
      "Iteration 518, loss = 0.32305708\n",
      "Iteration 519, loss = 0.32192892\n",
      "Iteration 520, loss = 0.32202941\n",
      "Iteration 521, loss = 0.32211585\n",
      "Iteration 522, loss = 0.32067645\n",
      "Iteration 523, loss = 0.32133103\n",
      "Iteration 524, loss = 0.32190768\n",
      "Iteration 525, loss = 0.32103253\n",
      "Iteration 526, loss = 0.32110388\n",
      "Iteration 527, loss = 0.32115531\n",
      "Iteration 528, loss = 0.32098234\n",
      "Iteration 529, loss = 0.32021928\n",
      "Iteration 530, loss = 0.32043614\n",
      "Iteration 531, loss = 0.32061084\n",
      "Iteration 532, loss = 0.32062202\n",
      "Iteration 533, loss = 0.31934936\n",
      "Iteration 534, loss = 0.31871053\n",
      "Iteration 535, loss = 0.31895207\n",
      "Iteration 536, loss = 0.31888750\n",
      "Iteration 537, loss = 0.31892493\n",
      "Iteration 538, loss = 0.31815263\n",
      "Iteration 539, loss = 0.31980123\n",
      "Iteration 540, loss = 0.31820305\n",
      "Iteration 541, loss = 0.31924428\n",
      "Iteration 542, loss = 0.31820525\n",
      "Iteration 543, loss = 0.31833835\n",
      "Iteration 544, loss = 0.31793119\n",
      "Iteration 545, loss = 0.31933890\n",
      "Iteration 546, loss = 0.31669574\n",
      "Iteration 547, loss = 0.31799965\n",
      "Iteration 548, loss = 0.31839033\n",
      "Iteration 549, loss = 0.31755869\n",
      "Iteration 550, loss = 0.31689044\n",
      "Iteration 551, loss = 0.31639049\n",
      "Iteration 552, loss = 0.31749002\n",
      "Iteration 553, loss = 0.31686152\n",
      "Iteration 554, loss = 0.31731151\n",
      "Iteration 555, loss = 0.31582150\n",
      "Iteration 556, loss = 0.31570770\n",
      "Iteration 557, loss = 0.31538136\n",
      "Iteration 558, loss = 0.31524289\n",
      "Iteration 559, loss = 0.31632617\n",
      "Iteration 560, loss = 0.31554759\n",
      "Iteration 561, loss = 0.31545873\n",
      "Iteration 562, loss = 0.31571726\n",
      "Iteration 563, loss = 0.31527696\n",
      "Iteration 564, loss = 0.31532635\n",
      "Iteration 565, loss = 0.31563651\n",
      "Iteration 566, loss = 0.31472388\n",
      "Iteration 567, loss = 0.31452626\n",
      "Iteration 568, loss = 0.31389544\n",
      "Iteration 569, loss = 0.31376556\n",
      "Iteration 570, loss = 0.31399560\n",
      "Iteration 571, loss = 0.31438034\n",
      "Iteration 572, loss = 0.31451273\n",
      "Iteration 573, loss = 0.31317770\n",
      "Iteration 574, loss = 0.31426578\n",
      "Iteration 575, loss = 0.31239926\n",
      "Iteration 576, loss = 0.31389974\n",
      "Iteration 577, loss = 0.31268535\n",
      "Iteration 578, loss = 0.31305389\n",
      "Iteration 579, loss = 0.31374773\n",
      "Iteration 580, loss = 0.31172695\n",
      "Iteration 581, loss = 0.31224551\n",
      "Iteration 582, loss = 0.31315735\n",
      "Iteration 583, loss = 0.31231650\n",
      "Iteration 584, loss = 0.31283792\n",
      "Iteration 585, loss = 0.31253541\n",
      "Iteration 586, loss = 0.31171827\n",
      "Iteration 587, loss = 0.31249279\n",
      "Iteration 588, loss = 0.31155645\n",
      "Iteration 589, loss = 0.31119152\n",
      "Iteration 590, loss = 0.31060089\n",
      "Iteration 591, loss = 0.31121011\n",
      "Iteration 592, loss = 0.31140452\n",
      "Iteration 593, loss = 0.31072329\n",
      "Iteration 594, loss = 0.31199435\n",
      "Iteration 595, loss = 0.31161588\n",
      "Iteration 596, loss = 0.31133724\n",
      "Iteration 597, loss = 0.31044530\n",
      "Iteration 598, loss = 0.31056761\n",
      "Iteration 599, loss = 0.31078176\n",
      "Iteration 600, loss = 0.31013230\n",
      "Iteration 601, loss = 0.31118148\n",
      "Iteration 602, loss = 0.31008801\n",
      "Iteration 603, loss = 0.30978431\n",
      "Iteration 604, loss = 0.30940758\n",
      "Iteration 605, loss = 0.30896429\n",
      "Iteration 606, loss = 0.30900025\n",
      "Iteration 607, loss = 0.30945085\n",
      "Iteration 608, loss = 0.30898725\n",
      "Iteration 609, loss = 0.30988255\n",
      "Iteration 610, loss = 0.30892394\n",
      "Iteration 611, loss = 0.30966308\n",
      "Iteration 612, loss = 0.30853412\n",
      "Iteration 613, loss = 0.30778960\n",
      "Iteration 614, loss = 0.30927623\n",
      "Iteration 615, loss = 0.30834899\n",
      "Iteration 616, loss = 0.30849516\n",
      "Iteration 617, loss = 0.30782725\n",
      "Iteration 618, loss = 0.30764960\n",
      "Iteration 619, loss = 0.30840134\n",
      "Iteration 620, loss = 0.30721756\n",
      "Iteration 621, loss = 0.30838444\n",
      "Iteration 622, loss = 0.30742087\n",
      "Iteration 623, loss = 0.30815359\n",
      "Iteration 624, loss = 0.30698076\n",
      "Iteration 625, loss = 0.30637787\n",
      "Iteration 626, loss = 0.30695997\n",
      "Iteration 627, loss = 0.30694077\n",
      "Iteration 628, loss = 0.30579868\n",
      "Iteration 629, loss = 0.30673345\n",
      "Iteration 630, loss = 0.30609266\n",
      "Iteration 631, loss = 0.30621889\n",
      "Iteration 632, loss = 0.30547007\n",
      "Iteration 633, loss = 0.30548866\n",
      "Iteration 634, loss = 0.30522395\n",
      "Iteration 635, loss = 0.30470021\n",
      "Iteration 636, loss = 0.30550949\n",
      "Iteration 637, loss = 0.30495467\n",
      "Iteration 638, loss = 0.30496828\n",
      "Iteration 639, loss = 0.30568980\n",
      "Iteration 640, loss = 0.30486289\n",
      "Iteration 641, loss = 0.30590729\n",
      "Iteration 642, loss = 0.30494837\n",
      "Iteration 643, loss = 0.30495994\n",
      "Iteration 644, loss = 0.30429366\n",
      "Iteration 645, loss = 0.30412323\n",
      "Iteration 646, loss = 0.30438380\n",
      "Iteration 647, loss = 0.30562794\n",
      "Iteration 648, loss = 0.30425888\n",
      "Iteration 649, loss = 0.30362666\n",
      "Iteration 650, loss = 0.30398948\n",
      "Iteration 651, loss = 0.30397726\n",
      "Iteration 652, loss = 0.30303613\n",
      "Iteration 653, loss = 0.30339286\n",
      "Iteration 654, loss = 0.30329190\n",
      "Iteration 655, loss = 0.30194607\n",
      "Iteration 656, loss = 0.30300359\n",
      "Iteration 657, loss = 0.30306403\n",
      "Iteration 658, loss = 0.30219714\n",
      "Iteration 659, loss = 0.30229062\n",
      "Iteration 660, loss = 0.30251941\n",
      "Iteration 661, loss = 0.30221371\n",
      "Iteration 662, loss = 0.30174264\n",
      "Iteration 663, loss = 0.30170289\n",
      "Iteration 664, loss = 0.30158786\n",
      "Iteration 665, loss = 0.30108942\n",
      "Iteration 666, loss = 0.30193178\n",
      "Iteration 667, loss = 0.30262263\n",
      "Iteration 668, loss = 0.30039115\n",
      "Iteration 669, loss = 0.30213321\n",
      "Iteration 670, loss = 0.30132923\n",
      "Iteration 671, loss = 0.30087140\n",
      "Iteration 672, loss = 0.30177030\n",
      "Iteration 673, loss = 0.30090849\n",
      "Iteration 674, loss = 0.30075505\n",
      "Iteration 675, loss = 0.30132990\n",
      "Iteration 676, loss = 0.30121839\n",
      "Iteration 677, loss = 0.29973657\n",
      "Iteration 678, loss = 0.30091010\n",
      "Iteration 679, loss = 0.29899186\n",
      "Iteration 680, loss = 0.29993188\n",
      "Iteration 681, loss = 0.30033777\n",
      "Iteration 682, loss = 0.30070050\n",
      "Iteration 683, loss = 0.29919981\n",
      "Iteration 684, loss = 0.29967530\n",
      "Iteration 685, loss = 0.29949064\n",
      "Iteration 686, loss = 0.29844580\n",
      "Iteration 687, loss = 0.29891214\n",
      "Iteration 688, loss = 0.30064339\n",
      "Iteration 689, loss = 0.29952564\n",
      "Iteration 690, loss = 0.29843253\n",
      "Iteration 691, loss = 0.29877033\n",
      "Iteration 692, loss = 0.29979001\n",
      "Iteration 693, loss = 0.29909796\n",
      "Iteration 694, loss = 0.30006185\n",
      "Iteration 695, loss = 0.29972167\n",
      "Iteration 696, loss = 0.29851236\n",
      "Iteration 697, loss = 0.29741059\n",
      "Iteration 698, loss = 0.29739713\n",
      "Iteration 699, loss = 0.29837899\n",
      "Iteration 700, loss = 0.29758994\n",
      "Iteration 701, loss = 0.29725254\n",
      "Iteration 702, loss = 0.29672096\n",
      "Iteration 703, loss = 0.29771305\n",
      "Iteration 704, loss = 0.29686190\n",
      "Iteration 705, loss = 0.29874221\n",
      "Iteration 706, loss = 0.29751569\n",
      "Iteration 707, loss = 0.29638132\n",
      "Iteration 708, loss = 0.29671409\n",
      "Iteration 709, loss = 0.29719949\n",
      "Iteration 710, loss = 0.29625832\n",
      "Iteration 711, loss = 0.29615016\n",
      "Iteration 712, loss = 0.29630136\n",
      "Iteration 713, loss = 0.29576080\n",
      "Iteration 714, loss = 0.29744032\n",
      "Iteration 715, loss = 0.29554951\n",
      "Iteration 716, loss = 0.29573859\n",
      "Iteration 717, loss = 0.29604436\n",
      "Iteration 718, loss = 0.29523831\n",
      "Iteration 719, loss = 0.29550420\n",
      "Iteration 720, loss = 0.29658798\n",
      "Iteration 721, loss = 0.29642542\n",
      "Iteration 722, loss = 0.29436293\n",
      "Iteration 723, loss = 0.29651637\n",
      "Iteration 724, loss = 0.29533638\n",
      "Iteration 725, loss = 0.29408244\n",
      "Iteration 726, loss = 0.29568466\n",
      "Iteration 727, loss = 0.29585317\n",
      "Iteration 728, loss = 0.29460789\n",
      "Iteration 729, loss = 0.29442699\n",
      "Iteration 730, loss = 0.29506092\n",
      "Iteration 731, loss = 0.29341376\n",
      "Iteration 732, loss = 0.29386453\n",
      "Iteration 733, loss = 0.29389394\n",
      "Iteration 734, loss = 0.29337402\n",
      "Iteration 735, loss = 0.29612574\n",
      "Iteration 736, loss = 0.29468566\n",
      "Iteration 737, loss = 0.29436897\n",
      "Iteration 738, loss = 0.29428944\n",
      "Iteration 739, loss = 0.29383809\n",
      "Iteration 740, loss = 0.29364291\n",
      "Iteration 741, loss = 0.29399673\n",
      "Iteration 742, loss = 0.29206632\n",
      "Iteration 743, loss = 0.29322298\n",
      "Iteration 744, loss = 0.29479224\n",
      "Iteration 745, loss = 0.29284091\n",
      "Iteration 746, loss = 0.29312920\n",
      "Iteration 747, loss = 0.29414140\n",
      "Iteration 748, loss = 0.29405018\n",
      "Iteration 749, loss = 0.29345518\n",
      "Iteration 750, loss = 0.29318267\n",
      "Iteration 751, loss = 0.29265146\n",
      "Iteration 752, loss = 0.29362318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [06:43<01:45, 35.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 753, loss = 0.29283881\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54867392\n",
      "Iteration 2, loss = 0.49101899\n",
      "Iteration 3, loss = 0.47265620\n",
      "Iteration 4, loss = 0.46497387\n",
      "Iteration 5, loss = 0.46125985\n",
      "Iteration 6, loss = 0.45863447\n",
      "Iteration 7, loss = 0.45679642\n",
      "Iteration 8, loss = 0.45544719\n",
      "Iteration 9, loss = 0.45426324\n",
      "Iteration 10, loss = 0.45314708\n",
      "Iteration 11, loss = 0.45247459\n",
      "Iteration 12, loss = 0.45151298\n",
      "Iteration 13, loss = 0.45075169\n",
      "Iteration 14, loss = 0.45014299\n",
      "Iteration 15, loss = 0.44925715\n",
      "Iteration 16, loss = 0.44850306\n",
      "Iteration 17, loss = 0.44826291\n",
      "Iteration 18, loss = 0.44757278\n",
      "Iteration 19, loss = 0.44687349\n",
      "Iteration 20, loss = 0.44618683\n",
      "Iteration 21, loss = 0.44577006\n",
      "Iteration 22, loss = 0.44495925\n",
      "Iteration 23, loss = 0.44476843\n",
      "Iteration 24, loss = 0.44396470\n",
      "Iteration 25, loss = 0.44349792\n",
      "Iteration 26, loss = 0.44299610\n",
      "Iteration 27, loss = 0.44239762\n",
      "Iteration 28, loss = 0.44193716\n",
      "Iteration 29, loss = 0.44134025\n",
      "Iteration 30, loss = 0.44085065\n",
      "Iteration 31, loss = 0.44045326\n",
      "Iteration 32, loss = 0.44003377\n",
      "Iteration 33, loss = 0.43937992\n",
      "Iteration 34, loss = 0.43892366\n",
      "Iteration 35, loss = 0.43885721\n",
      "Iteration 36, loss = 0.43836994\n",
      "Iteration 37, loss = 0.43812431\n",
      "Iteration 38, loss = 0.43742356\n",
      "Iteration 39, loss = 0.43740358\n",
      "Iteration 40, loss = 0.43677832\n",
      "Iteration 41, loss = 0.43625878\n",
      "Iteration 42, loss = 0.43595998\n",
      "Iteration 43, loss = 0.43573847\n",
      "Iteration 44, loss = 0.43528936\n",
      "Iteration 45, loss = 0.43521028\n",
      "Iteration 46, loss = 0.43457789\n",
      "Iteration 47, loss = 0.43422008\n",
      "Iteration 48, loss = 0.43403228\n",
      "Iteration 49, loss = 0.43358337\n",
      "Iteration 50, loss = 0.43339011\n",
      "Iteration 51, loss = 0.43303188\n",
      "Iteration 52, loss = 0.43262958\n",
      "Iteration 53, loss = 0.43257120\n",
      "Iteration 54, loss = 0.43204696\n",
      "Iteration 55, loss = 0.43167020\n",
      "Iteration 56, loss = 0.43141453\n",
      "Iteration 57, loss = 0.43105014\n",
      "Iteration 58, loss = 0.43100463\n",
      "Iteration 59, loss = 0.43067098\n",
      "Iteration 60, loss = 0.43005316\n",
      "Iteration 61, loss = 0.43006961\n",
      "Iteration 62, loss = 0.42973892\n",
      "Iteration 63, loss = 0.42949534\n",
      "Iteration 64, loss = 0.42935702\n",
      "Iteration 65, loss = 0.42910023\n",
      "Iteration 66, loss = 0.42855589\n",
      "Iteration 67, loss = 0.42831090\n",
      "Iteration 68, loss = 0.42839595\n",
      "Iteration 69, loss = 0.42773375\n",
      "Iteration 70, loss = 0.42720380\n",
      "Iteration 71, loss = 0.42694946\n",
      "Iteration 72, loss = 0.42691997\n",
      "Iteration 73, loss = 0.42668896\n",
      "Iteration 74, loss = 0.42585267\n",
      "Iteration 75, loss = 0.42583154\n",
      "Iteration 76, loss = 0.42594013\n",
      "Iteration 77, loss = 0.42553872\n",
      "Iteration 78, loss = 0.42520931\n",
      "Iteration 79, loss = 0.42485977\n",
      "Iteration 80, loss = 0.42441424\n",
      "Iteration 81, loss = 0.42441614\n",
      "Iteration 82, loss = 0.42382785\n",
      "Iteration 83, loss = 0.42429272\n",
      "Iteration 84, loss = 0.42343011\n",
      "Iteration 85, loss = 0.42341488\n",
      "Iteration 86, loss = 0.42300878\n",
      "Iteration 87, loss = 0.42250341\n",
      "Iteration 88, loss = 0.42253186\n",
      "Iteration 89, loss = 0.42187473\n",
      "Iteration 90, loss = 0.42204692\n",
      "Iteration 91, loss = 0.42142117\n",
      "Iteration 92, loss = 0.42140809\n",
      "Iteration 93, loss = 0.42107798\n",
      "Iteration 94, loss = 0.42066549\n",
      "Iteration 95, loss = 0.42035779\n",
      "Iteration 96, loss = 0.42058913\n",
      "Iteration 97, loss = 0.42011678\n",
      "Iteration 98, loss = 0.41959885\n",
      "Iteration 99, loss = 0.41970634\n",
      "Iteration 100, loss = 0.41918479\n",
      "Iteration 101, loss = 0.41875185\n",
      "Iteration 102, loss = 0.41848646\n",
      "Iteration 103, loss = 0.41836311\n",
      "Iteration 104, loss = 0.41801489\n",
      "Iteration 105, loss = 0.41796651\n",
      "Iteration 106, loss = 0.41784702\n",
      "Iteration 107, loss = 0.41707116\n",
      "Iteration 108, loss = 0.41677542\n",
      "Iteration 109, loss = 0.41649801\n",
      "Iteration 110, loss = 0.41621889\n",
      "Iteration 111, loss = 0.41627162\n",
      "Iteration 112, loss = 0.41594972\n",
      "Iteration 113, loss = 0.41543259\n",
      "Iteration 114, loss = 0.41490111\n",
      "Iteration 115, loss = 0.41527396\n",
      "Iteration 116, loss = 0.41457485\n",
      "Iteration 117, loss = 0.41469413\n",
      "Iteration 118, loss = 0.41454133\n",
      "Iteration 119, loss = 0.41464258\n",
      "Iteration 120, loss = 0.41390175\n",
      "Iteration 121, loss = 0.41313499\n",
      "Iteration 122, loss = 0.41298730\n",
      "Iteration 123, loss = 0.41329952\n",
      "Iteration 124, loss = 0.41292355\n",
      "Iteration 125, loss = 0.41276162\n",
      "Iteration 126, loss = 0.41233974\n",
      "Iteration 127, loss = 0.41207896\n",
      "Iteration 128, loss = 0.41235392\n",
      "Iteration 129, loss = 0.41121287\n",
      "Iteration 130, loss = 0.41125321\n",
      "Iteration 131, loss = 0.41106428\n",
      "Iteration 132, loss = 0.41092702\n",
      "Iteration 133, loss = 0.41038403\n",
      "Iteration 134, loss = 0.40977243\n",
      "Iteration 135, loss = 0.40977154\n",
      "Iteration 136, loss = 0.40986044\n",
      "Iteration 137, loss = 0.40927325\n",
      "Iteration 138, loss = 0.40904412\n",
      "Iteration 139, loss = 0.40851734\n",
      "Iteration 140, loss = 0.40824237\n",
      "Iteration 141, loss = 0.40843706\n",
      "Iteration 142, loss = 0.40794444\n",
      "Iteration 143, loss = 0.40760780\n",
      "Iteration 144, loss = 0.40763024\n",
      "Iteration 145, loss = 0.40747857\n",
      "Iteration 146, loss = 0.40681334\n",
      "Iteration 147, loss = 0.40680910\n",
      "Iteration 148, loss = 0.40651307\n",
      "Iteration 149, loss = 0.40616125\n",
      "Iteration 150, loss = 0.40592286\n",
      "Iteration 151, loss = 0.40596708\n",
      "Iteration 152, loss = 0.40512133\n",
      "Iteration 153, loss = 0.40510383\n",
      "Iteration 154, loss = 0.40541146\n",
      "Iteration 155, loss = 0.40464042\n",
      "Iteration 156, loss = 0.40447284\n",
      "Iteration 157, loss = 0.40380829\n",
      "Iteration 158, loss = 0.40360778\n",
      "Iteration 159, loss = 0.40344371\n",
      "Iteration 160, loss = 0.40344672\n",
      "Iteration 161, loss = 0.40284181\n",
      "Iteration 162, loss = 0.40284557\n",
      "Iteration 163, loss = 0.40259694\n",
      "Iteration 164, loss = 0.40257705\n",
      "Iteration 165, loss = 0.40174143\n",
      "Iteration 166, loss = 0.40158601\n",
      "Iteration 167, loss = 0.40216423\n",
      "Iteration 168, loss = 0.40131281\n",
      "Iteration 169, loss = 0.40095768\n",
      "Iteration 170, loss = 0.40073049\n",
      "Iteration 171, loss = 0.40084561\n",
      "Iteration 172, loss = 0.39998055\n",
      "Iteration 173, loss = 0.39978533\n",
      "Iteration 174, loss = 0.39995102\n",
      "Iteration 175, loss = 0.39925079\n",
      "Iteration 176, loss = 0.39931595\n",
      "Iteration 177, loss = 0.39860084\n",
      "Iteration 178, loss = 0.39873471\n",
      "Iteration 179, loss = 0.39842158\n",
      "Iteration 180, loss = 0.39842634\n",
      "Iteration 181, loss = 0.39774032\n",
      "Iteration 182, loss = 0.39764719\n",
      "Iteration 183, loss = 0.39791649\n",
      "Iteration 184, loss = 0.39693602\n",
      "Iteration 185, loss = 0.39662131\n",
      "Iteration 186, loss = 0.39667389\n",
      "Iteration 187, loss = 0.39643473\n",
      "Iteration 188, loss = 0.39588142\n",
      "Iteration 189, loss = 0.39563168\n",
      "Iteration 190, loss = 0.39537004\n",
      "Iteration 191, loss = 0.39520494\n",
      "Iteration 192, loss = 0.39543548\n",
      "Iteration 193, loss = 0.39471913\n",
      "Iteration 194, loss = 0.39454416\n",
      "Iteration 195, loss = 0.39411152\n",
      "Iteration 196, loss = 0.39448044\n",
      "Iteration 197, loss = 0.39439893\n",
      "Iteration 198, loss = 0.39310438\n",
      "Iteration 199, loss = 0.39284801\n",
      "Iteration 200, loss = 0.39335976\n",
      "Iteration 201, loss = 0.39305015\n",
      "Iteration 202, loss = 0.39227175\n",
      "Iteration 203, loss = 0.39292409\n",
      "Iteration 204, loss = 0.39199621\n",
      "Iteration 205, loss = 0.39177738\n",
      "Iteration 206, loss = 0.39102815\n",
      "Iteration 207, loss = 0.39177785\n",
      "Iteration 208, loss = 0.39115934\n",
      "Iteration 209, loss = 0.39066725\n",
      "Iteration 210, loss = 0.39017915\n",
      "Iteration 211, loss = 0.39004695\n",
      "Iteration 212, loss = 0.38958880\n",
      "Iteration 213, loss = 0.38932208\n",
      "Iteration 214, loss = 0.38903771\n",
      "Iteration 215, loss = 0.38930056\n",
      "Iteration 216, loss = 0.38864281\n",
      "Iteration 217, loss = 0.38872150\n",
      "Iteration 218, loss = 0.38831987\n",
      "Iteration 219, loss = 0.38791608\n",
      "Iteration 220, loss = 0.38770020\n",
      "Iteration 221, loss = 0.38738150\n",
      "Iteration 222, loss = 0.38660091\n",
      "Iteration 223, loss = 0.38724757\n",
      "Iteration 224, loss = 0.38656599\n",
      "Iteration 225, loss = 0.38691969\n",
      "Iteration 226, loss = 0.38606201\n",
      "Iteration 227, loss = 0.38634754\n",
      "Iteration 228, loss = 0.38553552\n",
      "Iteration 229, loss = 0.38586905\n",
      "Iteration 230, loss = 0.38505592\n",
      "Iteration 231, loss = 0.38495043\n",
      "Iteration 232, loss = 0.38495923\n",
      "Iteration 233, loss = 0.38387198\n",
      "Iteration 234, loss = 0.38447381\n",
      "Iteration 235, loss = 0.38420160\n",
      "Iteration 236, loss = 0.38390425\n",
      "Iteration 237, loss = 0.38412614\n",
      "Iteration 238, loss = 0.38290317\n",
      "Iteration 239, loss = 0.38280797\n",
      "Iteration 240, loss = 0.38283048\n",
      "Iteration 241, loss = 0.38285124\n",
      "Iteration 242, loss = 0.38269326\n",
      "Iteration 243, loss = 0.38183411\n",
      "Iteration 244, loss = 0.38140060\n",
      "Iteration 245, loss = 0.38211259\n",
      "Iteration 246, loss = 0.38066073\n",
      "Iteration 247, loss = 0.38091666\n",
      "Iteration 248, loss = 0.38126310\n",
      "Iteration 249, loss = 0.38073770\n",
      "Iteration 250, loss = 0.38022094\n",
      "Iteration 251, loss = 0.37991145\n",
      "Iteration 252, loss = 0.38024254\n",
      "Iteration 253, loss = 0.37916152\n",
      "Iteration 254, loss = 0.37947182\n",
      "Iteration 255, loss = 0.37863412\n",
      "Iteration 256, loss = 0.37913545\n",
      "Iteration 257, loss = 0.37876972\n",
      "Iteration 258, loss = 0.37813678\n",
      "Iteration 259, loss = 0.37749926\n",
      "Iteration 260, loss = 0.37777538\n",
      "Iteration 261, loss = 0.37780694\n",
      "Iteration 262, loss = 0.37757305\n",
      "Iteration 263, loss = 0.37712571\n",
      "Iteration 264, loss = 0.37662264\n",
      "Iteration 265, loss = 0.37661369\n",
      "Iteration 266, loss = 0.37617195\n",
      "Iteration 267, loss = 0.37558158\n",
      "Iteration 268, loss = 0.37557691\n",
      "Iteration 269, loss = 0.37554110\n",
      "Iteration 270, loss = 0.37602145\n",
      "Iteration 271, loss = 0.37522806\n",
      "Iteration 272, loss = 0.37432009\n",
      "Iteration 273, loss = 0.37468820\n",
      "Iteration 274, loss = 0.37443237\n",
      "Iteration 275, loss = 0.37420194\n",
      "Iteration 276, loss = 0.37390122\n",
      "Iteration 277, loss = 0.37407881\n",
      "Iteration 278, loss = 0.37322993\n",
      "Iteration 279, loss = 0.37358896\n",
      "Iteration 280, loss = 0.37343085\n",
      "Iteration 281, loss = 0.37307194\n",
      "Iteration 282, loss = 0.37283212\n",
      "Iteration 283, loss = 0.37219391\n",
      "Iteration 284, loss = 0.37305819\n",
      "Iteration 285, loss = 0.37161248\n",
      "Iteration 286, loss = 0.37139913\n",
      "Iteration 287, loss = 0.37129078\n",
      "Iteration 288, loss = 0.37174687\n",
      "Iteration 289, loss = 0.37074393\n",
      "Iteration 290, loss = 0.37098082\n",
      "Iteration 291, loss = 0.37109147\n",
      "Iteration 292, loss = 0.37059848\n",
      "Iteration 293, loss = 0.37018321\n",
      "Iteration 294, loss = 0.36986997\n",
      "Iteration 295, loss = 0.36942445\n",
      "Iteration 296, loss = 0.36946207\n",
      "Iteration 297, loss = 0.36897240\n",
      "Iteration 298, loss = 0.36853221\n",
      "Iteration 299, loss = 0.36838315\n",
      "Iteration 300, loss = 0.36831321\n",
      "Iteration 301, loss = 0.36808528\n",
      "Iteration 302, loss = 0.36740002\n",
      "Iteration 303, loss = 0.36774795\n",
      "Iteration 304, loss = 0.36751817\n",
      "Iteration 305, loss = 0.36794878\n",
      "Iteration 306, loss = 0.36748759\n",
      "Iteration 307, loss = 0.36676364\n",
      "Iteration 308, loss = 0.36629193\n",
      "Iteration 309, loss = 0.36544809\n",
      "Iteration 310, loss = 0.36569294\n",
      "Iteration 311, loss = 0.36628425\n",
      "Iteration 312, loss = 0.36564996\n",
      "Iteration 313, loss = 0.36540253\n",
      "Iteration 314, loss = 0.36607560\n",
      "Iteration 315, loss = 0.36464215\n",
      "Iteration 316, loss = 0.36513070\n",
      "Iteration 317, loss = 0.36415198\n",
      "Iteration 318, loss = 0.36444259\n",
      "Iteration 319, loss = 0.36403433\n",
      "Iteration 320, loss = 0.36356830\n",
      "Iteration 321, loss = 0.36457452\n",
      "Iteration 322, loss = 0.36372495\n",
      "Iteration 323, loss = 0.36366609\n",
      "Iteration 324, loss = 0.36300452\n",
      "Iteration 325, loss = 0.36238266\n",
      "Iteration 326, loss = 0.36240185\n",
      "Iteration 327, loss = 0.36239766\n",
      "Iteration 328, loss = 0.36267074\n",
      "Iteration 329, loss = 0.36235328\n",
      "Iteration 330, loss = 0.36127448\n",
      "Iteration 331, loss = 0.36216458\n",
      "Iteration 332, loss = 0.36091913\n",
      "Iteration 333, loss = 0.36132966\n",
      "Iteration 334, loss = 0.35991307\n",
      "Iteration 335, loss = 0.36128813\n",
      "Iteration 336, loss = 0.36034374\n",
      "Iteration 337, loss = 0.35964796\n",
      "Iteration 338, loss = 0.35960030\n",
      "Iteration 339, loss = 0.35891693\n",
      "Iteration 340, loss = 0.35835456\n",
      "Iteration 341, loss = 0.35881922\n",
      "Iteration 342, loss = 0.35846087\n",
      "Iteration 343, loss = 0.35950578\n",
      "Iteration 344, loss = 0.35776049\n",
      "Iteration 345, loss = 0.35924974\n",
      "Iteration 346, loss = 0.35752450\n",
      "Iteration 347, loss = 0.35732565\n",
      "Iteration 348, loss = 0.35787911\n",
      "Iteration 349, loss = 0.35725884\n",
      "Iteration 350, loss = 0.35668251\n",
      "Iteration 351, loss = 0.35642952\n",
      "Iteration 352, loss = 0.35752419\n",
      "Iteration 353, loss = 0.35659706\n",
      "Iteration 354, loss = 0.35715737\n",
      "Iteration 355, loss = 0.35618193\n",
      "Iteration 356, loss = 0.35562137\n",
      "Iteration 357, loss = 0.35493933\n",
      "Iteration 358, loss = 0.35490188\n",
      "Iteration 359, loss = 0.35491231\n",
      "Iteration 360, loss = 0.35491765\n",
      "Iteration 361, loss = 0.35453747\n",
      "Iteration 362, loss = 0.35471698\n",
      "Iteration 363, loss = 0.35321104\n",
      "Iteration 364, loss = 0.35397136\n",
      "Iteration 365, loss = 0.35370929\n",
      "Iteration 366, loss = 0.35297852\n",
      "Iteration 367, loss = 0.35322524\n",
      "Iteration 368, loss = 0.35258421\n",
      "Iteration 369, loss = 0.35203581\n",
      "Iteration 370, loss = 0.35316028\n",
      "Iteration 371, loss = 0.35212603\n",
      "Iteration 372, loss = 0.35280812\n",
      "Iteration 373, loss = 0.35249517\n",
      "Iteration 374, loss = 0.35194312\n",
      "Iteration 375, loss = 0.35157530\n",
      "Iteration 376, loss = 0.35068796\n",
      "Iteration 377, loss = 0.35212375\n",
      "Iteration 378, loss = 0.35093166\n",
      "Iteration 379, loss = 0.35053486\n",
      "Iteration 380, loss = 0.35152254\n",
      "Iteration 381, loss = 0.35025730\n",
      "Iteration 382, loss = 0.35074928\n",
      "Iteration 383, loss = 0.35064338\n",
      "Iteration 384, loss = 0.34911090\n",
      "Iteration 385, loss = 0.35015110\n",
      "Iteration 386, loss = 0.34954365\n",
      "Iteration 387, loss = 0.34925678\n",
      "Iteration 388, loss = 0.34950612\n",
      "Iteration 389, loss = 0.34903009\n",
      "Iteration 390, loss = 0.34846777\n",
      "Iteration 391, loss = 0.34824051\n",
      "Iteration 392, loss = 0.34878317\n",
      "Iteration 393, loss = 0.34816029\n",
      "Iteration 394, loss = 0.34737667\n",
      "Iteration 395, loss = 0.34797992\n",
      "Iteration 396, loss = 0.34783432\n",
      "Iteration 397, loss = 0.34620422\n",
      "Iteration 398, loss = 0.34753715\n",
      "Iteration 399, loss = 0.34692470\n",
      "Iteration 400, loss = 0.34643259\n",
      "Iteration 401, loss = 0.34611079\n",
      "Iteration 402, loss = 0.34566221\n",
      "Iteration 403, loss = 0.34566933\n",
      "Iteration 404, loss = 0.34529034\n",
      "Iteration 405, loss = 0.34435729\n",
      "Iteration 406, loss = 0.34454476\n",
      "Iteration 407, loss = 0.34506939\n",
      "Iteration 408, loss = 0.34399744\n",
      "Iteration 409, loss = 0.34467155\n",
      "Iteration 410, loss = 0.34492290\n",
      "Iteration 411, loss = 0.34356241\n",
      "Iteration 412, loss = 0.34407881\n",
      "Iteration 413, loss = 0.34378895\n",
      "Iteration 414, loss = 0.34385846\n",
      "Iteration 415, loss = 0.34369901\n",
      "Iteration 416, loss = 0.34397118\n",
      "Iteration 417, loss = 0.34390554\n",
      "Iteration 418, loss = 0.34295026\n",
      "Iteration 419, loss = 0.34312846\n",
      "Iteration 420, loss = 0.34244471\n",
      "Iteration 421, loss = 0.34245067\n",
      "Iteration 422, loss = 0.34210912\n",
      "Iteration 423, loss = 0.34197083\n",
      "Iteration 424, loss = 0.34230059\n",
      "Iteration 425, loss = 0.34190048\n",
      "Iteration 426, loss = 0.34179737\n",
      "Iteration 427, loss = 0.34114997\n",
      "Iteration 428, loss = 0.34079630\n",
      "Iteration 429, loss = 0.34110253\n",
      "Iteration 430, loss = 0.34210045\n",
      "Iteration 431, loss = 0.34191423\n",
      "Iteration 432, loss = 0.34061535\n",
      "Iteration 433, loss = 0.34008037\n",
      "Iteration 434, loss = 0.33983908\n",
      "Iteration 435, loss = 0.33910300\n",
      "Iteration 436, loss = 0.33934679\n",
      "Iteration 437, loss = 0.33957480\n",
      "Iteration 438, loss = 0.34011590\n",
      "Iteration 439, loss = 0.33884212\n",
      "Iteration 440, loss = 0.33871316\n",
      "Iteration 441, loss = 0.33925471\n",
      "Iteration 442, loss = 0.33828210\n",
      "Iteration 443, loss = 0.33794444\n",
      "Iteration 444, loss = 0.33818491\n",
      "Iteration 445, loss = 0.33778879\n",
      "Iteration 446, loss = 0.33757389\n",
      "Iteration 447, loss = 0.33731748\n",
      "Iteration 448, loss = 0.33818319\n",
      "Iteration 449, loss = 0.33834109\n",
      "Iteration 450, loss = 0.33827194\n",
      "Iteration 451, loss = 0.33691626\n",
      "Iteration 452, loss = 0.33711696\n",
      "Iteration 453, loss = 0.33726922\n",
      "Iteration 454, loss = 0.33661711\n",
      "Iteration 455, loss = 0.33671972\n",
      "Iteration 456, loss = 0.33662638\n",
      "Iteration 457, loss = 0.33539163\n",
      "Iteration 458, loss = 0.33580544\n",
      "Iteration 459, loss = 0.33636882\n",
      "Iteration 460, loss = 0.33660739\n",
      "Iteration 461, loss = 0.33503115\n",
      "Iteration 462, loss = 0.33483437\n",
      "Iteration 463, loss = 0.33475033\n",
      "Iteration 464, loss = 0.33451498\n",
      "Iteration 465, loss = 0.33495769\n",
      "Iteration 466, loss = 0.33484783\n",
      "Iteration 467, loss = 0.33413582\n",
      "Iteration 468, loss = 0.33365193\n",
      "Iteration 469, loss = 0.33363979\n",
      "Iteration 470, loss = 0.33448363\n",
      "Iteration 471, loss = 0.33422356\n",
      "Iteration 472, loss = 0.33367889\n",
      "Iteration 473, loss = 0.33314768\n",
      "Iteration 474, loss = 0.33355140\n",
      "Iteration 475, loss = 0.33306887\n",
      "Iteration 476, loss = 0.33338778\n",
      "Iteration 477, loss = 0.33277732\n",
      "Iteration 478, loss = 0.33210019\n",
      "Iteration 479, loss = 0.33176752\n",
      "Iteration 480, loss = 0.33164808\n",
      "Iteration 481, loss = 0.33223808\n",
      "Iteration 482, loss = 0.33152290\n",
      "Iteration 483, loss = 0.33105093\n",
      "Iteration 484, loss = 0.33179259\n",
      "Iteration 485, loss = 0.33206484\n",
      "Iteration 486, loss = 0.32961181\n",
      "Iteration 487, loss = 0.33255317\n",
      "Iteration 488, loss = 0.33102030\n",
      "Iteration 489, loss = 0.33114808\n",
      "Iteration 490, loss = 0.33002500\n",
      "Iteration 491, loss = 0.33053393\n",
      "Iteration 492, loss = 0.33075360\n",
      "Iteration 493, loss = 0.33045049\n",
      "Iteration 494, loss = 0.33042417\n",
      "Iteration 495, loss = 0.32992404\n",
      "Iteration 496, loss = 0.32990717\n",
      "Iteration 497, loss = 0.32884058\n",
      "Iteration 498, loss = 0.32965034\n",
      "Iteration 499, loss = 0.33006547\n",
      "Iteration 500, loss = 0.32909680\n",
      "Iteration 501, loss = 0.32968137\n",
      "Iteration 502, loss = 0.32914930\n",
      "Iteration 503, loss = 0.32923942\n",
      "Iteration 504, loss = 0.32789623\n",
      "Iteration 505, loss = 0.32735858\n",
      "Iteration 506, loss = 0.32810216\n",
      "Iteration 507, loss = 0.32761618\n",
      "Iteration 508, loss = 0.32729471\n",
      "Iteration 509, loss = 0.32630398\n",
      "Iteration 510, loss = 0.32794225\n",
      "Iteration 511, loss = 0.32762690\n",
      "Iteration 512, loss = 0.32705916\n",
      "Iteration 513, loss = 0.32659081\n",
      "Iteration 514, loss = 0.32641032\n",
      "Iteration 515, loss = 0.32691679\n",
      "Iteration 516, loss = 0.32709410\n",
      "Iteration 517, loss = 0.32676490\n",
      "Iteration 518, loss = 0.32432456\n",
      "Iteration 519, loss = 0.32525605\n",
      "Iteration 520, loss = 0.32615200\n",
      "Iteration 521, loss = 0.32524576\n",
      "Iteration 522, loss = 0.32567728\n",
      "Iteration 523, loss = 0.32610851\n",
      "Iteration 524, loss = 0.32518091\n",
      "Iteration 525, loss = 0.32463280\n",
      "Iteration 526, loss = 0.32581769\n",
      "Iteration 527, loss = 0.32415253\n",
      "Iteration 528, loss = 0.32525959\n",
      "Iteration 529, loss = 0.32476776\n",
      "Iteration 530, loss = 0.32434014\n",
      "Iteration 531, loss = 0.32361813\n",
      "Iteration 532, loss = 0.32446677\n",
      "Iteration 533, loss = 0.32427372\n",
      "Iteration 534, loss = 0.32280723\n",
      "Iteration 535, loss = 0.32380426\n",
      "Iteration 536, loss = 0.32373836\n",
      "Iteration 537, loss = 0.32327211\n",
      "Iteration 538, loss = 0.32262367\n",
      "Iteration 539, loss = 0.32365211\n",
      "Iteration 540, loss = 0.32363760\n",
      "Iteration 541, loss = 0.32380756\n",
      "Iteration 542, loss = 0.32274246\n",
      "Iteration 543, loss = 0.32225506\n",
      "Iteration 544, loss = 0.32197666\n",
      "Iteration 545, loss = 0.32110007\n",
      "Iteration 546, loss = 0.32136396\n",
      "Iteration 547, loss = 0.32316131\n",
      "Iteration 548, loss = 0.32144490\n",
      "Iteration 549, loss = 0.32112649\n",
      "Iteration 550, loss = 0.32130322\n",
      "Iteration 551, loss = 0.32060702\n",
      "Iteration 552, loss = 0.32098626\n",
      "Iteration 553, loss = 0.32041610\n",
      "Iteration 554, loss = 0.31995225\n",
      "Iteration 555, loss = 0.31999671\n",
      "Iteration 556, loss = 0.32117918\n",
      "Iteration 557, loss = 0.31922556\n",
      "Iteration 558, loss = 0.32007598\n",
      "Iteration 559, loss = 0.31903906\n",
      "Iteration 560, loss = 0.32040988\n",
      "Iteration 561, loss = 0.31942231\n",
      "Iteration 562, loss = 0.31910858\n",
      "Iteration 563, loss = 0.31868778\n",
      "Iteration 564, loss = 0.31937504\n",
      "Iteration 565, loss = 0.31889050\n",
      "Iteration 566, loss = 0.31779784\n",
      "Iteration 567, loss = 0.31879667\n",
      "Iteration 568, loss = 0.31927912\n",
      "Iteration 569, loss = 0.31809871\n",
      "Iteration 570, loss = 0.31841242\n",
      "Iteration 571, loss = 0.31921502\n",
      "Iteration 572, loss = 0.31832961\n",
      "Iteration 573, loss = 0.31885253\n",
      "Iteration 574, loss = 0.31697021\n",
      "Iteration 575, loss = 0.31719425\n",
      "Iteration 576, loss = 0.31875218\n",
      "Iteration 577, loss = 0.31627926\n",
      "Iteration 578, loss = 0.31645901\n",
      "Iteration 579, loss = 0.31639227\n",
      "Iteration 580, loss = 0.31629840\n",
      "Iteration 581, loss = 0.31559493\n",
      "Iteration 582, loss = 0.31631842\n",
      "Iteration 583, loss = 0.31613390\n",
      "Iteration 584, loss = 0.31604107\n",
      "Iteration 585, loss = 0.31517121\n",
      "Iteration 586, loss = 0.31653631\n",
      "Iteration 587, loss = 0.31556540\n",
      "Iteration 588, loss = 0.31501401\n",
      "Iteration 589, loss = 0.31595461\n",
      "Iteration 590, loss = 0.31527284\n",
      "Iteration 591, loss = 0.31610636\n",
      "Iteration 592, loss = 0.31499341\n",
      "Iteration 593, loss = 0.31592799\n",
      "Iteration 594, loss = 0.31501172\n",
      "Iteration 595, loss = 0.31412258\n",
      "Iteration 596, loss = 0.31447556\n",
      "Iteration 597, loss = 0.31319443\n",
      "Iteration 598, loss = 0.31494428\n",
      "Iteration 599, loss = 0.31322236\n",
      "Iteration 600, loss = 0.31347405\n",
      "Iteration 601, loss = 0.31434774\n",
      "Iteration 602, loss = 0.31319993\n",
      "Iteration 603, loss = 0.31378251\n",
      "Iteration 604, loss = 0.31300643\n",
      "Iteration 605, loss = 0.31371441\n",
      "Iteration 606, loss = 0.31242193\n",
      "Iteration 607, loss = 0.31289594\n",
      "Iteration 608, loss = 0.31238726\n",
      "Iteration 609, loss = 0.31162214\n",
      "Iteration 610, loss = 0.31142440\n",
      "Iteration 611, loss = 0.31375312\n",
      "Iteration 612, loss = 0.31220531\n",
      "Iteration 613, loss = 0.31284013\n",
      "Iteration 614, loss = 0.31196957\n",
      "Iteration 615, loss = 0.31181327\n",
      "Iteration 616, loss = 0.31111488\n",
      "Iteration 617, loss = 0.31139052\n",
      "Iteration 618, loss = 0.31194557\n",
      "Iteration 619, loss = 0.31158893\n",
      "Iteration 620, loss = 0.31163296\n",
      "Iteration 621, loss = 0.31091238\n",
      "Iteration 622, loss = 0.31070196\n",
      "Iteration 623, loss = 0.31173918\n",
      "Iteration 624, loss = 0.30991766\n",
      "Iteration 625, loss = 0.31235406\n",
      "Iteration 626, loss = 0.30937556\n",
      "Iteration 627, loss = 0.31179902\n",
      "Iteration 628, loss = 0.30916685\n",
      "Iteration 629, loss = 0.30923664\n",
      "Iteration 630, loss = 0.30942860\n",
      "Iteration 631, loss = 0.30922842\n",
      "Iteration 632, loss = 0.30948657\n",
      "Iteration 633, loss = 0.30874922\n",
      "Iteration 634, loss = 0.30969914\n",
      "Iteration 635, loss = 0.30857721\n",
      "Iteration 636, loss = 0.30908378\n",
      "Iteration 637, loss = 0.30897866\n",
      "Iteration 638, loss = 0.30883239\n",
      "Iteration 639, loss = 0.30960398\n",
      "Iteration 640, loss = 0.30870971\n",
      "Iteration 641, loss = 0.30958946\n",
      "Iteration 642, loss = 0.30789474\n",
      "Iteration 643, loss = 0.30772319\n",
      "Iteration 644, loss = 0.30770003\n",
      "Iteration 645, loss = 0.30827336\n",
      "Iteration 646, loss = 0.30796276\n",
      "Iteration 647, loss = 0.30766777\n",
      "Iteration 648, loss = 0.30715487\n",
      "Iteration 649, loss = 0.30832162\n",
      "Iteration 650, loss = 0.30736212\n",
      "Iteration 651, loss = 0.30734883\n",
      "Iteration 652, loss = 0.30768651\n",
      "Iteration 653, loss = 0.30675522\n",
      "Iteration 654, loss = 0.30750484\n",
      "Iteration 655, loss = 0.30728878\n",
      "Iteration 656, loss = 0.30689121\n",
      "Iteration 657, loss = 0.30730226\n",
      "Iteration 658, loss = 0.30736974\n",
      "Iteration 659, loss = 0.30633958\n",
      "Iteration 660, loss = 0.30538166\n",
      "Iteration 661, loss = 0.30585675\n",
      "Iteration 662, loss = 0.30533093\n",
      "Iteration 663, loss = 0.30546928\n",
      "Iteration 664, loss = 0.30494027\n",
      "Iteration 665, loss = 0.30710969\n",
      "Iteration 666, loss = 0.30531817\n",
      "Iteration 667, loss = 0.30439818\n",
      "Iteration 668, loss = 0.30568078\n",
      "Iteration 669, loss = 0.30665562\n",
      "Iteration 670, loss = 0.30528995\n",
      "Iteration 671, loss = 0.30372891\n",
      "Iteration 672, loss = 0.30526795\n",
      "Iteration 673, loss = 0.30537806\n",
      "Iteration 674, loss = 0.30518867\n",
      "Iteration 675, loss = 0.30309474\n",
      "Iteration 676, loss = 0.30586647\n",
      "Iteration 677, loss = 0.30348090\n",
      "Iteration 678, loss = 0.30375510\n",
      "Iteration 679, loss = 0.30538443\n",
      "Iteration 680, loss = 0.30449807\n",
      "Iteration 681, loss = 0.30350939\n",
      "Iteration 682, loss = 0.30377460\n",
      "Iteration 683, loss = 0.30238201\n",
      "Iteration 684, loss = 0.30317878\n",
      "Iteration 685, loss = 0.30277533\n",
      "Iteration 686, loss = 0.30324328\n",
      "Iteration 687, loss = 0.30242693\n",
      "Iteration 688, loss = 0.30122733\n",
      "Iteration 689, loss = 0.30294653\n",
      "Iteration 690, loss = 0.30224013\n",
      "Iteration 691, loss = 0.30122950\n",
      "Iteration 692, loss = 0.30227391\n",
      "Iteration 693, loss = 0.30190324\n",
      "Iteration 694, loss = 0.30172104\n",
      "Iteration 695, loss = 0.30343430\n",
      "Iteration 696, loss = 0.30172621\n",
      "Iteration 697, loss = 0.30062046\n",
      "Iteration 698, loss = 0.30152746\n",
      "Iteration 699, loss = 0.30118151\n",
      "Iteration 700, loss = 0.30195929\n",
      "Iteration 701, loss = 0.30118108\n",
      "Iteration 702, loss = 0.30139268\n",
      "Iteration 703, loss = 0.30161396\n",
      "Iteration 704, loss = 0.30086656\n",
      "Iteration 705, loss = 0.30102738\n",
      "Iteration 706, loss = 0.30086968\n",
      "Iteration 707, loss = 0.30093511\n",
      "Iteration 708, loss = 0.29965418\n",
      "Iteration 709, loss = 0.29987060\n",
      "Iteration 710, loss = 0.29900194\n",
      "Iteration 711, loss = 0.29857933\n",
      "Iteration 712, loss = 0.30018519\n",
      "Iteration 713, loss = 0.30016974\n",
      "Iteration 714, loss = 0.29912533\n",
      "Iteration 715, loss = 0.30195245\n",
      "Iteration 716, loss = 0.30068609\n",
      "Iteration 717, loss = 0.29923124\n",
      "Iteration 718, loss = 0.29946804\n",
      "Iteration 719, loss = 0.29905907\n",
      "Iteration 720, loss = 0.30004540\n",
      "Iteration 721, loss = 0.29940301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [07:26<01:15, 37.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 722, loss = 0.29898595\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.52162890\n",
      "Iteration 2, loss = 0.48773635\n",
      "Iteration 3, loss = 0.47244634\n",
      "Iteration 4, loss = 0.46538565\n",
      "Iteration 5, loss = 0.46153650\n",
      "Iteration 6, loss = 0.45879371\n",
      "Iteration 7, loss = 0.45698687\n",
      "Iteration 8, loss = 0.45525859\n",
      "Iteration 9, loss = 0.45403262\n",
      "Iteration 10, loss = 0.45273889\n",
      "Iteration 11, loss = 0.45200149\n",
      "Iteration 12, loss = 0.45081237\n",
      "Iteration 13, loss = 0.45009367\n",
      "Iteration 14, loss = 0.44941043\n",
      "Iteration 15, loss = 0.44873719\n",
      "Iteration 16, loss = 0.44818306\n",
      "Iteration 17, loss = 0.44718915\n",
      "Iteration 18, loss = 0.44675876\n",
      "Iteration 19, loss = 0.44630594\n",
      "Iteration 20, loss = 0.44560547\n",
      "Iteration 21, loss = 0.44520644\n",
      "Iteration 22, loss = 0.44475278\n",
      "Iteration 23, loss = 0.44425489\n",
      "Iteration 24, loss = 0.44367808\n",
      "Iteration 25, loss = 0.44339721\n",
      "Iteration 26, loss = 0.44307020\n",
      "Iteration 27, loss = 0.44249852\n",
      "Iteration 28, loss = 0.44199554\n",
      "Iteration 29, loss = 0.44171265\n",
      "Iteration 30, loss = 0.44123122\n",
      "Iteration 31, loss = 0.44087680\n",
      "Iteration 32, loss = 0.44027475\n",
      "Iteration 33, loss = 0.44001384\n",
      "Iteration 34, loss = 0.43997865\n",
      "Iteration 35, loss = 0.43927652\n",
      "Iteration 36, loss = 0.43890720\n",
      "Iteration 37, loss = 0.43850036\n",
      "Iteration 38, loss = 0.43811616\n",
      "Iteration 39, loss = 0.43777956\n",
      "Iteration 40, loss = 0.43736938\n",
      "Iteration 41, loss = 0.43698440\n",
      "Iteration 42, loss = 0.43679039\n",
      "Iteration 43, loss = 0.43648830\n",
      "Iteration 44, loss = 0.43583475\n",
      "Iteration 45, loss = 0.43583779\n",
      "Iteration 46, loss = 0.43512310\n",
      "Iteration 47, loss = 0.43480270\n",
      "Iteration 48, loss = 0.43432218\n",
      "Iteration 49, loss = 0.43457299\n",
      "Iteration 50, loss = 0.43381985\n",
      "Iteration 51, loss = 0.43338704\n",
      "Iteration 52, loss = 0.43283168\n",
      "Iteration 53, loss = 0.43279079\n",
      "Iteration 54, loss = 0.43240002\n",
      "Iteration 55, loss = 0.43204965\n",
      "Iteration 56, loss = 0.43173592\n",
      "Iteration 57, loss = 0.43164072\n",
      "Iteration 58, loss = 0.43102481\n",
      "Iteration 59, loss = 0.43065585\n",
      "Iteration 60, loss = 0.43053370\n",
      "Iteration 61, loss = 0.43001190\n",
      "Iteration 62, loss = 0.42973249\n",
      "Iteration 63, loss = 0.42959644\n",
      "Iteration 64, loss = 0.42904733\n",
      "Iteration 65, loss = 0.42882631\n",
      "Iteration 66, loss = 0.42859850\n",
      "Iteration 67, loss = 0.42846206\n",
      "Iteration 68, loss = 0.42797157\n",
      "Iteration 69, loss = 0.42797200\n",
      "Iteration 70, loss = 0.42710472\n",
      "Iteration 71, loss = 0.42691117\n",
      "Iteration 72, loss = 0.42662447\n",
      "Iteration 73, loss = 0.42618840\n",
      "Iteration 74, loss = 0.42624839\n",
      "Iteration 75, loss = 0.42587872\n",
      "Iteration 76, loss = 0.42548904\n",
      "Iteration 77, loss = 0.42520930\n",
      "Iteration 78, loss = 0.42476256\n",
      "Iteration 79, loss = 0.42453745\n",
      "Iteration 80, loss = 0.42397964\n",
      "Iteration 81, loss = 0.42382859\n",
      "Iteration 82, loss = 0.42378927\n",
      "Iteration 83, loss = 0.42365909\n",
      "Iteration 84, loss = 0.42309677\n",
      "Iteration 85, loss = 0.42262571\n",
      "Iteration 86, loss = 0.42242057\n",
      "Iteration 87, loss = 0.42224077\n",
      "Iteration 88, loss = 0.42189446\n",
      "Iteration 89, loss = 0.42164758\n",
      "Iteration 90, loss = 0.42137232\n",
      "Iteration 91, loss = 0.42105437\n",
      "Iteration 92, loss = 0.42104230\n",
      "Iteration 93, loss = 0.42063514\n",
      "Iteration 94, loss = 0.41979605\n",
      "Iteration 95, loss = 0.41984900\n",
      "Iteration 96, loss = 0.41965386\n",
      "Iteration 97, loss = 0.41938900\n",
      "Iteration 98, loss = 0.41884181\n",
      "Iteration 99, loss = 0.41856233\n",
      "Iteration 100, loss = 0.41848442\n",
      "Iteration 101, loss = 0.41841233\n",
      "Iteration 102, loss = 0.41796183\n",
      "Iteration 103, loss = 0.41756173\n",
      "Iteration 104, loss = 0.41712329\n",
      "Iteration 105, loss = 0.41698072\n",
      "Iteration 106, loss = 0.41689080\n",
      "Iteration 107, loss = 0.41628076\n",
      "Iteration 108, loss = 0.41617802\n",
      "Iteration 109, loss = 0.41570726\n",
      "Iteration 110, loss = 0.41587294\n",
      "Iteration 111, loss = 0.41515895\n",
      "Iteration 112, loss = 0.41532868\n",
      "Iteration 113, loss = 0.41506679\n",
      "Iteration 114, loss = 0.41452595\n",
      "Iteration 115, loss = 0.41386395\n",
      "Iteration 116, loss = 0.41376992\n",
      "Iteration 117, loss = 0.41393177\n",
      "Iteration 118, loss = 0.41341371\n",
      "Iteration 119, loss = 0.41330191\n",
      "Iteration 120, loss = 0.41278737\n",
      "Iteration 121, loss = 0.41285522\n",
      "Iteration 122, loss = 0.41197785\n",
      "Iteration 123, loss = 0.41213319\n",
      "Iteration 124, loss = 0.41155058\n",
      "Iteration 125, loss = 0.41145575\n",
      "Iteration 126, loss = 0.41110519\n",
      "Iteration 127, loss = 0.41101269\n",
      "Iteration 128, loss = 0.41090354\n",
      "Iteration 129, loss = 0.41070821\n",
      "Iteration 130, loss = 0.40964284\n",
      "Iteration 131, loss = 0.40967663\n",
      "Iteration 132, loss = 0.40944793\n",
      "Iteration 133, loss = 0.40912193\n",
      "Iteration 134, loss = 0.40884934\n",
      "Iteration 135, loss = 0.40894661\n",
      "Iteration 136, loss = 0.40867396\n",
      "Iteration 137, loss = 0.40811836\n",
      "Iteration 138, loss = 0.40795236\n",
      "Iteration 139, loss = 0.40757135\n",
      "Iteration 140, loss = 0.40746363\n",
      "Iteration 141, loss = 0.40715440\n",
      "Iteration 142, loss = 0.40693677\n",
      "Iteration 143, loss = 0.40610593\n",
      "Iteration 144, loss = 0.40679769\n",
      "Iteration 145, loss = 0.40604309\n",
      "Iteration 146, loss = 0.40559904\n",
      "Iteration 147, loss = 0.40614486\n",
      "Iteration 148, loss = 0.40536638\n",
      "Iteration 149, loss = 0.40493742\n",
      "Iteration 150, loss = 0.40485705\n",
      "Iteration 151, loss = 0.40471380\n",
      "Iteration 152, loss = 0.40400460\n",
      "Iteration 153, loss = 0.40457844\n",
      "Iteration 154, loss = 0.40358296\n",
      "Iteration 155, loss = 0.40359800\n",
      "Iteration 156, loss = 0.40306291\n",
      "Iteration 157, loss = 0.40268312\n",
      "Iteration 158, loss = 0.40273531\n",
      "Iteration 159, loss = 0.40193184\n",
      "Iteration 160, loss = 0.40224970\n",
      "Iteration 161, loss = 0.40163516\n",
      "Iteration 162, loss = 0.40161508\n",
      "Iteration 163, loss = 0.40143861\n",
      "Iteration 164, loss = 0.40112333\n",
      "Iteration 165, loss = 0.40091682\n",
      "Iteration 166, loss = 0.40055814\n",
      "Iteration 167, loss = 0.40027328\n",
      "Iteration 168, loss = 0.40052302\n",
      "Iteration 169, loss = 0.39982259\n",
      "Iteration 170, loss = 0.39995874\n",
      "Iteration 171, loss = 0.39925774\n",
      "Iteration 172, loss = 0.39860185\n",
      "Iteration 173, loss = 0.39851674\n",
      "Iteration 174, loss = 0.39828321\n",
      "Iteration 175, loss = 0.39835999\n",
      "Iteration 176, loss = 0.39735878\n",
      "Iteration 177, loss = 0.39795054\n",
      "Iteration 178, loss = 0.39729024\n",
      "Iteration 179, loss = 0.39656213\n",
      "Iteration 180, loss = 0.39724209\n",
      "Iteration 181, loss = 0.39685176\n",
      "Iteration 182, loss = 0.39636295\n",
      "Iteration 183, loss = 0.39626169\n",
      "Iteration 184, loss = 0.39513274\n",
      "Iteration 185, loss = 0.39568229\n",
      "Iteration 186, loss = 0.39490979\n",
      "Iteration 187, loss = 0.39463390\n",
      "Iteration 188, loss = 0.39463473\n",
      "Iteration 189, loss = 0.39436897\n",
      "Iteration 190, loss = 0.39380554\n",
      "Iteration 191, loss = 0.39366521\n",
      "Iteration 192, loss = 0.39316580\n",
      "Iteration 193, loss = 0.39314879\n",
      "Iteration 194, loss = 0.39330305\n",
      "Iteration 195, loss = 0.39252200\n",
      "Iteration 196, loss = 0.39271526\n",
      "Iteration 197, loss = 0.39153046\n",
      "Iteration 198, loss = 0.39180109\n",
      "Iteration 199, loss = 0.39170764\n",
      "Iteration 200, loss = 0.39108883\n",
      "Iteration 201, loss = 0.39138037\n",
      "Iteration 202, loss = 0.39059401\n",
      "Iteration 203, loss = 0.39043803\n",
      "Iteration 204, loss = 0.39022998\n",
      "Iteration 205, loss = 0.38965322\n",
      "Iteration 206, loss = 0.38992860\n",
      "Iteration 207, loss = 0.38982573\n",
      "Iteration 208, loss = 0.38944852\n",
      "Iteration 209, loss = 0.38890690\n",
      "Iteration 210, loss = 0.38877717\n",
      "Iteration 211, loss = 0.38851856\n",
      "Iteration 212, loss = 0.38813825\n",
      "Iteration 213, loss = 0.38797181\n",
      "Iteration 214, loss = 0.38788085\n",
      "Iteration 215, loss = 0.38763523\n",
      "Iteration 216, loss = 0.38705010\n",
      "Iteration 217, loss = 0.38707095\n",
      "Iteration 218, loss = 0.38668564\n",
      "Iteration 219, loss = 0.38764044\n",
      "Iteration 220, loss = 0.38662796\n",
      "Iteration 221, loss = 0.38502788\n",
      "Iteration 222, loss = 0.38476129\n",
      "Iteration 223, loss = 0.38511569\n",
      "Iteration 224, loss = 0.38520361\n",
      "Iteration 225, loss = 0.38467436\n",
      "Iteration 226, loss = 0.38443844\n",
      "Iteration 227, loss = 0.38397273\n",
      "Iteration 228, loss = 0.38412354\n",
      "Iteration 229, loss = 0.38380290\n",
      "Iteration 230, loss = 0.38331418\n",
      "Iteration 231, loss = 0.38283958\n",
      "Iteration 232, loss = 0.38272796\n",
      "Iteration 233, loss = 0.38267823\n",
      "Iteration 234, loss = 0.38227631\n",
      "Iteration 235, loss = 0.38201145\n",
      "Iteration 236, loss = 0.38141630\n",
      "Iteration 237, loss = 0.38111753\n",
      "Iteration 238, loss = 0.38112413\n",
      "Iteration 239, loss = 0.38133049\n",
      "Iteration 240, loss = 0.38012775\n",
      "Iteration 241, loss = 0.37997371\n",
      "Iteration 242, loss = 0.37997204\n",
      "Iteration 243, loss = 0.37948461\n",
      "Iteration 244, loss = 0.38020921\n",
      "Iteration 245, loss = 0.38026521\n",
      "Iteration 246, loss = 0.37919561\n",
      "Iteration 247, loss = 0.37915137\n",
      "Iteration 248, loss = 0.37804386\n",
      "Iteration 249, loss = 0.37778408\n",
      "Iteration 250, loss = 0.37758277\n",
      "Iteration 251, loss = 0.37776286\n",
      "Iteration 252, loss = 0.37753602\n",
      "Iteration 253, loss = 0.37721026\n",
      "Iteration 254, loss = 0.37710710\n",
      "Iteration 255, loss = 0.37667083\n",
      "Iteration 256, loss = 0.37640440\n",
      "Iteration 257, loss = 0.37589959\n",
      "Iteration 258, loss = 0.37578507\n",
      "Iteration 259, loss = 0.37522159\n",
      "Iteration 260, loss = 0.37523728\n",
      "Iteration 261, loss = 0.37510798\n",
      "Iteration 262, loss = 0.37434021\n",
      "Iteration 263, loss = 0.37471270\n",
      "Iteration 264, loss = 0.37407197\n",
      "Iteration 265, loss = 0.37350013\n",
      "Iteration 266, loss = 0.37406115\n",
      "Iteration 267, loss = 0.37314823\n",
      "Iteration 268, loss = 0.37343223\n",
      "Iteration 269, loss = 0.37274034\n",
      "Iteration 270, loss = 0.37262851\n",
      "Iteration 271, loss = 0.37231377\n",
      "Iteration 272, loss = 0.37205922\n",
      "Iteration 273, loss = 0.37209435\n",
      "Iteration 274, loss = 0.37174012\n",
      "Iteration 275, loss = 0.37133092\n",
      "Iteration 276, loss = 0.37263870\n",
      "Iteration 277, loss = 0.37147906\n",
      "Iteration 278, loss = 0.37071726\n",
      "Iteration 279, loss = 0.37065790\n",
      "Iteration 280, loss = 0.37018124\n",
      "Iteration 281, loss = 0.36989122\n",
      "Iteration 282, loss = 0.36953102\n",
      "Iteration 283, loss = 0.37026602\n",
      "Iteration 284, loss = 0.36991914\n",
      "Iteration 285, loss = 0.36896016\n",
      "Iteration 286, loss = 0.36872950\n",
      "Iteration 287, loss = 0.36863650\n",
      "Iteration 288, loss = 0.36811398\n",
      "Iteration 289, loss = 0.36930089\n",
      "Iteration 290, loss = 0.36770444\n",
      "Iteration 291, loss = 0.36699312\n",
      "Iteration 292, loss = 0.36735493\n",
      "Iteration 293, loss = 0.36716556\n",
      "Iteration 294, loss = 0.36735757\n",
      "Iteration 295, loss = 0.36703219\n",
      "Iteration 296, loss = 0.36660808\n",
      "Iteration 297, loss = 0.36644544\n",
      "Iteration 298, loss = 0.36562600\n",
      "Iteration 299, loss = 0.36478405\n",
      "Iteration 300, loss = 0.36562079\n",
      "Iteration 301, loss = 0.36535068\n",
      "Iteration 302, loss = 0.36450669\n",
      "Iteration 303, loss = 0.36444643\n",
      "Iteration 304, loss = 0.36475294\n",
      "Iteration 305, loss = 0.36381262\n",
      "Iteration 306, loss = 0.36461521\n",
      "Iteration 307, loss = 0.36407272\n",
      "Iteration 308, loss = 0.36316430\n",
      "Iteration 309, loss = 0.36281585\n",
      "Iteration 310, loss = 0.36311108\n",
      "Iteration 311, loss = 0.36227350\n",
      "Iteration 312, loss = 0.36154363\n",
      "Iteration 313, loss = 0.36160342\n",
      "Iteration 314, loss = 0.36229542\n",
      "Iteration 315, loss = 0.36153986\n",
      "Iteration 316, loss = 0.36085946\n",
      "Iteration 317, loss = 0.36050781\n",
      "Iteration 318, loss = 0.36089835\n",
      "Iteration 319, loss = 0.36061463\n",
      "Iteration 320, loss = 0.36005512\n",
      "Iteration 321, loss = 0.35912097\n",
      "Iteration 322, loss = 0.35918431\n",
      "Iteration 323, loss = 0.36019280\n",
      "Iteration 324, loss = 0.35965818\n",
      "Iteration 325, loss = 0.35874203\n",
      "Iteration 326, loss = 0.35916306\n",
      "Iteration 327, loss = 0.35917140\n",
      "Iteration 328, loss = 0.35810474\n",
      "Iteration 329, loss = 0.35859339\n",
      "Iteration 330, loss = 0.35788168\n",
      "Iteration 331, loss = 0.35751340\n",
      "Iteration 332, loss = 0.35753402\n",
      "Iteration 333, loss = 0.35747695\n",
      "Iteration 334, loss = 0.35757832\n",
      "Iteration 335, loss = 0.35673890\n",
      "Iteration 336, loss = 0.35712226\n",
      "Iteration 337, loss = 0.35634349\n",
      "Iteration 338, loss = 0.35620944\n",
      "Iteration 339, loss = 0.35582541\n",
      "Iteration 340, loss = 0.35515440\n",
      "Iteration 341, loss = 0.35529522\n",
      "Iteration 342, loss = 0.35448638\n",
      "Iteration 343, loss = 0.35450278\n",
      "Iteration 344, loss = 0.35455377\n",
      "Iteration 345, loss = 0.35422113\n",
      "Iteration 346, loss = 0.35418044\n",
      "Iteration 347, loss = 0.35290142\n",
      "Iteration 348, loss = 0.35334653\n",
      "Iteration 349, loss = 0.35315844\n",
      "Iteration 350, loss = 0.35338949\n",
      "Iteration 351, loss = 0.35278388\n",
      "Iteration 352, loss = 0.35362335\n",
      "Iteration 353, loss = 0.35277433\n",
      "Iteration 354, loss = 0.35252838\n",
      "Iteration 355, loss = 0.35313443\n",
      "Iteration 356, loss = 0.35278040\n",
      "Iteration 357, loss = 0.35181527\n",
      "Iteration 358, loss = 0.35169114\n",
      "Iteration 359, loss = 0.35079791\n",
      "Iteration 360, loss = 0.35129150\n",
      "Iteration 361, loss = 0.34972688\n",
      "Iteration 362, loss = 0.35074660\n",
      "Iteration 363, loss = 0.35056062\n",
      "Iteration 364, loss = 0.34975636\n",
      "Iteration 365, loss = 0.34909375\n",
      "Iteration 366, loss = 0.34923807\n",
      "Iteration 367, loss = 0.34904457\n",
      "Iteration 368, loss = 0.34965080\n",
      "Iteration 369, loss = 0.34860903\n",
      "Iteration 370, loss = 0.34837709\n",
      "Iteration 371, loss = 0.34892269\n",
      "Iteration 372, loss = 0.34875542\n",
      "Iteration 373, loss = 0.34733941\n",
      "Iteration 374, loss = 0.34737965\n",
      "Iteration 375, loss = 0.34769097\n",
      "Iteration 376, loss = 0.34709016\n",
      "Iteration 377, loss = 0.34619999\n",
      "Iteration 378, loss = 0.34685030\n",
      "Iteration 379, loss = 0.34613005\n",
      "Iteration 380, loss = 0.34560766\n",
      "Iteration 381, loss = 0.34500349\n",
      "Iteration 382, loss = 0.34619231\n",
      "Iteration 383, loss = 0.34652869\n",
      "Iteration 384, loss = 0.34555826\n",
      "Iteration 385, loss = 0.34507157\n",
      "Iteration 386, loss = 0.34488951\n",
      "Iteration 387, loss = 0.34492440\n",
      "Iteration 388, loss = 0.34463755\n",
      "Iteration 389, loss = 0.34430149\n",
      "Iteration 390, loss = 0.34406954\n",
      "Iteration 391, loss = 0.34345937\n",
      "Iteration 392, loss = 0.34388571\n",
      "Iteration 393, loss = 0.34358389\n",
      "Iteration 394, loss = 0.34308401\n",
      "Iteration 395, loss = 0.34224762\n",
      "Iteration 396, loss = 0.34241713\n",
      "Iteration 397, loss = 0.34296611\n",
      "Iteration 398, loss = 0.34115240\n",
      "Iteration 399, loss = 0.34110804\n",
      "Iteration 400, loss = 0.34218936\n",
      "Iteration 401, loss = 0.34085148\n",
      "Iteration 402, loss = 0.34140285\n",
      "Iteration 403, loss = 0.34002297\n",
      "Iteration 404, loss = 0.34035747\n",
      "Iteration 405, loss = 0.34078655\n",
      "Iteration 406, loss = 0.34026768\n",
      "Iteration 407, loss = 0.34011033\n",
      "Iteration 408, loss = 0.33987255\n",
      "Iteration 409, loss = 0.33860003\n",
      "Iteration 410, loss = 0.33969989\n",
      "Iteration 411, loss = 0.33998099\n",
      "Iteration 412, loss = 0.33783527\n",
      "Iteration 413, loss = 0.33798793\n",
      "Iteration 414, loss = 0.33865854\n",
      "Iteration 415, loss = 0.33844783\n",
      "Iteration 416, loss = 0.33840348\n",
      "Iteration 417, loss = 0.33803909\n",
      "Iteration 418, loss = 0.33713600\n",
      "Iteration 419, loss = 0.33686011\n",
      "Iteration 420, loss = 0.33713569\n",
      "Iteration 421, loss = 0.33657389\n",
      "Iteration 422, loss = 0.33746920\n",
      "Iteration 423, loss = 0.33611297\n",
      "Iteration 424, loss = 0.33622462\n",
      "Iteration 425, loss = 0.33667670\n",
      "Iteration 426, loss = 0.33624229\n",
      "Iteration 427, loss = 0.33525765\n",
      "Iteration 428, loss = 0.33536546\n",
      "Iteration 429, loss = 0.33517852\n",
      "Iteration 430, loss = 0.33540065\n",
      "Iteration 431, loss = 0.33380511\n",
      "Iteration 432, loss = 0.33386236\n",
      "Iteration 433, loss = 0.33339421\n",
      "Iteration 434, loss = 0.33394715\n",
      "Iteration 435, loss = 0.33338766\n",
      "Iteration 436, loss = 0.33371764\n",
      "Iteration 437, loss = 0.33276855\n",
      "Iteration 438, loss = 0.33399930\n",
      "Iteration 439, loss = 0.33343321\n",
      "Iteration 440, loss = 0.33265825\n",
      "Iteration 441, loss = 0.33148150\n",
      "Iteration 442, loss = 0.33132641\n",
      "Iteration 443, loss = 0.33147505\n",
      "Iteration 444, loss = 0.33292340\n",
      "Iteration 445, loss = 0.33119073\n",
      "Iteration 446, loss = 0.33208972\n",
      "Iteration 447, loss = 0.33116185\n",
      "Iteration 448, loss = 0.33075317\n",
      "Iteration 449, loss = 0.33114556\n",
      "Iteration 450, loss = 0.33058816\n",
      "Iteration 451, loss = 0.33047785\n",
      "Iteration 452, loss = 0.33114015\n",
      "Iteration 453, loss = 0.33079162\n",
      "Iteration 454, loss = 0.32949160\n",
      "Iteration 455, loss = 0.32990344\n",
      "Iteration 456, loss = 0.33009949\n",
      "Iteration 457, loss = 0.32960071\n",
      "Iteration 458, loss = 0.32987579\n",
      "Iteration 459, loss = 0.32876954\n",
      "Iteration 460, loss = 0.32899430\n",
      "Iteration 461, loss = 0.32783010\n",
      "Iteration 462, loss = 0.32809892\n",
      "Iteration 463, loss = 0.32927248\n",
      "Iteration 464, loss = 0.32766236\n",
      "Iteration 465, loss = 0.32744876\n",
      "Iteration 466, loss = 0.32797527\n",
      "Iteration 467, loss = 0.32695268\n",
      "Iteration 468, loss = 0.32618031\n",
      "Iteration 469, loss = 0.32813583\n",
      "Iteration 470, loss = 0.32795449\n",
      "Iteration 471, loss = 0.32624655\n",
      "Iteration 472, loss = 0.32601471\n",
      "Iteration 473, loss = 0.32621632\n",
      "Iteration 474, loss = 0.32631633\n",
      "Iteration 475, loss = 0.32539315\n",
      "Iteration 476, loss = 0.32575336\n",
      "Iteration 477, loss = 0.32606623\n",
      "Iteration 478, loss = 0.32505278\n",
      "Iteration 479, loss = 0.32464919\n",
      "Iteration 480, loss = 0.32519385\n",
      "Iteration 481, loss = 0.32468092\n",
      "Iteration 482, loss = 0.32410103\n",
      "Iteration 483, loss = 0.32437555\n",
      "Iteration 484, loss = 0.32372734\n",
      "Iteration 485, loss = 0.32416969\n",
      "Iteration 486, loss = 0.32293074\n",
      "Iteration 487, loss = 0.32415728\n",
      "Iteration 488, loss = 0.32381921\n",
      "Iteration 489, loss = 0.32196531\n",
      "Iteration 490, loss = 0.32263277\n",
      "Iteration 491, loss = 0.32274117\n",
      "Iteration 492, loss = 0.32264986\n",
      "Iteration 493, loss = 0.32214030\n",
      "Iteration 494, loss = 0.32312048\n",
      "Iteration 495, loss = 0.32246339\n",
      "Iteration 496, loss = 0.32191730\n",
      "Iteration 497, loss = 0.32145401\n",
      "Iteration 498, loss = 0.32117073\n",
      "Iteration 499, loss = 0.32146639\n",
      "Iteration 500, loss = 0.32084768\n",
      "Iteration 501, loss = 0.32019658\n",
      "Iteration 502, loss = 0.31987527\n",
      "Iteration 503, loss = 0.32090836\n",
      "Iteration 504, loss = 0.31994721\n",
      "Iteration 505, loss = 0.31974987\n",
      "Iteration 506, loss = 0.32048947\n",
      "Iteration 507, loss = 0.32003686\n",
      "Iteration 508, loss = 0.31938520\n",
      "Iteration 509, loss = 0.31970410\n",
      "Iteration 510, loss = 0.31915884\n",
      "Iteration 511, loss = 0.31916442\n",
      "Iteration 512, loss = 0.31904961\n",
      "Iteration 513, loss = 0.31940405\n",
      "Iteration 514, loss = 0.31949881\n",
      "Iteration 515, loss = 0.31749624\n",
      "Iteration 516, loss = 0.31780300\n",
      "Iteration 517, loss = 0.31857070\n",
      "Iteration 518, loss = 0.31824641\n",
      "Iteration 519, loss = 0.31809281\n",
      "Iteration 520, loss = 0.31747782\n",
      "Iteration 521, loss = 0.31753821\n",
      "Iteration 522, loss = 0.31669467\n",
      "Iteration 523, loss = 0.31626047\n",
      "Iteration 524, loss = 0.31608266\n",
      "Iteration 525, loss = 0.31693381\n",
      "Iteration 526, loss = 0.31610401\n",
      "Iteration 527, loss = 0.31705553\n",
      "Iteration 528, loss = 0.31555689\n",
      "Iteration 529, loss = 0.31630228\n",
      "Iteration 530, loss = 0.31511667\n",
      "Iteration 531, loss = 0.31647498\n",
      "Iteration 532, loss = 0.31645600\n",
      "Iteration 533, loss = 0.31550632\n",
      "Iteration 534, loss = 0.31500132\n",
      "Iteration 535, loss = 0.31482049\n",
      "Iteration 536, loss = 0.31472473\n",
      "Iteration 537, loss = 0.31457190\n",
      "Iteration 538, loss = 0.31486487\n",
      "Iteration 539, loss = 0.31418325\n",
      "Iteration 540, loss = 0.31499989\n",
      "Iteration 541, loss = 0.31467269\n",
      "Iteration 542, loss = 0.31330368\n",
      "Iteration 543, loss = 0.31336713\n",
      "Iteration 544, loss = 0.31314189\n",
      "Iteration 545, loss = 0.31341657\n",
      "Iteration 546, loss = 0.31268217\n",
      "Iteration 547, loss = 0.31237221\n",
      "Iteration 548, loss = 0.31281758\n",
      "Iteration 549, loss = 0.31257098\n",
      "Iteration 550, loss = 0.31239550\n",
      "Iteration 551, loss = 0.31168862\n",
      "Iteration 552, loss = 0.31209664\n",
      "Iteration 553, loss = 0.31297938\n",
      "Iteration 554, loss = 0.31170441\n",
      "Iteration 555, loss = 0.31082537\n",
      "Iteration 556, loss = 0.31167249\n",
      "Iteration 557, loss = 0.31111958\n",
      "Iteration 558, loss = 0.31223628\n",
      "Iteration 559, loss = 0.31108976\n",
      "Iteration 560, loss = 0.31113112\n",
      "Iteration 561, loss = 0.31121173\n",
      "Iteration 562, loss = 0.31137294\n",
      "Iteration 563, loss = 0.30989465\n",
      "Iteration 564, loss = 0.31010948\n",
      "Iteration 565, loss = 0.31039858\n",
      "Iteration 566, loss = 0.31102094\n",
      "Iteration 567, loss = 0.30940553\n",
      "Iteration 568, loss = 0.31015151\n",
      "Iteration 569, loss = 0.30919853\n",
      "Iteration 570, loss = 0.31032346\n",
      "Iteration 571, loss = 0.30879085\n",
      "Iteration 572, loss = 0.30981623\n",
      "Iteration 573, loss = 0.30878585\n",
      "Iteration 574, loss = 0.30867418\n",
      "Iteration 575, loss = 0.30941293\n",
      "Iteration 576, loss = 0.30963140\n",
      "Iteration 577, loss = 0.30905493\n",
      "Iteration 578, loss = 0.30866191\n",
      "Iteration 579, loss = 0.30869290\n",
      "Iteration 580, loss = 0.30856987\n",
      "Iteration 581, loss = 0.30936082\n",
      "Iteration 582, loss = 0.30649559\n",
      "Iteration 583, loss = 0.30709628\n",
      "Iteration 584, loss = 0.30773843\n",
      "Iteration 585, loss = 0.30723330\n",
      "Iteration 586, loss = 0.30746658\n",
      "Iteration 587, loss = 0.30691705\n",
      "Iteration 588, loss = 0.30642920\n",
      "Iteration 589, loss = 0.30657847\n",
      "Iteration 590, loss = 0.30808497\n",
      "Iteration 591, loss = 0.30667740\n",
      "Iteration 592, loss = 0.30584218\n",
      "Iteration 593, loss = 0.30746968\n",
      "Iteration 594, loss = 0.30613911\n",
      "Iteration 595, loss = 0.30703237\n",
      "Iteration 596, loss = 0.30752811\n",
      "Iteration 597, loss = 0.30497673\n",
      "Iteration 598, loss = 0.30477416\n",
      "Iteration 599, loss = 0.30575863\n",
      "Iteration 600, loss = 0.30480891\n",
      "Iteration 601, loss = 0.30427658\n",
      "Iteration 602, loss = 0.30620411\n",
      "Iteration 603, loss = 0.30629904\n",
      "Iteration 604, loss = 0.30329474\n",
      "Iteration 605, loss = 0.30596310\n",
      "Iteration 606, loss = 0.30587373\n",
      "Iteration 607, loss = 0.30382088\n",
      "Iteration 608, loss = 0.30329743\n",
      "Iteration 609, loss = 0.30488227\n",
      "Iteration 610, loss = 0.30356269\n",
      "Iteration 611, loss = 0.30342116\n",
      "Iteration 612, loss = 0.30354744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [08:01<00:36, 36.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 613, loss = 0.30321247\n",
      "Iteration 614, loss = 0.30421121\n",
      "Iteration 615, loss = 0.30442817\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.52334597\n",
      "Iteration 2, loss = 0.48473290\n",
      "Iteration 3, loss = 0.46931158\n",
      "Iteration 4, loss = 0.46244051\n",
      "Iteration 5, loss = 0.45903910\n",
      "Iteration 6, loss = 0.45661843\n",
      "Iteration 7, loss = 0.45478232\n",
      "Iteration 8, loss = 0.45347240\n",
      "Iteration 9, loss = 0.45272193\n",
      "Iteration 10, loss = 0.45152996\n",
      "Iteration 11, loss = 0.45089678\n",
      "Iteration 12, loss = 0.45010121\n",
      "Iteration 13, loss = 0.44916399\n",
      "Iteration 14, loss = 0.44865174\n",
      "Iteration 15, loss = 0.44781985\n",
      "Iteration 16, loss = 0.44721384\n",
      "Iteration 17, loss = 0.44693229\n",
      "Iteration 18, loss = 0.44620795\n",
      "Iteration 19, loss = 0.44541549\n",
      "Iteration 20, loss = 0.44514759\n",
      "Iteration 21, loss = 0.44434453\n",
      "Iteration 22, loss = 0.44390308\n",
      "Iteration 23, loss = 0.44355711\n",
      "Iteration 24, loss = 0.44291449\n",
      "Iteration 25, loss = 0.44235550\n",
      "Iteration 26, loss = 0.44199211\n",
      "Iteration 27, loss = 0.44148903\n",
      "Iteration 28, loss = 0.44105627\n",
      "Iteration 29, loss = 0.44040849\n",
      "Iteration 30, loss = 0.44060803\n",
      "Iteration 31, loss = 0.43954842\n",
      "Iteration 32, loss = 0.43898091\n",
      "Iteration 33, loss = 0.43904532\n",
      "Iteration 34, loss = 0.43830457\n",
      "Iteration 35, loss = 0.43800746\n",
      "Iteration 36, loss = 0.43763508\n",
      "Iteration 37, loss = 0.43685553\n",
      "Iteration 38, loss = 0.43658291\n",
      "Iteration 39, loss = 0.43600253\n",
      "Iteration 40, loss = 0.43570802\n",
      "Iteration 41, loss = 0.43546745\n",
      "Iteration 42, loss = 0.43483551\n",
      "Iteration 43, loss = 0.43484621\n",
      "Iteration 44, loss = 0.43406658\n",
      "Iteration 45, loss = 0.43394061\n",
      "Iteration 46, loss = 0.43386122\n",
      "Iteration 47, loss = 0.43315530\n",
      "Iteration 48, loss = 0.43292842\n",
      "Iteration 49, loss = 0.43245035\n",
      "Iteration 50, loss = 0.43195689\n",
      "Iteration 51, loss = 0.43197327\n",
      "Iteration 52, loss = 0.43122537\n",
      "Iteration 53, loss = 0.43116641\n",
      "Iteration 54, loss = 0.43101773\n",
      "Iteration 55, loss = 0.43017829\n",
      "Iteration 56, loss = 0.43011247\n",
      "Iteration 57, loss = 0.42946660\n",
      "Iteration 58, loss = 0.42908662\n",
      "Iteration 59, loss = 0.42929244\n",
      "Iteration 60, loss = 0.42871450\n",
      "Iteration 61, loss = 0.42816306\n",
      "Iteration 62, loss = 0.42788970\n",
      "Iteration 63, loss = 0.42778162\n",
      "Iteration 64, loss = 0.42721040\n",
      "Iteration 65, loss = 0.42702974\n",
      "Iteration 66, loss = 0.42686645\n",
      "Iteration 67, loss = 0.42711741\n",
      "Iteration 68, loss = 0.42615720\n",
      "Iteration 69, loss = 0.42597529\n",
      "Iteration 70, loss = 0.42547584\n",
      "Iteration 71, loss = 0.42530356\n",
      "Iteration 72, loss = 0.42511608\n",
      "Iteration 73, loss = 0.42467642\n",
      "Iteration 74, loss = 0.42427348\n",
      "Iteration 75, loss = 0.42413356\n",
      "Iteration 76, loss = 0.42372720\n",
      "Iteration 77, loss = 0.42358894\n",
      "Iteration 78, loss = 0.42300787\n",
      "Iteration 79, loss = 0.42310159\n",
      "Iteration 80, loss = 0.42291855\n",
      "Iteration 81, loss = 0.42208828\n",
      "Iteration 82, loss = 0.42261837\n",
      "Iteration 83, loss = 0.42174247\n",
      "Iteration 84, loss = 0.42132167\n",
      "Iteration 85, loss = 0.42138466\n",
      "Iteration 86, loss = 0.42075151\n",
      "Iteration 87, loss = 0.42058182\n",
      "Iteration 88, loss = 0.42016043\n",
      "Iteration 89, loss = 0.42000391\n",
      "Iteration 90, loss = 0.41951723\n",
      "Iteration 91, loss = 0.41945068\n",
      "Iteration 92, loss = 0.41900080\n",
      "Iteration 93, loss = 0.41863585\n",
      "Iteration 94, loss = 0.41868628\n",
      "Iteration 95, loss = 0.41822299\n",
      "Iteration 96, loss = 0.41819892\n",
      "Iteration 97, loss = 0.41750218\n",
      "Iteration 98, loss = 0.41748634\n",
      "Iteration 99, loss = 0.41735543\n",
      "Iteration 100, loss = 0.41717248\n",
      "Iteration 101, loss = 0.41627183\n",
      "Iteration 102, loss = 0.41617310\n",
      "Iteration 103, loss = 0.41571989\n",
      "Iteration 104, loss = 0.41566217\n",
      "Iteration 105, loss = 0.41575373\n",
      "Iteration 106, loss = 0.41562665\n",
      "Iteration 107, loss = 0.41481336\n",
      "Iteration 108, loss = 0.41497524\n",
      "Iteration 109, loss = 0.41440988\n",
      "Iteration 110, loss = 0.41495642\n",
      "Iteration 111, loss = 0.41352853\n",
      "Iteration 112, loss = 0.41334756\n",
      "Iteration 113, loss = 0.41344145\n",
      "Iteration 114, loss = 0.41293393\n",
      "Iteration 115, loss = 0.41314796\n",
      "Iteration 116, loss = 0.41217291\n",
      "Iteration 117, loss = 0.41198389\n",
      "Iteration 118, loss = 0.41171533\n",
      "Iteration 119, loss = 0.41153614\n",
      "Iteration 120, loss = 0.41092770\n",
      "Iteration 121, loss = 0.41071226\n",
      "Iteration 122, loss = 0.41055443\n",
      "Iteration 123, loss = 0.40985211\n",
      "Iteration 124, loss = 0.40972764\n",
      "Iteration 125, loss = 0.40963724\n",
      "Iteration 126, loss = 0.40954308\n",
      "Iteration 127, loss = 0.40856178\n",
      "Iteration 128, loss = 0.40834466\n",
      "Iteration 129, loss = 0.40837252\n",
      "Iteration 130, loss = 0.40819121\n",
      "Iteration 131, loss = 0.40830759\n",
      "Iteration 132, loss = 0.40737867\n",
      "Iteration 133, loss = 0.40706510\n",
      "Iteration 134, loss = 0.40726566\n",
      "Iteration 135, loss = 0.40695443\n",
      "Iteration 136, loss = 0.40685494\n",
      "Iteration 137, loss = 0.40638884\n",
      "Iteration 138, loss = 0.40589362\n",
      "Iteration 139, loss = 0.40505124\n",
      "Iteration 140, loss = 0.40541537\n",
      "Iteration 141, loss = 0.40507261\n",
      "Iteration 142, loss = 0.40456743\n",
      "Iteration 143, loss = 0.40432775\n",
      "Iteration 144, loss = 0.40399036\n",
      "Iteration 145, loss = 0.40381649\n",
      "Iteration 146, loss = 0.40353672\n",
      "Iteration 147, loss = 0.40303818\n",
      "Iteration 148, loss = 0.40347362\n",
      "Iteration 149, loss = 0.40299494\n",
      "Iteration 150, loss = 0.40258213\n",
      "Iteration 151, loss = 0.40207029\n",
      "Iteration 152, loss = 0.40143476\n",
      "Iteration 153, loss = 0.40129931\n",
      "Iteration 154, loss = 0.40097580\n",
      "Iteration 155, loss = 0.40108939\n",
      "Iteration 156, loss = 0.40037081\n",
      "Iteration 157, loss = 0.40070387\n",
      "Iteration 158, loss = 0.40033792\n",
      "Iteration 159, loss = 0.39924812\n",
      "Iteration 160, loss = 0.39920034\n",
      "Iteration 161, loss = 0.39875992\n",
      "Iteration 162, loss = 0.39907256\n",
      "Iteration 163, loss = 0.39921255\n",
      "Iteration 164, loss = 0.39788231\n",
      "Iteration 165, loss = 0.39777640\n",
      "Iteration 166, loss = 0.39804184\n",
      "Iteration 167, loss = 0.39713070\n",
      "Iteration 168, loss = 0.39691367\n",
      "Iteration 169, loss = 0.39665945\n",
      "Iteration 170, loss = 0.39676586\n",
      "Iteration 171, loss = 0.39606626\n",
      "Iteration 172, loss = 0.39613665\n",
      "Iteration 173, loss = 0.39549488\n",
      "Iteration 174, loss = 0.39552620\n",
      "Iteration 175, loss = 0.39537478\n",
      "Iteration 176, loss = 0.39439755\n",
      "Iteration 177, loss = 0.39443514\n",
      "Iteration 178, loss = 0.39418250\n",
      "Iteration 179, loss = 0.39366548\n",
      "Iteration 180, loss = 0.39352412\n",
      "Iteration 181, loss = 0.39328976\n",
      "Iteration 182, loss = 0.39272131\n",
      "Iteration 183, loss = 0.39266943\n",
      "Iteration 184, loss = 0.39223224\n",
      "Iteration 185, loss = 0.39237121\n",
      "Iteration 186, loss = 0.39158972\n",
      "Iteration 187, loss = 0.39134508\n",
      "Iteration 188, loss = 0.39149320\n",
      "Iteration 189, loss = 0.39085369\n",
      "Iteration 190, loss = 0.39052389\n",
      "Iteration 191, loss = 0.39061874\n",
      "Iteration 192, loss = 0.39028675\n",
      "Iteration 193, loss = 0.38959166\n",
      "Iteration 194, loss = 0.38966738\n",
      "Iteration 195, loss = 0.38984833\n",
      "Iteration 196, loss = 0.38957856\n",
      "Iteration 197, loss = 0.38851763\n",
      "Iteration 198, loss = 0.38815722\n",
      "Iteration 199, loss = 0.38792716\n",
      "Iteration 200, loss = 0.38759271\n",
      "Iteration 201, loss = 0.38692225\n",
      "Iteration 202, loss = 0.38676469\n",
      "Iteration 203, loss = 0.38636862\n",
      "Iteration 204, loss = 0.38600787\n",
      "Iteration 205, loss = 0.38586488\n",
      "Iteration 206, loss = 0.38573951\n",
      "Iteration 207, loss = 0.38580164\n",
      "Iteration 208, loss = 0.38581751\n",
      "Iteration 209, loss = 0.38426477\n",
      "Iteration 210, loss = 0.38420288\n",
      "Iteration 211, loss = 0.38430817\n",
      "Iteration 212, loss = 0.38371942\n",
      "Iteration 213, loss = 0.38407692\n",
      "Iteration 214, loss = 0.38316231\n",
      "Iteration 215, loss = 0.38350983\n",
      "Iteration 216, loss = 0.38265087\n",
      "Iteration 217, loss = 0.38186792\n",
      "Iteration 218, loss = 0.38229170\n",
      "Iteration 219, loss = 0.38225748\n",
      "Iteration 220, loss = 0.38180043\n",
      "Iteration 221, loss = 0.38127517\n",
      "Iteration 222, loss = 0.38139079\n",
      "Iteration 223, loss = 0.38111404\n",
      "Iteration 224, loss = 0.38032719\n",
      "Iteration 225, loss = 0.38014490\n",
      "Iteration 226, loss = 0.37959191\n",
      "Iteration 227, loss = 0.38020435\n",
      "Iteration 228, loss = 0.37939114\n",
      "Iteration 229, loss = 0.37939828\n",
      "Iteration 230, loss = 0.37852942\n",
      "Iteration 231, loss = 0.37894507\n",
      "Iteration 232, loss = 0.37806332\n",
      "Iteration 233, loss = 0.37756343\n",
      "Iteration 234, loss = 0.37752819\n",
      "Iteration 235, loss = 0.37748763\n",
      "Iteration 236, loss = 0.37641875\n",
      "Iteration 237, loss = 0.37604548\n",
      "Iteration 238, loss = 0.37657553\n",
      "Iteration 239, loss = 0.37561894\n",
      "Iteration 240, loss = 0.37642780\n",
      "Iteration 241, loss = 0.37538120\n",
      "Iteration 242, loss = 0.37506929\n",
      "Iteration 243, loss = 0.37433466\n",
      "Iteration 244, loss = 0.37448362\n",
      "Iteration 245, loss = 0.37473749\n",
      "Iteration 246, loss = 0.37314203\n",
      "Iteration 247, loss = 0.37327106\n",
      "Iteration 248, loss = 0.37359669\n",
      "Iteration 249, loss = 0.37260163\n",
      "Iteration 250, loss = 0.37254332\n",
      "Iteration 251, loss = 0.37274412\n",
      "Iteration 252, loss = 0.37261134\n",
      "Iteration 253, loss = 0.37201890\n",
      "Iteration 254, loss = 0.37182504\n",
      "Iteration 255, loss = 0.37177896\n",
      "Iteration 256, loss = 0.37026637\n",
      "Iteration 257, loss = 0.37070976\n",
      "Iteration 258, loss = 0.37114955\n",
      "Iteration 259, loss = 0.37111501\n",
      "Iteration 260, loss = 0.37034094\n",
      "Iteration 261, loss = 0.36985683\n",
      "Iteration 262, loss = 0.36920075\n",
      "Iteration 263, loss = 0.36916658\n",
      "Iteration 264, loss = 0.36916207\n",
      "Iteration 265, loss = 0.36933890\n",
      "Iteration 266, loss = 0.36894668\n",
      "Iteration 267, loss = 0.36817685\n",
      "Iteration 268, loss = 0.36744941\n",
      "Iteration 269, loss = 0.36729241\n",
      "Iteration 270, loss = 0.36728262\n",
      "Iteration 271, loss = 0.36643910\n",
      "Iteration 272, loss = 0.36768087\n",
      "Iteration 273, loss = 0.36620778\n",
      "Iteration 274, loss = 0.36624123\n",
      "Iteration 275, loss = 0.36571020\n",
      "Iteration 276, loss = 0.36604613\n",
      "Iteration 277, loss = 0.36491560\n",
      "Iteration 278, loss = 0.36486914\n",
      "Iteration 279, loss = 0.36426798\n",
      "Iteration 280, loss = 0.36403215\n",
      "Iteration 281, loss = 0.36424368\n",
      "Iteration 282, loss = 0.36376875\n",
      "Iteration 283, loss = 0.36319203\n",
      "Iteration 284, loss = 0.36313329\n",
      "Iteration 285, loss = 0.36392272\n",
      "Iteration 286, loss = 0.36343970\n",
      "Iteration 287, loss = 0.36210108\n",
      "Iteration 288, loss = 0.36232892\n",
      "Iteration 289, loss = 0.36150978\n",
      "Iteration 290, loss = 0.36083156\n",
      "Iteration 291, loss = 0.36048376\n",
      "Iteration 292, loss = 0.36141498\n",
      "Iteration 293, loss = 0.36025071\n",
      "Iteration 294, loss = 0.36143728\n",
      "Iteration 295, loss = 0.36003608\n",
      "Iteration 296, loss = 0.36028412\n",
      "Iteration 297, loss = 0.36005492\n",
      "Iteration 298, loss = 0.35936891\n",
      "Iteration 299, loss = 0.36013649\n",
      "Iteration 300, loss = 0.35874697\n",
      "Iteration 301, loss = 0.35819347\n",
      "Iteration 302, loss = 0.35861016\n",
      "Iteration 303, loss = 0.35752527\n",
      "Iteration 304, loss = 0.35800171\n",
      "Iteration 305, loss = 0.35849784\n",
      "Iteration 306, loss = 0.35750315\n",
      "Iteration 307, loss = 0.35799550\n",
      "Iteration 308, loss = 0.35686611\n",
      "Iteration 309, loss = 0.35600384\n",
      "Iteration 310, loss = 0.35608436\n",
      "Iteration 311, loss = 0.35595952\n",
      "Iteration 312, loss = 0.35503150\n",
      "Iteration 313, loss = 0.35556274\n",
      "Iteration 314, loss = 0.35457937\n",
      "Iteration 315, loss = 0.35399910\n",
      "Iteration 316, loss = 0.35468350\n",
      "Iteration 317, loss = 0.35365046\n",
      "Iteration 318, loss = 0.35348162\n",
      "Iteration 319, loss = 0.35355927\n",
      "Iteration 320, loss = 0.35430094\n",
      "Iteration 321, loss = 0.35267077\n",
      "Iteration 322, loss = 0.35250973\n",
      "Iteration 323, loss = 0.35198906\n",
      "Iteration 324, loss = 0.35181327\n",
      "Iteration 325, loss = 0.35238302\n",
      "Iteration 326, loss = 0.35173771\n",
      "Iteration 327, loss = 0.35201540\n",
      "Iteration 328, loss = 0.35044120\n",
      "Iteration 329, loss = 0.35094728\n",
      "Iteration 330, loss = 0.35086229\n",
      "Iteration 331, loss = 0.35035371\n",
      "Iteration 332, loss = 0.35040428\n",
      "Iteration 333, loss = 0.35075217\n",
      "Iteration 334, loss = 0.34950165\n",
      "Iteration 335, loss = 0.34884756\n",
      "Iteration 336, loss = 0.34863770\n",
      "Iteration 337, loss = 0.34840689\n",
      "Iteration 338, loss = 0.34854008\n",
      "Iteration 339, loss = 0.34911946\n",
      "Iteration 340, loss = 0.34808423\n",
      "Iteration 341, loss = 0.34789051\n",
      "Iteration 342, loss = 0.34703504\n",
      "Iteration 343, loss = 0.34764851\n",
      "Iteration 344, loss = 0.34721971\n",
      "Iteration 345, loss = 0.34765790\n",
      "Iteration 346, loss = 0.34506595\n",
      "Iteration 347, loss = 0.34542137\n",
      "Iteration 348, loss = 0.34604161\n",
      "Iteration 349, loss = 0.34479209\n",
      "Iteration 350, loss = 0.34642553\n",
      "Iteration 351, loss = 0.34542712\n",
      "Iteration 352, loss = 0.34551852\n",
      "Iteration 353, loss = 0.34478395\n",
      "Iteration 354, loss = 0.34372267\n",
      "Iteration 355, loss = 0.34408961\n",
      "Iteration 356, loss = 0.34411927\n",
      "Iteration 357, loss = 0.34380145\n",
      "Iteration 358, loss = 0.34345429\n",
      "Iteration 359, loss = 0.34285779\n",
      "Iteration 360, loss = 0.34264995\n",
      "Iteration 361, loss = 0.34235751\n",
      "Iteration 362, loss = 0.34200369\n",
      "Iteration 363, loss = 0.34253486\n",
      "Iteration 364, loss = 0.34191176\n",
      "Iteration 365, loss = 0.34081365\n",
      "Iteration 366, loss = 0.34104613\n",
      "Iteration 367, loss = 0.34139964\n",
      "Iteration 368, loss = 0.34020034\n",
      "Iteration 369, loss = 0.34028441\n",
      "Iteration 370, loss = 0.33980165\n",
      "Iteration 371, loss = 0.33979172\n",
      "Iteration 372, loss = 0.33998903\n",
      "Iteration 373, loss = 0.33857926\n",
      "Iteration 374, loss = 0.33906330\n",
      "Iteration 375, loss = 0.33841270\n",
      "Iteration 376, loss = 0.33943701\n",
      "Iteration 377, loss = 0.33804397\n",
      "Iteration 378, loss = 0.33876663\n",
      "Iteration 379, loss = 0.33759878\n",
      "Iteration 380, loss = 0.33680239\n",
      "Iteration 381, loss = 0.33870045\n",
      "Iteration 382, loss = 0.33705585\n",
      "Iteration 383, loss = 0.33680594\n",
      "Iteration 384, loss = 0.33699345\n",
      "Iteration 385, loss = 0.33636712\n",
      "Iteration 386, loss = 0.33574527\n",
      "Iteration 387, loss = 0.33664032\n",
      "Iteration 388, loss = 0.33482801\n",
      "Iteration 389, loss = 0.33502609\n",
      "Iteration 390, loss = 0.33456389\n",
      "Iteration 391, loss = 0.33535893\n",
      "Iteration 392, loss = 0.33467787\n",
      "Iteration 393, loss = 0.33431618\n",
      "Iteration 394, loss = 0.33451773\n",
      "Iteration 395, loss = 0.33270583\n",
      "Iteration 396, loss = 0.33441078\n",
      "Iteration 397, loss = 0.33350870\n",
      "Iteration 398, loss = 0.33311054\n",
      "Iteration 399, loss = 0.33251452\n",
      "Iteration 400, loss = 0.33223832\n",
      "Iteration 401, loss = 0.33260687\n",
      "Iteration 402, loss = 0.33202232\n",
      "Iteration 403, loss = 0.33218889\n",
      "Iteration 404, loss = 0.33157611\n",
      "Iteration 405, loss = 0.33123161\n",
      "Iteration 406, loss = 0.33132418\n",
      "Iteration 407, loss = 0.33053223\n",
      "Iteration 408, loss = 0.33116546\n",
      "Iteration 409, loss = 0.33023884\n",
      "Iteration 410, loss = 0.32970597\n",
      "Iteration 411, loss = 0.32911561\n",
      "Iteration 412, loss = 0.32917341\n",
      "Iteration 413, loss = 0.32922209\n",
      "Iteration 414, loss = 0.32908790\n",
      "Iteration 415, loss = 0.32856560\n",
      "Iteration 416, loss = 0.32882406\n",
      "Iteration 417, loss = 0.32864894\n",
      "Iteration 418, loss = 0.32800716\n",
      "Iteration 419, loss = 0.32804038\n",
      "Iteration 420, loss = 0.32694904\n",
      "Iteration 421, loss = 0.32762077\n",
      "Iteration 422, loss = 0.32703281\n",
      "Iteration 423, loss = 0.32689381\n",
      "Iteration 424, loss = 0.32660944\n",
      "Iteration 425, loss = 0.32592466\n",
      "Iteration 426, loss = 0.32578018\n",
      "Iteration 427, loss = 0.32645461\n",
      "Iteration 428, loss = 0.32654012\n",
      "Iteration 429, loss = 0.32425591\n",
      "Iteration 430, loss = 0.32478412\n",
      "Iteration 431, loss = 0.32453228\n",
      "Iteration 432, loss = 0.32520184\n",
      "Iteration 433, loss = 0.32484126\n",
      "Iteration 434, loss = 0.32463472\n",
      "Iteration 435, loss = 0.32330084\n",
      "Iteration 436, loss = 0.32368044\n",
      "Iteration 437, loss = 0.32298306\n",
      "Iteration 438, loss = 0.32324419\n",
      "Iteration 439, loss = 0.32335656\n",
      "Iteration 440, loss = 0.32351761\n",
      "Iteration 441, loss = 0.32297494\n",
      "Iteration 442, loss = 0.32346847\n",
      "Iteration 443, loss = 0.32135780\n",
      "Iteration 444, loss = 0.32297269\n",
      "Iteration 445, loss = 0.32037087\n",
      "Iteration 446, loss = 0.32199292\n",
      "Iteration 447, loss = 0.32158426\n",
      "Iteration 448, loss = 0.32021692\n",
      "Iteration 449, loss = 0.32066565\n",
      "Iteration 450, loss = 0.32142301\n",
      "Iteration 451, loss = 0.32073719\n",
      "Iteration 452, loss = 0.31981119\n",
      "Iteration 453, loss = 0.32172907\n",
      "Iteration 454, loss = 0.31980137\n",
      "Iteration 455, loss = 0.31869494\n",
      "Iteration 456, loss = 0.31879778\n",
      "Iteration 457, loss = 0.31939014\n",
      "Iteration 458, loss = 0.31875542\n",
      "Iteration 459, loss = 0.31857716\n",
      "Iteration 460, loss = 0.31824112\n",
      "Iteration 461, loss = 0.31841450\n",
      "Iteration 462, loss = 0.31731689\n",
      "Iteration 463, loss = 0.31891395\n",
      "Iteration 464, loss = 0.31812851\n",
      "Iteration 465, loss = 0.31706083\n",
      "Iteration 466, loss = 0.31702808\n",
      "Iteration 467, loss = 0.31767433\n",
      "Iteration 468, loss = 0.31554248\n",
      "Iteration 469, loss = 0.31693880\n",
      "Iteration 470, loss = 0.31524513\n",
      "Iteration 471, loss = 0.31657155\n",
      "Iteration 472, loss = 0.31585543\n",
      "Iteration 473, loss = 0.31548868\n",
      "Iteration 474, loss = 0.31490911\n",
      "Iteration 475, loss = 0.31493083\n",
      "Iteration 476, loss = 0.31521449\n",
      "Iteration 477, loss = 0.31549024\n",
      "Iteration 478, loss = 0.31434229\n",
      "Iteration 479, loss = 0.31356019\n",
      "Iteration 480, loss = 0.31488970\n",
      "Iteration 481, loss = 0.31369290\n",
      "Iteration 482, loss = 0.31366852\n",
      "Iteration 483, loss = 0.31300851\n",
      "Iteration 484, loss = 0.31330818\n",
      "Iteration 485, loss = 0.31272831\n",
      "Iteration 486, loss = 0.31250805\n",
      "Iteration 487, loss = 0.31246045\n",
      "Iteration 488, loss = 0.31216755\n",
      "Iteration 489, loss = 0.31211039\n",
      "Iteration 490, loss = 0.31070923\n",
      "Iteration 491, loss = 0.31159832\n",
      "Iteration 492, loss = 0.31043509\n",
      "Iteration 493, loss = 0.31020754\n",
      "Iteration 494, loss = 0.31177779\n",
      "Iteration 495, loss = 0.31025623\n",
      "Iteration 496, loss = 0.31100690\n",
      "Iteration 497, loss = 0.30967134\n",
      "Iteration 498, loss = 0.31079767\n",
      "Iteration 499, loss = 0.31039396\n",
      "Iteration 500, loss = 0.30963888\n",
      "Iteration 501, loss = 0.30853233\n",
      "Iteration 502, loss = 0.30887761\n",
      "Iteration 503, loss = 0.30874246\n",
      "Iteration 504, loss = 0.30916878\n",
      "Iteration 505, loss = 0.30981549\n",
      "Iteration 506, loss = 0.31076812\n",
      "Iteration 507, loss = 0.30991893\n",
      "Iteration 508, loss = 0.30811130\n",
      "Iteration 509, loss = 0.30877113\n",
      "Iteration 510, loss = 0.30818045\n",
      "Iteration 511, loss = 0.30802284\n",
      "Iteration 512, loss = 0.30782306\n",
      "Iteration 513, loss = 0.30739690\n",
      "Iteration 514, loss = 0.30725202\n",
      "Iteration 515, loss = 0.30729739\n",
      "Iteration 516, loss = 0.30603048\n",
      "Iteration 517, loss = 0.30775786\n",
      "Iteration 518, loss = 0.30598687\n",
      "Iteration 519, loss = 0.30553707\n",
      "Iteration 520, loss = 0.30542487\n",
      "Iteration 521, loss = 0.30509704\n",
      "Iteration 522, loss = 0.30448726\n",
      "Iteration 523, loss = 0.30529143\n",
      "Iteration 524, loss = 0.30437767\n",
      "Iteration 525, loss = 0.30613310\n",
      "Iteration 526, loss = 0.30583847\n",
      "Iteration 527, loss = 0.30482496\n",
      "Iteration 528, loss = 0.30322570\n",
      "Iteration 529, loss = 0.30249905\n",
      "Iteration 530, loss = 0.30418040\n",
      "Iteration 531, loss = 0.30440739\n",
      "Iteration 532, loss = 0.30357750\n",
      "Iteration 533, loss = 0.30424590\n",
      "Iteration 534, loss = 0.30286153\n",
      "Iteration 535, loss = 0.30306194\n",
      "Iteration 536, loss = 0.30342329\n",
      "Iteration 537, loss = 0.30362159\n",
      "Iteration 538, loss = 0.30265895\n",
      "Iteration 539, loss = 0.30180644\n",
      "Iteration 540, loss = 0.30264048\n",
      "Iteration 541, loss = 0.30426453\n",
      "Iteration 542, loss = 0.30235773\n",
      "Iteration 543, loss = 0.30251659\n",
      "Iteration 544, loss = 0.30079246\n",
      "Iteration 545, loss = 0.30070471\n",
      "Iteration 546, loss = 0.30054422\n",
      "Iteration 547, loss = 0.30185519\n",
      "Iteration 548, loss = 0.30109282\n",
      "Iteration 549, loss = 0.30003464\n",
      "Iteration 550, loss = 0.29997215\n",
      "Iteration 551, loss = 0.30085880\n",
      "Iteration 552, loss = 0.29934589\n",
      "Iteration 553, loss = 0.29933170\n",
      "Iteration 554, loss = 0.29877338\n",
      "Iteration 555, loss = 0.29925169\n",
      "Iteration 556, loss = 0.29939957\n",
      "Iteration 557, loss = 0.29915758\n",
      "Iteration 558, loss = 0.29738006\n",
      "Iteration 559, loss = 0.29875487\n",
      "Iteration 560, loss = 0.29860428\n",
      "Iteration 561, loss = 0.29850409\n",
      "Iteration 562, loss = 0.29777989\n",
      "Iteration 563, loss = 0.29814720\n",
      "Iteration 564, loss = 0.29794907\n",
      "Iteration 565, loss = 0.29770101\n",
      "Iteration 566, loss = 0.29670869\n",
      "Iteration 567, loss = 0.29690289\n",
      "Iteration 568, loss = 0.29708135\n",
      "Iteration 569, loss = 0.29782187\n",
      "Iteration 570, loss = 0.29598638\n",
      "Iteration 571, loss = 0.29690186\n",
      "Iteration 572, loss = 0.29537928\n",
      "Iteration 573, loss = 0.29510427\n",
      "Iteration 574, loss = 0.29569875\n",
      "Iteration 575, loss = 0.29554768\n",
      "Iteration 576, loss = 0.29656771\n",
      "Iteration 577, loss = 0.29671223\n",
      "Iteration 578, loss = 0.29489471\n",
      "Iteration 579, loss = 0.29607772\n",
      "Iteration 580, loss = 0.29580980\n",
      "Iteration 581, loss = 0.29439003\n",
      "Iteration 582, loss = 0.29375542\n",
      "Iteration 583, loss = 0.29399504\n",
      "Iteration 584, loss = 0.29518700\n",
      "Iteration 585, loss = 0.29524801\n",
      "Iteration 586, loss = 0.29388282\n",
      "Iteration 587, loss = 0.29485790\n",
      "Iteration 588, loss = 0.29346504\n",
      "Iteration 589, loss = 0.29372983\n",
      "Iteration 590, loss = 0.29398655\n",
      "Iteration 591, loss = 0.29239837\n",
      "Iteration 592, loss = 0.29254488\n",
      "Iteration 593, loss = 0.29233290\n",
      "Iteration 594, loss = 0.29241934\n",
      "Iteration 595, loss = 0.29161111\n",
      "Iteration 596, loss = 0.29175498\n",
      "Iteration 597, loss = 0.29228612\n",
      "Iteration 598, loss = 0.29340317\n",
      "Iteration 599, loss = 0.29140248\n",
      "Iteration 600, loss = 0.29095173\n",
      "Iteration 601, loss = 0.29143409\n",
      "Iteration 602, loss = 0.28984860\n",
      "Iteration 603, loss = 0.29104259\n",
      "Iteration 604, loss = 0.29042676\n",
      "Iteration 605, loss = 0.29135040\n",
      "Iteration 606, loss = 0.29081848\n",
      "Iteration 607, loss = 0.28997811\n",
      "Iteration 608, loss = 0.29015287\n",
      "Iteration 609, loss = 0.29083520\n",
      "Iteration 610, loss = 0.29069393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [08:36<00:00, 25.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 611, loss = 0.29112871\n",
      "Iteration 612, loss = 0.28979956\n",
      "Iteration 613, loss = 0.28990843\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_size = np.linspace(1, 40, 20)\n",
    "hidden_layer_size = hidden_layer_size.astype(int)\n",
    "test_err, train_err = [], []\n",
    "train_acc, test_acc = [], []\n",
    "\n",
    "for size in tqdm(hidden_layer_size):\n",
    "    model = MLPClassifier(hidden_layer_sizes=(size,), \n",
    "                          solver='sgd', activation='relu', max_iter=1000, random_state=13, learning_rate_init=0.01, verbose = \"True\")\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    train_err.append(np.mean(y_train != y_train_pred))\n",
    "    test_err.append(np.mean(y_test != y_test_pred))\n",
    "    \n",
    "    train_acc.append(accuracy_score(y_train, y_train_pred))\n",
    "    test_acc.append(accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "367c0495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: минимальное значение ошибки = 0.12560418459908626, число нейронов = 33\n",
      "Test: минимальное значение ошибки = 0.22055513878469618, число нейронов = 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hidden_layer_size, train_err, 'b-o', label = 'train')\n",
    "plt.plot(hidden_layer_size, test_err, 'r-o', label = 'test')\n",
    "plt.xlim([np.min(hidden_layer_size), np.max(hidden_layer_size)])\n",
    "plt.title('Error is as a function of hidden_layer_size')\n",
    "plt.xlabel('hidden_layer_size')\n",
    "plt.ylabel('error')\n",
    "plt.legend()\n",
    "\n",
    "print(f\"Train: минимальное значение ошибки = {np.min(train_err)}, число нейронов = {hidden_layer_size[np.argmin(train_err)]}\")\n",
    "print(f\"Test: минимальное значение ошибки = {np.min(test_err)}, число нейронов = {hidden_layer_size[np.argmin(test_err)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57dd63cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: максимальное значение accuracy = 0.8743958154009137, число нейронов = 33\n",
      "Test: максимальное значение accuracy = 0.7794448612153039, число нейронов = 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hidden_layer_size, train_acc, 'r-o', label = 'train')\n",
    "plt.plot(hidden_layer_size, test_acc, 'b-o', label = 'test')\n",
    "plt.xlim([np.min(hidden_layer_size), np.max(hidden_layer_size)])\n",
    "plt.title('Accuracy is a function of hidden_layer_size')\n",
    "plt.xlabel('hidden_layer_size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "print(f\"Train: максимальное значение accuracy = {np.max(train_acc)}, число нейронов = {hidden_layer_size[np.argmax(train_acc)]}\")\n",
    "print(f\"Test: максимальное значение accuracy = {np.max(test_acc)}, число нейронов = {hidden_layer_size[np.argmax(test_acc)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c256319",
   "metadata": {},
   "source": [
    "Переобучение осталось"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d491c878-14de-400a-a71b-79f904144b28",
   "metadata": {},
   "source": [
    "## Подбор числа нейронов в двухслойном классификаторе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a9ef1e16-fa2e-40f2-9553-3cd236b86d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [41:03, 246.36s/it]\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "sizes = np.linspace(1, 60, n).astype(int)\n",
    "test_err = np.zeros((n, n))\n",
    "train_err = np.zeros((n, n))\n",
    "train_acc = np.zeros((n, n))\n",
    "test_acc = np.zeros((n, n))\n",
    "\n",
    "for i, size_1 in tqdm(enumerate(sizes)):\n",
    "    for j, size_2 in enumerate(sizes):\n",
    "        model = MLPClassifier(hidden_layer_sizes=(size_1, size_2, ), \n",
    "                              solver='adam', activation='relu', max_iter=3000, random_state=13)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "\n",
    "        train_err[i][j] = np.mean(y_train != y_train_pred)\n",
    "        test_err[i][j] = np.mean(y_test != y_test_pred)\n",
    "\n",
    "        train_acc[i][j] = accuracy_score(y_train, y_train_pred)\n",
    "        test_acc[i][j] = accuracy_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e982f7d-f6b0-4499-a826-428eba5de9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: минимальное значение ошибки = 0.04634840760113885, число нейронов = (8, 9)\n",
      "Test: минимальное значение ошибки = 0.22168042010502625, число нейронов = (0, 2)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(121)\n",
    "plt.title(\"Train error\")\n",
    "plt.imshow(train_err)\n",
    "plt.xticks(np.arange(n), np.arange(1, 50, 50 / n))\n",
    "plt.yticks(np.arange(n), np.arange(1, 50, 50 / n))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Test error\")\n",
    "plt.imshow(test_err)\n",
    "plt.xticks(np.arange(n), np.arange(1, 50, 50 / n))\n",
    "plt.yticks(np.arange(n), np.arange(1, 50, 50 / n))\n",
    "\n",
    "cax = plt.axes([0.96, 0.15, 0.02, 0.7])\n",
    "plt.colorbar(cax=cax)\n",
    "\n",
    "train_min_loss = np.where(train_err == np.min(train_err))\n",
    "test_min_loss = np.where(test_err == np.min(test_err))\n",
    "\n",
    "train_min_loss_sizes = np.where(train_err == np.min(train_err))\n",
    "print(f\"Train: минимальное значение ошибки = {np.min(train_err)}, число нейронов = (\" \\\n",
    "      f\"{train_min_loss_sizes[0][0]}, {train_min_loss_sizes[1][0]})\")\n",
    "\n",
    "test_min_loss_sizes = np.where(test_err == np.min(test_err))\n",
    "print(f\"Test: минимальное значение ошибки = {np.min(test_err)}, число нейронов = (\" \\\n",
    "      f\"{test_min_loss_sizes[0][0]}, {test_min_loss_sizes[1][0]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b077a2cb-db9f-4832-b0f5-f3b80310940a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: максимальное значение accuracy = 0.9536515923988611, число нейронов = (8, 9)\n",
      "Test: максимальное значение accuracy = 0.7783195798949737, число нейронов = (0, 2)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(121)\n",
    "plt.title(\"Train accuracy\")\n",
    "plt.imshow(train_acc)\n",
    "plt.xticks(np.arange(n), np.arange(1, 50, 50 / n))\n",
    "plt.yticks(np.arange(n), np.arange(1, 50, 50 / n))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Test accuracy\")\n",
    "plt.imshow(test_acc)\n",
    "plt.xticks(np.arange(n), np.arange(1, 50, 50 / n))\n",
    "plt.yticks(np.arange(n), np.arange(1, 50, 50 / n))\n",
    "\n",
    "cax = plt.axes([0.95, 0.15, 0.02, 0.7])\n",
    "plt.colorbar(cax=cax)\n",
    "\n",
    "train_max_acc = np.where(train_acc == np.max(train_acc))\n",
    "test_max_acc = np.where(test_acc == np.max(test_acc))\n",
    "\n",
    "\n",
    "train_max_acc_sizes = np.where(train_acc == np.max(train_acc))\n",
    "print(f\"Train: максимальное значение accuracy = {np.max(train_acc)}, число нейронов = (\" \\\n",
    "      f\"{train_max_acc_sizes[0][0]}, {train_max_acc_sizes[1][0]})\")\n",
    "\n",
    "test_max_acc_sizes = np.where(test_acc == np.max(test_acc))\n",
    "print(f\"Test: максимальное значение accuracy = {np.max(test_acc)}, число нейронов = (\" \\\n",
    "      f\"{test_max_acc_sizes[0][0]}, {test_max_acc_sizes[1][0]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c7c7c6-5b48-4a11-8606-a7663cc5fcae",
   "metadata": {},
   "source": [
    "## Подбор параметра регуляризации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "41a45cc7-0ad1-44b6-a6e8-31a4ae3457ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [06:48<00:00,  8.17s/it]\n"
     ]
    }
   ],
   "source": [
    "alpha_arr = np.logspace(-5, 5, 50)\n",
    "test_err = []\n",
    "train_err = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "for alpha in tqdm(alpha_arr):\n",
    "    mlp_model = MLPClassifier(alpha = alpha, hidden_layer_sizes = (22,), \n",
    "                              solver='adam', activation='relu', max_iter=1000, random_state=13)\n",
    "    mlp_model.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = mlp_model.predict(X_train)\n",
    "    y_test_pred = mlp_model.predict(X_test)\n",
    "    \n",
    "    train_err.append(np.mean(y_train != y_train_pred))\n",
    "    test_err.append(np.mean(y_test != y_test_pred))\n",
    "    \n",
    "    train_acc.append(accuracy_score(y_train, y_train_pred))\n",
    "    test_acc.append(accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "929e2d24-c09a-4a36-af02-9bf27093150b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f97100c3a00>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(alpha_arr, train_err, 'b-o', label = 'train')\n",
    "plt.semilogx(alpha_arr, test_err, 'r-o', label = 'test')\n",
    "plt.xlim([np.min(alpha_arr), np.max(alpha_arr)])\n",
    "plt.title('Error vs. alpha')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('error')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4e096630-544f-4f2e-8bf6-6fd53a0fdf44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f97301c5430>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(alpha_arr, train_acc, 'b-o', label = 'train')\n",
    "plt.semilogx(alpha_arr, test_acc, 'r-o', label = 'test')\n",
    "plt.xlim([np.min(alpha_arr), np.max(alpha_arr)])\n",
    "plt.title('Acc vs. alpha')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('acc')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1c3e982e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 79.48% Test: 77.76%\n"
     ]
    }
   ],
   "source": [
    "print(\"Train: {:.2f}% Test: {:.2f}%\".format(accuracy_score(y_train.astype('int64'), y_train_prediction)\n",
    "      * 100, accuracy_score(y_test.astype('int'), y_test_prediction) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c8a863",
   "metadata": {},
   "source": [
    "Из графика выше видим, что при значениях alpha < 1 наблюдается переобучение. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e10ec61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14705687611732768 0.22093023255813954\n",
      "0.49417133613238384\n"
     ]
    }
   ],
   "source": [
    "min_train_err = np.min(train_err)\n",
    "min_test_err = np.min(test_err)\n",
    "print(min_train_err, min_test_err)\n",
    "\n",
    "alpha_optarr = alpha_arr[test_err == min_test_err]\n",
    "alpha_opt = alpha_optarr[0]\n",
    "print(alpha_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b2931d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_err = []\n",
    "train_err = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "mlp_model = MLPClassifier(alpha = alpha_opt, hidden_layer_sizes = (22,), \n",
    "                              solver='adam', activation='relu', max_iter=1000, random_state=13)\n",
    "mlp_model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = mlp_model.predict(X_train)\n",
    "y_test_pred = mlp_model.predict(X_test)\n",
    "    \n",
    "train_err.append(np.mean(y_train != y_train_pred))\n",
    "test_err.append(np.mean(y_test != y_test_pred))\n",
    "    \n",
    "train_acc.append(accuracy_score(y_train, y_train_pred))\n",
    "test_acc.append(accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6adf9318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 79.48% Test: 77.76%\n"
     ]
    }
   ],
   "source": [
    "print(\"Train: {:.2f}% Test: {:.2f}%\".format(accuracy_score(y_train.astype('int64'), y_train_prediction)\n",
    "      * 100, accuracy_score(y_test.astype('int'), y_test_prediction) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3993f7-485c-47cf-81c3-6c9f8e4be118",
   "metadata": {},
   "source": [
    "## Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9ae769",
   "metadata": {},
   "source": [
    "## Подбор числа нейронов в однослойном классификаторе"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0541c997",
   "metadata": {},
   "source": [
    "1) Видим, что у нас переобучение, даже с другими параметрами и другим алгоритмом\n",
    "\n",
    "2) Максимальное значение accuracy = 78.1%, число нейронов = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98a6e15",
   "metadata": {},
   "source": [
    "## Подбор числа нейронов в двухслойном классификаторе"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b117a0",
   "metadata": {},
   "source": [
    "1) Большая разница минимального значения у тренировочной выборки и тестовой - значит тут переобучение.\n",
    "\n",
    "2) Максимальное значение accuracy = 78.5%, число нейронов = (0, 8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55d022c",
   "metadata": {},
   "source": [
    "## Подбор параметра регуляризации"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62721ec",
   "metadata": {},
   "source": [
    "При смене параметра регуляризации избавиться от переобучения не удалось. Это может быть обусловленно сложной моделью. Точность повысить так и не удалось."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
